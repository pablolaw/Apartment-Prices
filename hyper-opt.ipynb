{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngboost import NGBRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from ngboost.ngboost import NGBoost\n",
    "from ngboost.learners import default_tree_learner\n",
    "from ngboost.distns import Normal\n",
    "from ngboost.scores import MLE\n",
    "from ngboost import NGBRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import median_absolute_error, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from hyperopt import hp, tpe, space_eval\n",
    "from hyperopt.fmin import fmin\n",
    "from hyperopt import STATUS_OK, Trials\n",
    "from data_loader import load_toronto\n",
    "from preprocessing import pipeline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import logging\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "NGB_PATH = os.path.join(\"models\", \"ngb_model.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load-in Data**\n",
    "We are using a 72%, 8%, 20% training, validation and testing split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rent = load_toronto(test_prop=0.2)\n",
    "\n",
    "# Load in data\n",
    "X_train = rent['train']['data']\n",
    "y_train = rent['train']['labels']\n",
    "X_test = rent['test']['data']\n",
    "y_test = rent['test']['labels']\n",
    "\n",
    "# Create validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hyperparameters Set-up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree hyperparameters\n",
    "tree_hp = []\n",
    "for max_f in ['sqrt', 'log2', 0.5, 0.75]: # Subsample columns\n",
    "    for max_d in [1, 3, 5, 6]: # Subsample rows\n",
    "        tree_hp.append(DecisionTreeRegressor(\n",
    "            max_features=max_f, \n",
    "            max_depth=max_d, \n",
    "            random_state=69))\n",
    "\n",
    "space = {\n",
    "    'model': {\n",
    "        'learning_rate':hp.uniform('learning_rate', .005, 0.5),\n",
    "        'minibatch_frac':hp.choice('minibatch_frac', [1.0, 0.8, 0.7, 0.6]),\n",
    "        'Base':hp.choice('Base', tree_hp),\n",
    "        'n_estimators': hp.choice('n_estimators', [100, 500, 1000, 2000])\n",
    "    },\n",
    "    'preprocessing': {\n",
    "        'n_neighbors': hp.choice('n_neighbors', [3, 5, 10]),\n",
    "        'mul': hp.choice('mul', [True, False])\n",
    "    }\n",
    "}\n",
    "\n",
    "default_model_params = {\"verbose_eval\":1,\n",
    "                        \"random_state\":69}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Run Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    params['model'].update(default_params)\n",
    "    print(params)\n",
    "    \n",
    "    # Run through data pipeline\n",
    "    data_pipeline = pipeline(**params['preprocessing'])\n",
    "    X_train_c = data_pipeline.fit_transform(X_train)\n",
    "    X_val_c = data_pipeline.transform(X_val)\n",
    "    \n",
    "    # Run through model\n",
    "    ngb = NGBRegressor(**params['model']).fit(X_train_c, y_train, X_val=X_val_c, Y_val=y_val, early_stopping_rounds=5)\n",
    "    loss = ngb.evals_result['val']['LOGSCORE'][ngb.best_val_loss_itr]\n",
    "    results = {'loss':loss, 'status':STATUS_OK}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning hyperparameter tuning...\n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=1,\n",
      "                      max_features=0.75, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.39807986515855387, 'minibatch_frac': 0.7, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5211 val_loss=1.4817 scale=1.0000 norm=1.0651\n",
      "[iter 1] loss=1.3809 val_loss=1.4231 scale=1.0000 norm=0.9095\n",
      "[iter 2] loss=1.2811 val_loss=1.1600 scale=1.0000 norm=0.8414\n",
      "[iter 3] loss=1.0669 val_loss=1.0350 scale=2.0000 norm=1.4140\n",
      "[iter 4] loss=0.9526 val_loss=0.9542 scale=2.0000 norm=1.4383\n",
      "[iter 5] loss=0.8026 val_loss=0.9270 scale=2.0000 norm=1.4402\n",
      "[iter 6] loss=0.7471 val_loss=0.9196 scale=1.0000 norm=0.7245\n",
      "[iter 7] loss=0.7281 val_loss=0.8216 scale=1.0000 norm=0.7283\n",
      "[iter 8] loss=0.7182 val_loss=0.7653 scale=1.0000 norm=0.7477\n",
      "[iter 9] loss=0.7110 val_loss=0.7500 scale=1.0000 norm=0.7432\n",
      "[iter 10] loss=0.6647 val_loss=0.7424 scale=1.0000 norm=0.7327\n",
      "[iter 11] loss=0.6784 val_loss=0.7347 scale=1.0000 norm=0.7580\n",
      "[iter 12] loss=0.6401 val_loss=0.7214 scale=1.0000 norm=0.7212\n",
      "[iter 13] loss=0.5918 val_loss=0.7173 scale=2.0000 norm=1.3925\n",
      "[iter 14] loss=0.6076 val_loss=0.6896 scale=1.0000 norm=0.7545\n",
      "[iter 15] loss=0.6146 val_loss=0.6744 scale=1.0000 norm=0.7556\n",
      "[iter 16] loss=0.5804 val_loss=0.6914 scale=2.0000 norm=1.4618\n",
      "[iter 17] loss=0.5948 val_loss=0.6901 scale=1.0000 norm=0.7324\n",
      "[iter 18] loss=0.5722 val_loss=0.6795 scale=1.0000 norm=0.7162\n",
      "[iter 19] loss=0.5666 val_loss=0.6838 scale=1.0000 norm=0.7224\n",
      "[iter 20] loss=0.5590 val_loss=0.6607 scale=2.0000 norm=1.4506\n",
      "[iter 21] loss=0.5854 val_loss=0.6558 scale=1.0000 norm=0.7466\n",
      "[iter 22] loss=0.5411 val_loss=0.6474 scale=2.0000 norm=1.4547\n",
      "[iter 23] loss=0.5377 val_loss=0.6521 scale=2.0000 norm=1.4202\n",
      "[iter 24] loss=0.5424 val_loss=0.6466 scale=2.0000 norm=1.4526\n",
      "[iter 25] loss=0.5537 val_loss=0.6477 scale=1.0000 norm=0.7256\n",
      "[iter 26] loss=0.5742 val_loss=0.6478 scale=0.0010 norm=0.0007\n",
      "[iter 27] loss=0.5332 val_loss=0.6566 scale=2.0000 norm=1.4167\n",
      "[iter 28] loss=0.5634 val_loss=0.6549 scale=1.0000 norm=0.7404\n",
      "== Early stopping achieved.                            \n",
      "== Best iteration / VAL24 (val_loss=0.6466)            \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=5,\n",
      "                      max_features=0.75, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.19977719898021373, 'minibatch_frac': 0.8, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 5}}\n",
      "[iter 0] loss=1.5200 val_loss=1.2452 scale=1.0000 norm=1.0640                    \n",
      "[iter 1] loss=1.1851 val_loss=1.0689 scale=2.0000 norm=1.5232                    \n",
      "[iter 2] loss=1.0050 val_loss=0.9330 scale=2.0000 norm=1.3211                    \n",
      "[iter 3] loss=0.8455 val_loss=0.8138 scale=2.0000 norm=1.2397                    \n",
      "[iter 4] loss=0.7419 val_loss=0.7515 scale=2.0000 norm=1.2084                    \n",
      "[iter 5] loss=0.6272 val_loss=0.6922 scale=2.0000 norm=1.1775                    \n",
      "[iter 6] loss=0.5227 val_loss=0.6833 scale=2.0000 norm=1.1597                    \n",
      "[iter 7] loss=0.4590 val_loss=0.6544 scale=2.0000 norm=1.1487                    \n",
      "[iter 8] loss=0.4101 val_loss=0.6376 scale=2.0000 norm=1.1672                    \n",
      "[iter 9] loss=0.3677 val_loss=0.6201 scale=2.0000 norm=1.1765                    \n",
      "[iter 10] loss=0.3221 val_loss=0.6340 scale=2.0000 norm=1.1734                   \n",
      "[iter 11] loss=0.3117 val_loss=0.6435 scale=1.0000 norm=0.5965                   \n",
      "[iter 12] loss=0.2749 val_loss=0.6552 scale=1.0000 norm=0.5843                   \n",
      "[iter 13] loss=0.2659 val_loss=0.6550 scale=1.0000 norm=0.5836                   \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL9 (val_loss=0.6201)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=1,\n",
      "                      max_features='sqrt', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.17123063670681649, 'minibatch_frac': 0.8, 'n_estimators': 1000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5200 val_loss=1.4845 scale=1.0000 norm=1.0640                    \n",
      "[iter 1] loss=1.4317 val_loss=1.4620 scale=1.0000 norm=0.9791                    \n",
      "[iter 2] loss=1.4102 val_loss=1.4455 scale=2.0000 norm=1.9310                    \n",
      "[iter 3] loss=1.3418 val_loss=1.4464 scale=2.0000 norm=1.8322                    \n",
      "[iter 4] loss=1.3882 val_loss=1.4582 scale=2.0000 norm=1.9667                    \n",
      "[iter 5] loss=1.3762 val_loss=1.4522 scale=1.0000 norm=0.9777                    \n",
      "[iter 6] loss=1.3115 val_loss=1.4309 scale=2.0000 norm=1.8380                    \n",
      "[iter 7] loss=1.3616 val_loss=1.4388 scale=1.0000 norm=0.9886                    \n",
      "[iter 8] loss=1.3617 val_loss=1.4467 scale=1.0000 norm=0.9896                    \n",
      "[iter 9] loss=1.3510 val_loss=1.4478 scale=1.0000 norm=0.9761                    \n",
      "[iter 10] loss=1.3363 val_loss=1.4491 scale=1.0000 norm=0.9651                   \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL6 (val_loss=1.4309)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features='log2', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.1728081694748664, 'minibatch_frac': 0.7, 'n_estimators': 1000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5211 val_loss=1.2715 scale=1.0000 norm=1.0651                    \n",
      "[iter 1] loss=1.2068 val_loss=1.1331 scale=2.0000 norm=1.5697                    \n",
      "[iter 2] loss=1.0641 val_loss=1.0103 scale=2.0000 norm=1.4081                    \n",
      "[iter 3] loss=0.9206 val_loss=0.9182 scale=2.0000 norm=1.2898                    \n",
      "[iter 4] loss=0.8254 val_loss=0.8560 scale=2.0000 norm=1.2521                    \n",
      "[iter 5] loss=0.7225 val_loss=0.7915 scale=2.0000 norm=1.2129                    \n",
      "[iter 6] loss=0.6203 val_loss=0.7463 scale=2.0000 norm=1.1764                    \n",
      "[iter 7] loss=0.5504 val_loss=0.7100 scale=2.0000 norm=1.1605                    \n",
      "[iter 8] loss=0.4826 val_loss=0.7103 scale=2.0000 norm=1.1641                    \n",
      "[iter 9] loss=0.4200 val_loss=0.7179 scale=2.0000 norm=1.1653                    \n",
      "[iter 10] loss=0.3703 val_loss=0.7215 scale=1.0000 norm=0.5751                   \n",
      "[iter 11] loss=0.3577 val_loss=0.7259 scale=1.0000 norm=0.5794                   \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL7 (val_loss=0.7100)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=5,\n",
      "                      max_features=0.75, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.2527496175847827, 'minibatch_frac': 0.8, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 10}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=1.5200 val_loss=1.2472 scale=1.0000 norm=1.0640                    \n",
      "[iter 1] loss=1.1738 val_loss=1.0115 scale=2.0000 norm=1.4887                    \n",
      "[iter 2] loss=0.9699 val_loss=0.8684 scale=2.0000 norm=1.2879                    \n",
      "[iter 3] loss=0.7845 val_loss=0.7624 scale=2.0000 norm=1.2157                    \n",
      "[iter 4] loss=0.6654 val_loss=0.6780 scale=2.0000 norm=1.1972                    \n",
      "[iter 5] loss=0.5381 val_loss=0.6322 scale=2.0000 norm=1.1627                    \n",
      "[iter 6] loss=0.4351 val_loss=0.6360 scale=2.0000 norm=1.1544                    \n",
      "[iter 7] loss=0.3925 val_loss=0.6354 scale=2.0000 norm=1.1647                    \n",
      "[iter 8] loss=0.3743 val_loss=0.6413 scale=1.0000 norm=0.6039                    \n",
      "[iter 9] loss=0.3559 val_loss=0.6318 scale=1.0000 norm=0.6095                    \n",
      "[iter 10] loss=0.3337 val_loss=0.6521 scale=1.0000 norm=0.6006                   \n",
      "[iter 11] loss=0.3264 val_loss=0.6712 scale=1.0000 norm=0.5994                   \n",
      "[iter 12] loss=0.2897 val_loss=0.7110 scale=2.0000 norm=1.1840                   \n",
      "[iter 13] loss=0.2697 val_loss=0.7149 scale=1.0000 norm=0.5961                   \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL9 (val_loss=0.6318)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=5,\n",
      "                      max_features='sqrt', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.41936768031905625, 'minibatch_frac': 0.7, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5211 val_loss=1.2473 scale=1.0000 norm=1.0651                    \n",
      "[iter 1] loss=1.2026 val_loss=0.9947 scale=2.0000 norm=1.5073                    \n",
      "[iter 2] loss=0.9804 val_loss=0.8334 scale=2.0000 norm=1.3573                    \n",
      "[iter 3] loss=0.7705 val_loss=0.8205 scale=2.0000 norm=1.3027                    \n",
      "[iter 4] loss=0.6998 val_loss=0.8296 scale=2.0000 norm=1.3109                    \n",
      "[iter 5] loss=0.6359 val_loss=0.8361 scale=1.0000 norm=0.6635                    \n",
      "[iter 6] loss=0.5762 val_loss=0.8396 scale=1.0000 norm=0.6531                    \n",
      "[iter 7] loss=0.5792 val_loss=0.9118 scale=2.0000 norm=1.2990                    \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL3 (val_loss=0.8205)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=5,\n",
      "                      max_features='log2', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.23134009413866694, 'minibatch_frac': 0.7, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5211 val_loss=1.2965 scale=1.0000 norm=1.0651                    \n",
      "[iter 1] loss=1.2103 val_loss=1.1448 scale=2.0000 norm=1.5733                    \n",
      "[iter 2] loss=1.0656 val_loss=0.9721 scale=2.0000 norm=1.4448                    \n",
      "[iter 3] loss=0.8981 val_loss=0.8597 scale=2.0000 norm=1.3183                    \n",
      "[iter 4] loss=0.7925 val_loss=0.7928 scale=2.0000 norm=1.2812                    \n",
      "[iter 5] loss=0.6826 val_loss=0.7461 scale=2.0000 norm=1.2432                    \n",
      "[iter 6] loss=0.5834 val_loss=0.7109 scale=2.0000 norm=1.2272                    \n",
      "[iter 7] loss=0.5415 val_loss=0.6904 scale=1.0000 norm=0.6144                    \n",
      "[iter 8] loss=0.5305 val_loss=0.6685 scale=1.0000 norm=0.6327                    \n",
      "[iter 9] loss=0.5059 val_loss=0.6701 scale=1.0000 norm=0.6279                    \n",
      "[iter 10] loss=0.4706 val_loss=0.6633 scale=1.0000 norm=0.6190                   \n",
      "[iter 11] loss=0.4666 val_loss=0.6447 scale=1.0000 norm=0.6326                   \n",
      "[iter 12] loss=0.4267 val_loss=0.6479 scale=1.0000 norm=0.6144                   \n",
      "[iter 13] loss=0.4126 val_loss=0.6387 scale=1.0000 norm=0.6157                   \n",
      "[iter 14] loss=0.3953 val_loss=0.6413 scale=1.0000 norm=0.6157                   \n",
      "[iter 15] loss=0.4077 val_loss=0.6403 scale=1.0000 norm=0.6294                   \n",
      "[iter 16] loss=0.3764 val_loss=0.6558 scale=1.0000 norm=0.6241                   \n",
      "[iter 17] loss=0.3798 val_loss=0.6545 scale=1.0000 norm=0.6162                   \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL13 (val_loss=0.6387)                                      \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.11800636603298993, 'minibatch_frac': 1.0, 'n_estimators': 2000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.2955 scale=1.0000 norm=1.0277                    \n",
      "[iter 1] loss=1.2108 val_loss=1.1648 scale=2.0000 norm=1.5828                    \n",
      "[iter 2] loss=1.0741 val_loss=1.0692 scale=2.0000 norm=1.4094                    \n",
      "[iter 3] loss=0.9730 val_loss=0.9738 scale=2.0000 norm=1.3161                    \n",
      "[iter 4] loss=0.8805 val_loss=0.8768 scale=2.0000 norm=1.2539                    \n",
      "[iter 5] loss=0.7944 val_loss=0.8125 scale=2.0000 norm=1.2105                    \n",
      "[iter 6] loss=0.7156 val_loss=0.7380 scale=2.0000 norm=1.1815                    \n",
      "[iter 7] loss=0.6405 val_loss=0.6786 scale=2.0000 norm=1.1560                    \n",
      "[iter 8] loss=0.5693 val_loss=0.6276 scale=2.0000 norm=1.1330                    \n",
      "[iter 9] loss=0.5050 val_loss=0.5948 scale=2.0000 norm=1.1162                    \n",
      "[iter 10] loss=0.4489 val_loss=0.5667 scale=2.0000 norm=1.1053                   \n",
      "[iter 11] loss=0.3969 val_loss=0.5469 scale=2.0000 norm=1.0945                   \n",
      "[iter 12] loss=0.3509 val_loss=0.5417 scale=2.0000 norm=1.0878                   \n",
      "[iter 13] loss=0.3101 val_loss=0.5325 scale=2.0000 norm=1.0823                   \n",
      "[iter 14] loss=0.2737 val_loss=0.5357 scale=2.0000 norm=1.0782                   \n",
      "[iter 15] loss=0.2426 val_loss=0.5390 scale=2.0000 norm=1.0777                   \n",
      "[iter 16] loss=0.2144 val_loss=0.5568 scale=2.0000 norm=1.0763                   \n",
      "[iter 17] loss=0.1886 val_loss=0.5751 scale=2.0000 norm=1.0773                   \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL13 (val_loss=0.5325)                                      \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.75, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.38485689012748203, 'minibatch_frac': 0.8, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 5}}\n",
      "[iter 0] loss=1.5200 val_loss=1.1952 scale=1.0000 norm=1.0640                    \n",
      "[iter 1] loss=1.1483 val_loss=0.9703 scale=2.0000 norm=1.3986                    \n",
      "[iter 2] loss=0.8597 val_loss=0.8135 scale=2.0000 norm=1.2018                    \n",
      "[iter 3] loss=0.6226 val_loss=0.7647 scale=2.0000 norm=1.1707                    \n",
      "[iter 4] loss=0.4793 val_loss=0.8515 scale=2.0000 norm=1.1468                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 5] loss=0.3899 val_loss=0.9666 scale=2.0000 norm=1.1625                    \n",
      "[iter 6] loss=0.3145 val_loss=1.0115 scale=1.0000 norm=0.5932                    \n",
      "[iter 7] loss=0.3221 val_loss=0.9898 scale=1.0000 norm=0.5934                    \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL3 (val_loss=0.7647)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=5,\n",
      "                      max_features='sqrt', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.03327516673636869, 'minibatch_frac': 0.7, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 10}}\n",
      "[iter 0] loss=1.5211 val_loss=1.4783 scale=1.0000 norm=1.0651                    \n",
      "[iter 1] loss=1.3077 val_loss=1.4385 scale=1.0000 norm=0.8740                    \n",
      "[iter 2] loss=1.2651 val_loss=1.4058 scale=1.0000 norm=0.8431                    \n",
      "[iter 3] loss=1.2430 val_loss=1.3611 scale=1.0000 norm=0.8188                    \n",
      "[iter 4] loss=1.2189 val_loss=1.3325 scale=1.0000 norm=0.8115                    \n",
      "[iter 5] loss=1.1826 val_loss=1.2809 scale=1.0000 norm=0.7861                    \n",
      "[iter 6] loss=1.1625 val_loss=1.2591 scale=1.0000 norm=0.7671                    \n",
      "[iter 7] loss=1.1548 val_loss=1.2471 scale=1.0000 norm=0.7705                    \n",
      "[iter 8] loss=1.1436 val_loss=1.2107 scale=2.0000 norm=1.5343                    \n",
      "[iter 9] loss=1.1107 val_loss=1.1849 scale=2.0000 norm=1.4849                    \n",
      "[iter 10] loss=1.0777 val_loss=1.1551 scale=2.0000 norm=1.4514                   \n",
      "[iter 11] loss=1.0604 val_loss=1.1440 scale=1.0000 norm=0.7246                   \n",
      "[iter 12] loss=1.0438 val_loss=1.1186 scale=2.0000 norm=1.4165                   \n",
      "[iter 13] loss=1.0094 val_loss=1.0930 scale=2.0000 norm=1.3908                   \n",
      "[iter 14] loss=1.0013 val_loss=1.0688 scale=2.0000 norm=1.3982                   \n",
      "[iter 15] loss=0.9695 val_loss=1.0466 scale=2.0000 norm=1.3672                   \n",
      "[iter 16] loss=0.9491 val_loss=1.0268 scale=2.0000 norm=1.3580                   \n",
      "[iter 17] loss=0.9300 val_loss=1.0070 scale=2.0000 norm=1.3529                   \n",
      "[iter 18] loss=0.9037 val_loss=0.9886 scale=2.0000 norm=1.3282                   \n",
      "[iter 19] loss=0.8814 val_loss=0.9725 scale=2.0000 norm=1.3139                   \n",
      "[iter 20] loss=0.8642 val_loss=0.9519 scale=2.0000 norm=1.3019                   \n",
      "[iter 21] loss=0.8486 val_loss=0.9340 scale=2.0000 norm=1.3088                   \n",
      "[iter 22] loss=0.8248 val_loss=0.9144 scale=2.0000 norm=1.2899                   \n",
      "[iter 23] loss=0.7935 val_loss=0.8957 scale=2.0000 norm=1.2535                   \n",
      "[iter 24] loss=0.7758 val_loss=0.8802 scale=2.0000 norm=1.2521                   \n",
      "[iter 25] loss=0.7571 val_loss=0.8636 scale=2.0000 norm=1.2495                   \n",
      "[iter 26] loss=0.7509 val_loss=0.8513 scale=2.0000 norm=1.2631                   \n",
      "[iter 27] loss=0.7214 val_loss=0.8379 scale=2.0000 norm=1.2303                   \n",
      "[iter 28] loss=0.7138 val_loss=0.8244 scale=2.0000 norm=1.2436                   \n",
      "[iter 29] loss=0.6930 val_loss=0.8111 scale=2.0000 norm=1.2325                   \n",
      "[iter 30] loss=0.6786 val_loss=0.7968 scale=2.0000 norm=1.2338                   \n",
      "[iter 31] loss=0.6568 val_loss=0.7846 scale=2.0000 norm=1.2173                   \n",
      "[iter 32] loss=0.6420 val_loss=0.7718 scale=2.0000 norm=1.2237                   \n",
      "[iter 33] loss=0.6279 val_loss=0.7603 scale=2.0000 norm=1.2152                   \n",
      "[iter 34] loss=0.6092 val_loss=0.7462 scale=2.0000 norm=1.2002                   \n",
      "[iter 35] loss=0.5961 val_loss=0.7380 scale=2.0000 norm=1.2004                   \n",
      "[iter 36] loss=0.5791 val_loss=0.7287 scale=2.0000 norm=1.1934                   \n",
      "[iter 37] loss=0.5651 val_loss=0.7163 scale=2.0000 norm=1.1898                   \n",
      "[iter 38] loss=0.5417 val_loss=0.7062 scale=2.0000 norm=1.1658                   \n",
      "[iter 39] loss=0.5382 val_loss=0.7002 scale=2.0000 norm=1.1882                   \n",
      "[iter 40] loss=0.5296 val_loss=0.6942 scale=2.0000 norm=1.1950                   \n",
      "[iter 41] loss=0.5060 val_loss=0.6855 scale=2.0000 norm=1.1701                   \n",
      "[iter 42] loss=0.4934 val_loss=0.6776 scale=2.0000 norm=1.1717                   \n",
      "[iter 43] loss=0.4817 val_loss=0.6717 scale=1.0000 norm=0.5843                   \n",
      "[iter 44] loss=0.4805 val_loss=0.6677 scale=2.0000 norm=1.1742                   \n",
      "[iter 45] loss=0.4800 val_loss=0.6640 scale=1.0000 norm=0.5923                   \n",
      "[iter 46] loss=0.4642 val_loss=0.6592 scale=2.0000 norm=1.1806                   \n",
      "[iter 47] loss=0.4573 val_loss=0.6570 scale=1.0000 norm=0.5886                   \n",
      "[iter 48] loss=0.4466 val_loss=0.6490 scale=2.0000 norm=1.1665                   \n",
      "[iter 49] loss=0.4373 val_loss=0.6446 scale=1.0000 norm=0.5855                   \n",
      "[iter 50] loss=0.4354 val_loss=0.6413 scale=2.0000 norm=1.1761                   \n",
      "[iter 51] loss=0.4240 val_loss=0.6399 scale=1.0000 norm=0.5845                   \n",
      "[iter 52] loss=0.4128 val_loss=0.6377 scale=2.0000 norm=1.1521                   \n",
      "[iter 53] loss=0.4064 val_loss=0.6358 scale=1.0000 norm=0.5837                   \n",
      "[iter 54] loss=0.3977 val_loss=0.6340 scale=1.0000 norm=0.5786                   \n",
      "[iter 55] loss=0.4037 val_loss=0.6308 scale=1.0000 norm=0.5887                   \n",
      "[iter 56] loss=0.3955 val_loss=0.6302 scale=1.0000 norm=0.5874                   \n",
      "[iter 57] loss=0.3882 val_loss=0.6276 scale=1.0000 norm=0.5800                   \n",
      "[iter 58] loss=0.3814 val_loss=0.6245 scale=2.0000 norm=1.1613                   \n",
      "[iter 59] loss=0.3852 val_loss=0.6219 scale=2.0000 norm=1.1786                   \n",
      "[iter 60] loss=0.3742 val_loss=0.6219 scale=2.0000 norm=1.1710                   \n",
      "[iter 61] loss=0.3763 val_loss=0.6195 scale=1.0000 norm=0.5925                   \n",
      "[iter 62] loss=0.3625 val_loss=0.6194 scale=2.0000 norm=1.1644                   \n",
      "[iter 63] loss=0.3563 val_loss=0.6171 scale=2.0000 norm=1.1695                   \n",
      "[iter 64] loss=0.3383 val_loss=0.6137 scale=1.0000 norm=0.5767                   \n",
      "[iter 65] loss=0.3423 val_loss=0.6119 scale=1.0000 norm=0.5840                   \n",
      "[iter 66] loss=0.3285 val_loss=0.6134 scale=2.0000 norm=1.1529                   \n",
      "[iter 67] loss=0.3385 val_loss=0.6121 scale=1.0000 norm=0.5870                   \n",
      "[iter 68] loss=0.3276 val_loss=0.6123 scale=1.0000 norm=0.5795                   \n",
      "[iter 69] loss=0.3389 val_loss=0.6109 scale=1.0000 norm=0.5884                   \n",
      "[iter 70] loss=0.3291 val_loss=0.6109 scale=1.0000 norm=0.5833                   \n",
      "[iter 71] loss=0.3193 val_loss=0.6100 scale=1.0000 norm=0.5865                   \n",
      "[iter 72] loss=0.3111 val_loss=0.6082 scale=2.0000 norm=1.1567                   \n",
      "[iter 73] loss=0.3071 val_loss=0.6070 scale=1.0000 norm=0.5820                   \n",
      "[iter 74] loss=0.3022 val_loss=0.6069 scale=1.0000 norm=0.5750                   \n",
      "[iter 75] loss=0.3070 val_loss=0.6064 scale=1.0000 norm=0.5823                   \n",
      "[iter 76] loss=0.3079 val_loss=0.6056 scale=1.0000 norm=0.5896                   \n",
      "[iter 77] loss=0.3066 val_loss=0.6062 scale=1.0000 norm=0.5860                   \n",
      "[iter 78] loss=0.3060 val_loss=0.6059 scale=1.0000 norm=0.5861                   \n",
      "[iter 79] loss=0.2911 val_loss=0.6040 scale=1.0000 norm=0.5827                   \n",
      "[iter 80] loss=0.2824 val_loss=0.6022 scale=1.0000 norm=0.5749                   \n",
      "[iter 81] loss=0.2873 val_loss=0.6011 scale=1.0000 norm=0.5845                   \n",
      "[iter 82] loss=0.2869 val_loss=0.6008 scale=1.0000 norm=0.5868                   \n",
      "[iter 83] loss=0.2789 val_loss=0.6017 scale=1.0000 norm=0.5754                   \n",
      "[iter 84] loss=0.2686 val_loss=0.6023 scale=1.0000 norm=0.5740                   \n",
      "[iter 85] loss=0.2665 val_loss=0.6027 scale=1.0000 norm=0.5706                   \n",
      "[iter 86] loss=0.2837 val_loss=0.6036 scale=1.0000 norm=0.5881                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL82 (val_loss=0.6008)                                      \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.75, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.046547118706627026, 'minibatch_frac': 0.8, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 5}}\n",
      "[iter 0] loss=1.5200 val_loss=1.3900 scale=1.0000 norm=1.0640                     \n",
      "[iter 1] loss=1.2622 val_loss=1.3027 scale=2.0000 norm=1.6734                     \n",
      "[iter 2] loss=1.1701 val_loss=1.2297 scale=2.0000 norm=1.5409                     \n",
      "[iter 3] loss=1.1112 val_loss=1.1713 scale=2.0000 norm=1.4594                     \n",
      "[iter 4] loss=1.0723 val_loss=1.1295 scale=2.0000 norm=1.4251                     \n",
      "[iter 5] loss=1.0260 val_loss=1.0881 scale=2.0000 norm=1.3820                     \n",
      "[iter 6] loss=0.9798 val_loss=1.0524 scale=2.0000 norm=1.3311                     \n",
      "[iter 7] loss=0.9435 val_loss=1.0181 scale=2.0000 norm=1.3093                     \n",
      "[iter 8] loss=0.9165 val_loss=0.9846 scale=2.0000 norm=1.2977                     \n",
      "[iter 9] loss=0.8787 val_loss=0.9558 scale=2.0000 norm=1.2636                     \n",
      "[iter 10] loss=0.8384 val_loss=0.9270 scale=2.0000 norm=1.2403                    \n",
      "[iter 11] loss=0.8094 val_loss=0.8965 scale=2.0000 norm=1.2330                    \n",
      "[iter 12] loss=0.7700 val_loss=0.8682 scale=2.0000 norm=1.1996                    \n",
      "[iter 13] loss=0.7352 val_loss=0.8417 scale=2.0000 norm=1.1869                    \n",
      "[iter 14] loss=0.7071 val_loss=0.8159 scale=2.0000 norm=1.1810                    \n",
      "[iter 15] loss=0.6779 val_loss=0.7922 scale=2.0000 norm=1.1711                    \n",
      "[iter 16] loss=0.6456 val_loss=0.7679 scale=2.0000 norm=1.1590                    \n",
      "[iter 17] loss=0.6261 val_loss=0.7451 scale=2.0000 norm=1.1613                    \n",
      "[iter 18] loss=0.5959 val_loss=0.7266 scale=2.0000 norm=1.1486                    \n",
      "[iter 19] loss=0.5678 val_loss=0.7057 scale=2.0000 norm=1.1353                    \n",
      "[iter 20] loss=0.5415 val_loss=0.6891 scale=2.0000 norm=1.1287                    \n",
      "[iter 21] loss=0.5190 val_loss=0.6710 scale=2.0000 norm=1.1245                    \n",
      "[iter 22] loss=0.4930 val_loss=0.6562 scale=2.0000 norm=1.1197                    \n",
      "[iter 23] loss=0.4627 val_loss=0.6429 scale=2.0000 norm=1.0988                    \n",
      "[iter 24] loss=0.4470 val_loss=0.6315 scale=2.0000 norm=1.1026                    \n",
      "[iter 25] loss=0.4240 val_loss=0.6214 scale=2.0000 norm=1.1036                    \n",
      "[iter 26] loss=0.4048 val_loss=0.6107 scale=2.0000 norm=1.0971                    \n",
      "[iter 27] loss=0.3805 val_loss=0.6035 scale=2.0000 norm=1.0808                    \n",
      "[iter 28] loss=0.3639 val_loss=0.5939 scale=2.0000 norm=1.0846                    \n",
      "[iter 29] loss=0.3435 val_loss=0.5875 scale=2.0000 norm=1.0745                    \n",
      "[iter 30] loss=0.3254 val_loss=0.5858 scale=2.0000 norm=1.0729                    \n",
      "[iter 31] loss=0.3149 val_loss=0.5829 scale=2.0000 norm=1.0751                    \n",
      "[iter 32] loss=0.2944 val_loss=0.5777 scale=2.0000 norm=1.0757                    \n",
      "[iter 33] loss=0.2782 val_loss=0.5755 scale=2.0000 norm=1.0632                    \n",
      "[iter 34] loss=0.2610 val_loss=0.5837 scale=2.0000 norm=1.0636                    \n",
      "[iter 35] loss=0.2488 val_loss=0.5819 scale=2.0000 norm=1.0617                    \n",
      "[iter 36] loss=0.2341 val_loss=0.5859 scale=2.0000 norm=1.0571                    \n",
      "[iter 37] loss=0.2272 val_loss=0.5890 scale=2.0000 norm=1.0610                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL33 (val_loss=0.5755)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=5,\n",
      "                      max_features='log2', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.05598091363949317, 'minibatch_frac': 1.0, 'n_estimators': 2000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.4822 scale=1.0000 norm=1.0277                     \n",
      "[iter 1] loss=1.2823 val_loss=1.3661 scale=1.0000 norm=0.8523                     \n",
      "[iter 2] loss=1.2197 val_loss=1.2925 scale=1.0000 norm=0.8046                     \n",
      "[iter 3] loss=1.1866 val_loss=1.2592 scale=1.0000 norm=0.7831                     \n",
      "[iter 4] loss=1.1600 val_loss=1.2288 scale=1.0000 norm=0.7675                     \n",
      "[iter 5] loss=1.1374 val_loss=1.2082 scale=1.0000 norm=0.7546                     \n",
      "[iter 6] loss=1.1156 val_loss=1.1728 scale=2.0000 norm=1.4864                     \n",
      "[iter 7] loss=1.0805 val_loss=1.1306 scale=2.0000 norm=1.4573                     \n",
      "[iter 8] loss=1.0414 val_loss=1.0847 scale=2.0000 norm=1.4268                     \n",
      "[iter 9] loss=0.9954 val_loss=1.0436 scale=2.0000 norm=1.3861                     \n",
      "[iter 10] loss=0.9591 val_loss=1.0107 scale=2.0000 norm=1.3634                    \n",
      "[iter 11] loss=0.9216 val_loss=0.9804 scale=2.0000 norm=1.3395                    \n",
      "[iter 12] loss=0.8868 val_loss=0.9484 scale=2.0000 norm=1.3208                    \n",
      "[iter 13] loss=0.8531 val_loss=0.9198 scale=2.0000 norm=1.3031                    \n",
      "[iter 14] loss=0.8210 val_loss=0.8894 scale=2.0000 norm=1.2882                    \n",
      "[iter 15] loss=0.7871 val_loss=0.8655 scale=2.0000 norm=1.2715                    \n",
      "[iter 16] loss=0.7584 val_loss=0.8384 scale=2.0000 norm=1.2618                    \n",
      "[iter 17] loss=0.7273 val_loss=0.8166 scale=2.0000 norm=1.2488                    \n",
      "[iter 18] loss=0.7000 val_loss=0.7945 scale=2.0000 norm=1.2418                    \n",
      "[iter 19] loss=0.6714 val_loss=0.7753 scale=2.0000 norm=1.2310                    \n",
      "[iter 20] loss=0.6434 val_loss=0.7555 scale=2.0000 norm=1.2204                    \n",
      "[iter 21] loss=0.6149 val_loss=0.7366 scale=2.0000 norm=1.2093                    \n",
      "[iter 22] loss=0.5894 val_loss=0.7200 scale=2.0000 norm=1.2030                    \n",
      "[iter 23] loss=0.5659 val_loss=0.7047 scale=2.0000 norm=1.1979                    \n",
      "[iter 24] loss=0.5409 val_loss=0.6919 scale=2.0000 norm=1.1900                    \n",
      "[iter 25] loss=0.5186 val_loss=0.6800 scale=2.0000 norm=1.1849                    \n",
      "[iter 26] loss=0.4985 val_loss=0.6689 scale=2.0000 norm=1.1820                    \n",
      "[iter 27] loss=0.4780 val_loss=0.6572 scale=2.0000 norm=1.1781                    \n",
      "[iter 28] loss=0.4603 val_loss=0.6529 scale=1.0000 norm=0.5882                    \n",
      "[iter 29] loss=0.4514 val_loss=0.6445 scale=2.0000 norm=1.1751                    \n",
      "[iter 30] loss=0.4345 val_loss=0.6389 scale=2.0000 norm=1.1736                    \n",
      "[iter 31] loss=0.4202 val_loss=0.6345 scale=1.0000 norm=0.5871                    \n",
      "[iter 32] loss=0.4131 val_loss=0.6299 scale=1.0000 norm=0.5872                    \n",
      "[iter 33] loss=0.4052 val_loss=0.6189 scale=2.0000 norm=1.1731                    \n",
      "[iter 34] loss=0.3896 val_loss=0.6164 scale=1.0000 norm=0.5848                    \n",
      "[iter 35] loss=0.3831 val_loss=0.6138 scale=1.0000 norm=0.5851                    \n",
      "[iter 36] loss=0.3768 val_loss=0.6142 scale=1.0000 norm=0.5853                    \n",
      "[iter 37] loss=0.3701 val_loss=0.6132 scale=1.0000 norm=0.5853                    \n",
      "[iter 38] loss=0.3640 val_loss=0.6110 scale=1.0000 norm=0.5854                    \n",
      "[iter 39] loss=0.3569 val_loss=0.6095 scale=1.0000 norm=0.5847                    \n",
      "[iter 40] loss=0.3513 val_loss=0.6114 scale=2.0000 norm=1.1699                    \n",
      "[iter 41] loss=0.3410 val_loss=0.6101 scale=1.0000 norm=0.5856                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 42] loss=0.3355 val_loss=0.6090 scale=1.0000 norm=0.5854                    \n",
      "[iter 43] loss=0.3297 val_loss=0.6083 scale=1.0000 norm=0.5851                    \n",
      "[iter 44] loss=0.3240 val_loss=0.6069 scale=1.0000 norm=0.5847                    \n",
      "[iter 45] loss=0.3185 val_loss=0.6062 scale=1.0000 norm=0.5845                    \n",
      "[iter 46] loss=0.3131 val_loss=0.6075 scale=1.0000 norm=0.5842                    \n",
      "[iter 47] loss=0.3087 val_loss=0.6058 scale=1.0000 norm=0.5844                    \n",
      "[iter 48] loss=0.3040 val_loss=0.6069 scale=1.0000 norm=0.5845                    \n",
      "[iter 49] loss=0.2999 val_loss=0.6081 scale=2.0000 norm=1.1702                    \n",
      "[iter 50] loss=0.2930 val_loss=0.6012 scale=1.0000 norm=0.5870                    \n",
      "[iter 51] loss=0.2883 val_loss=0.6028 scale=1.0000 norm=0.5870                    \n",
      "[iter 52] loss=0.2843 val_loss=0.6035 scale=1.0000 norm=0.5869                    \n",
      "[iter 53] loss=0.2800 val_loss=0.6041 scale=1.0000 norm=0.5871                    \n",
      "[iter 54] loss=0.2760 val_loss=0.6038 scale=1.0000 norm=0.5876                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL50 (val_loss=0.6012)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=1,\n",
      "                      max_features='log2', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.2617387462204275, 'minibatch_frac': 0.7, 'n_estimators': 1000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5211 val_loss=1.4542 scale=1.0000 norm=1.0651                     \n",
      "[iter 1] loss=1.4201 val_loss=1.4555 scale=2.0000 norm=1.9372                     \n",
      "[iter 2] loss=1.3963 val_loss=1.4253 scale=2.0000 norm=1.9375                     \n",
      "[iter 3] loss=1.3200 val_loss=1.4342 scale=2.0000 norm=1.8314                     \n",
      "[iter 4] loss=1.3950 val_loss=1.4444 scale=1.0000 norm=1.0203                     \n",
      "[iter 5] loss=1.3592 val_loss=1.4392 scale=1.0000 norm=0.9820                     \n",
      "[iter 6] loss=1.2912 val_loss=1.4216 scale=2.0000 norm=1.8391                     \n",
      "[iter 7] loss=1.3702 val_loss=1.4358 scale=1.0000 norm=1.0192                     \n",
      "[iter 8] loss=1.3526 val_loss=1.4472 scale=1.0000 norm=0.9971                     \n",
      "[iter 9] loss=1.3465 val_loss=1.4464 scale=1.0000 norm=0.9830                     \n",
      "[iter 10] loss=1.3272 val_loss=1.4410 scale=2.0000 norm=1.9335                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL6 (val_loss=1.4216)                                        \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features='sqrt', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.03863644837735789, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.4714 scale=1.0000 norm=1.0277                     \n",
      "[iter 1] loss=1.3041 val_loss=1.4139 scale=1.0000 norm=0.8698                     \n",
      "[iter 2] loss=1.2474 val_loss=1.3120 scale=2.0000 norm=1.6512                     \n",
      "[iter 3] loss=1.1763 val_loss=1.2532 scale=2.0000 norm=1.5519                     \n",
      "[iter 4] loss=1.1311 val_loss=1.2106 scale=2.0000 norm=1.4980                     \n",
      "[iter 5] loss=1.0951 val_loss=1.1733 scale=2.0000 norm=1.4588                     \n",
      "[iter 6] loss=1.0619 val_loss=1.1408 scale=2.0000 norm=1.4266                     \n",
      "[iter 7] loss=1.0314 val_loss=1.1072 scale=2.0000 norm=1.4004                     \n",
      "[iter 8] loss=1.0010 val_loss=1.0803 scale=2.0000 norm=1.3756                     \n",
      "[iter 9] loss=0.9745 val_loss=1.0549 scale=2.0000 norm=1.3572                     \n",
      "[iter 10] loss=0.9469 val_loss=1.0269 scale=2.0000 norm=1.3379                    \n",
      "[iter 11] loss=0.9181 val_loss=1.0024 scale=2.0000 norm=1.3181                    \n",
      "[iter 12] loss=0.8909 val_loss=0.9777 scale=2.0000 norm=1.3017                    \n",
      "[iter 13] loss=0.8639 val_loss=0.9530 scale=2.0000 norm=1.2866                    \n",
      "[iter 14] loss=0.8363 val_loss=0.9297 scale=2.0000 norm=1.2692                    \n",
      "[iter 15] loss=0.8103 val_loss=0.9095 scale=2.0000 norm=1.2559                    \n",
      "[iter 16] loss=0.7858 val_loss=0.8902 scale=2.0000 norm=1.2446                    \n",
      "[iter 17] loss=0.7619 val_loss=0.8697 scale=2.0000 norm=1.2339                    \n",
      "[iter 18] loss=0.7364 val_loss=0.8498 scale=2.0000 norm=1.2214                    \n",
      "[iter 19] loss=0.7121 val_loss=0.8296 scale=2.0000 norm=1.2116                    \n",
      "[iter 20] loss=0.6885 val_loss=0.8129 scale=2.0000 norm=1.2026                    \n",
      "[iter 21] loss=0.6660 val_loss=0.7964 scale=2.0000 norm=1.1938                    \n",
      "[iter 22] loss=0.6448 val_loss=0.7797 scale=2.0000 norm=1.1871                    \n",
      "[iter 23] loss=0.6237 val_loss=0.7653 scale=2.0000 norm=1.1803                    \n",
      "[iter 24] loss=0.6038 val_loss=0.7491 scale=2.0000 norm=1.1746                    \n",
      "[iter 25] loss=0.5826 val_loss=0.7355 scale=2.0000 norm=1.1672                    \n",
      "[iter 26] loss=0.5625 val_loss=0.7214 scale=2.0000 norm=1.1605                    \n",
      "[iter 27] loss=0.5424 val_loss=0.7096 scale=2.0000 norm=1.1543                    \n",
      "[iter 28] loss=0.5243 val_loss=0.6962 scale=2.0000 norm=1.1499                    \n",
      "[iter 29] loss=0.5063 val_loss=0.6852 scale=2.0000 norm=1.1451                    \n",
      "[iter 30] loss=0.4881 val_loss=0.6724 scale=2.0000 norm=1.1405                    \n",
      "[iter 31] loss=0.4713 val_loss=0.6619 scale=2.0000 norm=1.1368                    \n",
      "[iter 32] loss=0.4541 val_loss=0.6511 scale=2.0000 norm=1.1328                    \n",
      "[iter 33] loss=0.4377 val_loss=0.6331 scale=2.0000 norm=1.1287                    \n",
      "[iter 34] loss=0.4223 val_loss=0.6237 scale=2.0000 norm=1.1254                    \n",
      "[iter 35] loss=0.4056 val_loss=0.6152 scale=2.0000 norm=1.1202                    \n",
      "[iter 36] loss=0.3879 val_loss=0.6094 scale=2.0000 norm=1.1127                    \n",
      "[iter 37] loss=0.3729 val_loss=0.6045 scale=2.0000 norm=1.1094                    \n",
      "[iter 38] loss=0.3585 val_loss=0.6009 scale=2.0000 norm=1.1065                    \n",
      "[iter 39] loss=0.3452 val_loss=0.5972 scale=2.0000 norm=1.1046                    \n",
      "[iter 40] loss=0.3322 val_loss=0.5953 scale=2.0000 norm=1.1026                    \n",
      "[iter 41] loss=0.3199 val_loss=0.5928 scale=1.0000 norm=0.5507                    \n",
      "[iter 42] loss=0.3139 val_loss=0.5902 scale=2.0000 norm=1.1009                    \n",
      "[iter 43] loss=0.3028 val_loss=0.5867 scale=2.0000 norm=1.1008                    \n",
      "[iter 44] loss=0.2921 val_loss=0.5846 scale=2.0000 norm=1.1001                    \n",
      "[iter 45] loss=0.2817 val_loss=0.5813 scale=2.0000 norm=1.0995                    \n",
      "[iter 46] loss=0.2714 val_loss=0.5794 scale=2.0000 norm=1.0986                    \n",
      "[iter 47] loss=0.2619 val_loss=0.5809 scale=2.0000 norm=1.0986                    \n",
      "[iter 48] loss=0.2533 val_loss=0.5771 scale=2.0000 norm=1.0991                    \n",
      "[iter 49] loss=0.2450 val_loss=0.5765 scale=1.0000 norm=0.5498                    \n",
      "[iter 50] loss=0.2409 val_loss=0.5757 scale=1.0000 norm=0.5498                    \n",
      "[iter 51] loss=0.2367 val_loss=0.5748 scale=2.0000 norm=1.0997                    \n",
      "[iter 52] loss=0.2294 val_loss=0.5736 scale=2.0000 norm=1.1000                    \n",
      "[iter 53] loss=0.2227 val_loss=0.5727 scale=1.0000 norm=0.5508                    \n",
      "[iter 54] loss=0.2189 val_loss=0.5728 scale=1.0000 norm=0.5508                    \n",
      "[iter 55] loss=0.2155 val_loss=0.5728 scale=1.0000 norm=0.5511                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 56] loss=0.2118 val_loss=0.5732 scale=1.0000 norm=0.5511                    \n",
      "[iter 57] loss=0.2082 val_loss=0.5741 scale=1.0000 norm=0.5509                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL53 (val_loss=0.5727)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=1,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.01173921864663752, 'minibatch_frac': 0.8, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5200 val_loss=1.5817 scale=1.0000 norm=1.0640                     \n",
      "[iter 1] loss=1.4567 val_loss=1.5314 scale=1.0000 norm=1.0024                     \n",
      "[iter 2] loss=1.4038 val_loss=1.5051 scale=1.0000 norm=0.9533                     \n",
      "[iter 3] loss=1.3639 val_loss=1.4855 scale=1.0000 norm=0.9138                     \n",
      "[iter 4] loss=1.3563 val_loss=1.4703 scale=1.0000 norm=0.9119                     \n",
      "[iter 5] loss=1.3549 val_loss=1.4561 scale=1.0000 norm=0.9135                     \n",
      "[iter 6] loss=1.3228 val_loss=1.4424 scale=1.0000 norm=0.8819                     \n",
      "[iter 7] loss=1.3206 val_loss=1.4301 scale=1.0000 norm=0.8854                     \n",
      "[iter 8] loss=1.3112 val_loss=1.4195 scale=1.0000 norm=0.8788                     \n",
      "[iter 9] loss=1.2939 val_loss=1.3982 scale=2.0000 norm=1.7255                     \n",
      "[iter 10] loss=1.2696 val_loss=1.3859 scale=2.0000 norm=1.6849                    \n",
      "[iter 11] loss=1.2776 val_loss=1.3675 scale=2.0000 norm=1.7113                    \n",
      "[iter 12] loss=1.2384 val_loss=1.3529 scale=2.0000 norm=1.6382                    \n",
      "[iter 13] loss=1.2318 val_loss=1.3398 scale=2.0000 norm=1.6387                    \n",
      "[iter 14] loss=1.2392 val_loss=1.3265 scale=2.0000 norm=1.6554                    \n",
      "[iter 15] loss=1.2264 val_loss=1.3162 scale=2.0000 norm=1.6411                    \n",
      "[iter 16] loss=1.2107 val_loss=1.3066 scale=2.0000 norm=1.6133                    \n",
      "[iter 17] loss=1.2262 val_loss=1.2955 scale=2.0000 norm=1.6601                    \n",
      "[iter 18] loss=1.2072 val_loss=1.2858 scale=2.0000 norm=1.6307                    \n",
      "[iter 19] loss=1.1971 val_loss=1.2809 scale=1.0000 norm=0.8071                    \n",
      "[iter 20] loss=1.1774 val_loss=1.2737 scale=2.0000 norm=1.5777                    \n",
      "[iter 21] loss=1.1809 val_loss=1.2665 scale=2.0000 norm=1.5946                    \n",
      "[iter 22] loss=1.1868 val_loss=1.2618 scale=1.0000 norm=0.8033                    \n",
      "[iter 23] loss=1.1616 val_loss=1.2535 scale=2.0000 norm=1.5601                    \n",
      "[iter 24] loss=1.1703 val_loss=1.2468 scale=2.0000 norm=1.5916                    \n",
      "[iter 25] loss=1.1654 val_loss=1.2426 scale=1.0000 norm=0.7939                    \n",
      "[iter 26] loss=1.1659 val_loss=1.2351 scale=2.0000 norm=1.5906                    \n",
      "[iter 27] loss=1.1371 val_loss=1.2297 scale=2.0000 norm=1.5380                    \n",
      "[iter 28] loss=1.1460 val_loss=1.2259 scale=1.0000 norm=0.7827                    \n",
      "[iter 29] loss=1.1350 val_loss=1.2210 scale=2.0000 norm=1.5487                    \n",
      "[iter 30] loss=1.1387 val_loss=1.2143 scale=2.0000 norm=1.5619                    \n",
      "[iter 31] loss=1.1313 val_loss=1.2101 scale=2.0000 norm=1.5483                    \n",
      "[iter 32] loss=1.1314 val_loss=1.2052 scale=2.0000 norm=1.5621                    \n",
      "[iter 33] loss=1.1196 val_loss=1.1990 scale=2.0000 norm=1.5467                    \n",
      "[iter 34] loss=1.1165 val_loss=1.1957 scale=1.0000 norm=0.7706                    \n",
      "[iter 35] loss=1.1164 val_loss=1.1916 scale=2.0000 norm=1.5460                    \n",
      "[iter 36] loss=1.1109 val_loss=1.1872 scale=2.0000 norm=1.5442                    \n",
      "[iter 37] loss=1.1097 val_loss=1.1840 scale=1.0000 norm=0.7734                    \n",
      "[iter 38] loss=1.0963 val_loss=1.1811 scale=1.0000 norm=0.7582                    \n",
      "[iter 39] loss=1.0976 val_loss=1.1774 scale=2.0000 norm=1.5306                    \n",
      "[iter 40] loss=1.0989 val_loss=1.1742 scale=1.0000 norm=0.7706                    \n",
      "[iter 41] loss=1.0788 val_loss=1.1713 scale=1.0000 norm=0.7503                    \n",
      "[iter 42] loss=1.0850 val_loss=1.1683 scale=1.0000 norm=0.7561                    \n",
      "[iter 43] loss=1.0873 val_loss=1.1656 scale=1.0000 norm=0.7616                    \n",
      "[iter 44] loss=1.0789 val_loss=1.1618 scale=2.0000 norm=1.5128                    \n",
      "[iter 45] loss=1.0875 val_loss=1.1586 scale=2.0000 norm=1.5389                    \n",
      "[iter 46] loss=1.0777 val_loss=1.1557 scale=1.0000 norm=0.7624                    \n",
      "[iter 47] loss=1.0691 val_loss=1.1520 scale=2.0000 norm=1.5142                    \n",
      "[iter 48] loss=1.0598 val_loss=1.1494 scale=1.0000 norm=0.7440                    \n",
      "[iter 49] loss=1.0632 val_loss=1.1467 scale=1.0000 norm=0.7554                    \n",
      "[iter 50] loss=1.0609 val_loss=1.1429 scale=2.0000 norm=1.5100                    \n",
      "[iter 51] loss=1.0611 val_loss=1.1397 scale=2.0000 norm=1.5133                    \n",
      "[iter 52] loss=1.0396 val_loss=1.1372 scale=1.0000 norm=0.7369                    \n",
      "[iter 53] loss=1.0524 val_loss=1.1325 scale=2.0000 norm=1.5052                    \n",
      "[iter 54] loss=1.0361 val_loss=1.1300 scale=1.0000 norm=0.7421                    \n",
      "[iter 55] loss=1.0512 val_loss=1.1257 scale=2.0000 norm=1.5163                    \n",
      "[iter 56] loss=1.0407 val_loss=1.1215 scale=2.0000 norm=1.5075                    \n",
      "[iter 57] loss=1.0387 val_loss=1.1183 scale=2.0000 norm=1.5054                    \n",
      "[iter 58] loss=1.0297 val_loss=1.1159 scale=1.0000 norm=0.7445                    \n",
      "[iter 59] loss=1.0247 val_loss=1.1135 scale=1.0000 norm=0.7419                    \n",
      "[iter 60] loss=1.0295 val_loss=1.1104 scale=1.0000 norm=0.7475                    \n",
      "[iter 61] loss=1.0178 val_loss=1.1078 scale=2.0000 norm=1.4799                    \n",
      "[iter 62] loss=1.0108 val_loss=1.1055 scale=1.0000 norm=0.7339                    \n",
      "[iter 63] loss=1.0109 val_loss=1.1029 scale=2.0000 norm=1.4737                    \n",
      "[iter 64] loss=0.9974 val_loss=1.1007 scale=1.0000 norm=0.7270                    \n",
      "[iter 65] loss=1.0146 val_loss=1.0983 scale=2.0000 norm=1.4950                    \n",
      "[iter 66] loss=1.0044 val_loss=1.0962 scale=1.0000 norm=0.7372                    \n",
      "[iter 67] loss=1.0012 val_loss=1.0933 scale=2.0000 norm=1.4785                    \n",
      "[iter 68] loss=0.9969 val_loss=1.0912 scale=1.0000 norm=0.7307                    \n",
      "[iter 69] loss=1.0055 val_loss=1.0882 scale=1.0000 norm=0.7436                    \n",
      "[iter 70] loss=0.9888 val_loss=1.0862 scale=1.0000 norm=0.7306                    \n",
      "[iter 71] loss=0.9954 val_loss=1.0822 scale=2.0000 norm=1.4801                    \n",
      "[iter 72] loss=0.9873 val_loss=1.0793 scale=1.0000 norm=0.7339                    \n",
      "[iter 73] loss=0.9827 val_loss=1.0774 scale=1.0000 norm=0.7328                    \n",
      "[iter 74] loss=0.9893 val_loss=1.0749 scale=1.0000 norm=0.7429                    \n",
      "[iter 75] loss=0.9886 val_loss=1.0729 scale=1.0000 norm=0.7394                    \n",
      "[iter 76] loss=0.9937 val_loss=1.0703 scale=2.0000 norm=1.4961                    \n",
      "[iter 77] loss=0.9785 val_loss=1.0685 scale=1.0000 norm=0.7326                    \n",
      "[iter 78] loss=0.9816 val_loss=1.0657 scale=1.0000 norm=0.7395                    \n",
      "[iter 79] loss=0.9764 val_loss=1.0633 scale=2.0000 norm=1.4799                    \n",
      "[iter 80] loss=0.9607 val_loss=1.0610 scale=2.0000 norm=1.4484                    \n",
      "[iter 81] loss=0.9674 val_loss=1.0571 scale=2.0000 norm=1.4674                    \n",
      "[iter 82] loss=0.9633 val_loss=1.0553 scale=1.0000 norm=0.7300                    \n",
      "[iter 83] loss=0.9583 val_loss=1.0532 scale=1.0000 norm=0.7255                    \n",
      "[iter 84] loss=0.9545 val_loss=1.0496 scale=2.0000 norm=1.4564                    \n",
      "[iter 85] loss=0.9373 val_loss=1.0479 scale=1.0000 norm=0.7110                    \n",
      "[iter 86] loss=0.9564 val_loss=1.0450 scale=2.0000 norm=1.4649                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 87] loss=0.9441 val_loss=1.0414 scale=2.0000 norm=1.4517                    \n",
      "[iter 88] loss=0.9377 val_loss=1.0393 scale=2.0000 norm=1.4465                    \n",
      "[iter 89] loss=0.9400 val_loss=1.0376 scale=1.0000 norm=0.7258                    \n",
      "[iter 90] loss=0.9398 val_loss=1.0356 scale=1.0000 norm=0.7262                    \n",
      "[iter 91] loss=0.9284 val_loss=1.0327 scale=2.0000 norm=1.4340                    \n",
      "[iter 92] loss=0.9420 val_loss=1.0289 scale=2.0000 norm=1.4672                    \n",
      "[iter 93] loss=0.9467 val_loss=1.0274 scale=1.0000 norm=0.7415                    \n",
      "[iter 94] loss=0.9329 val_loss=1.0259 scale=1.0000 norm=0.7298                    \n",
      "[iter 95] loss=0.9349 val_loss=1.0236 scale=1.0000 norm=0.7311                    \n",
      "[iter 96] loss=0.9411 val_loss=1.0222 scale=2.0000 norm=1.4807                    \n",
      "[iter 97] loss=0.9311 val_loss=1.0197 scale=1.0000 norm=0.7296                    \n",
      "[iter 98] loss=0.9194 val_loss=1.0174 scale=1.0000 norm=0.7195                    \n",
      "[iter 99] loss=0.9154 val_loss=1.0159 scale=1.0000 norm=0.7168                    \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.75, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.07007152113008955, 'minibatch_frac': 0.6, 'n_estimators': 2000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5178 val_loss=1.4116 scale=1.0000 norm=1.0617                     \n",
      "[iter 1] loss=1.2678 val_loss=1.2312 scale=2.0000 norm=1.6735                     \n",
      "[iter 2] loss=1.1495 val_loss=1.1596 scale=2.0000 norm=1.5015                     \n",
      "[iter 3] loss=1.0636 val_loss=1.0969 scale=2.0000 norm=1.3967                     \n",
      "[iter 4] loss=1.0146 val_loss=1.0362 scale=2.0000 norm=1.3643                     \n",
      "[iter 5] loss=0.9498 val_loss=0.9889 scale=2.0000 norm=1.3017                     \n",
      "[iter 6] loss=0.8943 val_loss=0.9412 scale=2.0000 norm=1.2509                     \n",
      "[iter 7] loss=0.8532 val_loss=0.9011 scale=2.0000 norm=1.2397                     \n",
      "[iter 8] loss=0.7990 val_loss=0.8499 scale=2.0000 norm=1.2268                     \n",
      "[iter 9] loss=0.7528 val_loss=0.8158 scale=2.0000 norm=1.2033                     \n",
      "[iter 10] loss=0.7016 val_loss=0.7832 scale=2.0000 norm=1.1736                    \n",
      "[iter 11] loss=0.6632 val_loss=0.7498 scale=2.0000 norm=1.1840                    \n",
      "[iter 12] loss=0.6155 val_loss=0.7198 scale=2.0000 norm=1.1464                    \n",
      "[iter 13] loss=0.5669 val_loss=0.6909 scale=2.0000 norm=1.1228                    \n",
      "[iter 14] loss=0.5365 val_loss=0.6706 scale=2.0000 norm=1.1345                    \n",
      "[iter 15] loss=0.5026 val_loss=0.6485 scale=2.0000 norm=1.1224                    \n",
      "[iter 16] loss=0.4663 val_loss=0.6283 scale=2.0000 norm=1.1107                    \n",
      "[iter 17] loss=0.4440 val_loss=0.6195 scale=2.0000 norm=1.1034                    \n",
      "[iter 18] loss=0.4133 val_loss=0.6115 scale=2.0000 norm=1.1020                    \n",
      "[iter 19] loss=0.3804 val_loss=0.6011 scale=2.0000 norm=1.0889                    \n",
      "[iter 20] loss=0.3494 val_loss=0.5921 scale=2.0000 norm=1.0844                    \n",
      "[iter 21] loss=0.3280 val_loss=0.5855 scale=2.0000 norm=1.0880                    \n",
      "[iter 22] loss=0.2970 val_loss=0.5876 scale=2.0000 norm=1.0807                    \n",
      "[iter 23] loss=0.2708 val_loss=0.5845 scale=2.0000 norm=1.0646                    \n",
      "[iter 24] loss=0.2764 val_loss=0.5880 scale=2.0000 norm=1.0878                    \n",
      "[iter 25] loss=0.2480 val_loss=0.5856 scale=2.0000 norm=1.0676                    \n",
      "[iter 26] loss=0.2520 val_loss=0.5822 scale=2.0000 norm=1.1004                    \n",
      "[iter 27] loss=0.2165 val_loss=0.5834 scale=2.0000 norm=1.0625                    \n",
      "[iter 28] loss=0.2061 val_loss=0.5859 scale=1.0000 norm=0.5376                    \n",
      "[iter 29] loss=0.2026 val_loss=0.5952 scale=2.0000 norm=1.0737                    \n",
      "[iter 30] loss=0.1965 val_loss=0.5919 scale=2.0000 norm=1.0784                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL26 (val_loss=0.5822)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=1,\n",
      "                      max_features='log2', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.058432438741091025, 'minibatch_frac': 0.7, 'n_estimators': 1000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 10}}\n",
      "[iter 0] loss=1.5211 val_loss=1.5402 scale=1.0000 norm=1.0651                     \n",
      "[iter 1] loss=1.4757 val_loss=1.5160 scale=1.0000 norm=1.0205                     \n",
      "[iter 2] loss=1.4607 val_loss=1.5002 scale=1.0000 norm=1.0076                     \n",
      "[iter 3] loss=1.3747 val_loss=1.4837 scale=2.0000 norm=1.8467                     \n",
      "[iter 4] loss=1.4533 val_loss=1.4784 scale=1.0000 norm=1.0121                     \n",
      "[iter 5] loss=1.4296 val_loss=1.4693 scale=1.0000 norm=0.9907                     \n",
      "[iter 6] loss=1.3621 val_loss=1.4607 scale=2.0000 norm=1.8515                     \n",
      "[iter 7] loss=1.4285 val_loss=1.4602 scale=2.0000 norm=1.9978                     \n",
      "[iter 8] loss=1.4216 val_loss=1.4606 scale=2.0000 norm=1.9891                     \n",
      "[iter 9] loss=1.3924 val_loss=1.4634 scale=2.0000 norm=1.9329                     \n",
      "[iter 10] loss=1.3853 val_loss=1.4626 scale=1.0000 norm=0.9626                    \n",
      "[iter 11] loss=1.3965 val_loss=1.4622 scale=2.0000 norm=1.9588                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL7 (val_loss=1.4602)                                        \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=5,\n",
      "                      max_features=0.75, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.24542666332617738, 'minibatch_frac': 0.7, 'n_estimators': 2000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 5}}\n",
      "[iter 0] loss=1.5211 val_loss=1.2240 scale=1.0000 norm=1.0651                     \n",
      "[iter 1] loss=1.1810 val_loss=1.0386 scale=2.0000 norm=1.5016                     \n",
      "[iter 2] loss=0.9813 val_loss=0.9017 scale=2.0000 norm=1.2937                     \n",
      "[iter 3] loss=0.7978 val_loss=0.8001 scale=2.0000 norm=1.2211                     \n",
      "[iter 4] loss=0.7016 val_loss=0.7533 scale=2.0000 norm=1.2104                     \n",
      "[iter 5] loss=0.5710 val_loss=0.6948 scale=2.0000 norm=1.1670                     \n",
      "[iter 6] loss=0.4730 val_loss=0.7130 scale=2.0000 norm=1.1683                     \n",
      "[iter 7] loss=0.4307 val_loss=0.7302 scale=2.0000 norm=1.1844                     \n",
      "[iter 8] loss=0.3984 val_loss=0.7117 scale=2.0000 norm=1.2145                     \n",
      "[iter 9] loss=0.3679 val_loss=0.7286 scale=1.0000 norm=0.6158                     \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL5 (val_loss=0.6948)                                        \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.4515233503857897, 'minibatch_frac': 0.7, 'n_estimators': 1000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 5}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=1.5211 val_loss=1.1282 scale=1.0000 norm=1.0651                     \n",
      "[iter 1] loss=1.1424 val_loss=0.8814 scale=2.0000 norm=1.3660                     \n",
      "[iter 2] loss=0.8372 val_loss=0.7390 scale=2.0000 norm=1.2150                     \n",
      "[iter 3] loss=0.5763 val_loss=0.7978 scale=2.0000 norm=1.1788                     \n",
      "[iter 4] loss=0.5598 val_loss=0.7711 scale=1.0000 norm=0.6552                     \n",
      "[iter 5] loss=0.4659 val_loss=0.7209 scale=2.0000 norm=1.1929                     \n",
      "[iter 6] loss=0.4046 val_loss=0.7466 scale=1.0000 norm=0.6263                     \n",
      "[iter 7] loss=0.4173 val_loss=0.7505 scale=1.0000 norm=0.6145                     \n",
      "[iter 8] loss=0.4024 val_loss=0.8205 scale=1.0000 norm=0.6150                     \n",
      "[iter 9] loss=0.3733 val_loss=0.9709 scale=1.0000 norm=0.6234                     \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL5 (val_loss=0.7209)                                        \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=1,\n",
      "                      max_features='log2', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.35866129419276915, 'minibatch_frac': 0.8, 'n_estimators': 2000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 10}}\n",
      "[iter 0] loss=1.5200 val_loss=1.4403 scale=1.0000 norm=1.0640                     \n",
      "[iter 1] loss=1.4059 val_loss=1.4584 scale=2.0000 norm=1.9116                     \n",
      "[iter 2] loss=1.3800 val_loss=1.4131 scale=2.0000 norm=1.9328                     \n",
      "[iter 3] loss=1.3050 val_loss=1.4202 scale=1.0000 norm=0.9221                     \n",
      "[iter 4] loss=1.3573 val_loss=1.4322 scale=1.0000 norm=0.9895                     \n",
      "[iter 5] loss=1.3410 val_loss=1.4427 scale=2.0000 norm=1.9508                     \n",
      "[iter 6] loss=1.2810 val_loss=1.4191 scale=2.0000 norm=1.8505                     \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL2 (val_loss=1.4131)                                        \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features='sqrt', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.11403353255855306, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.3410 scale=1.0000 norm=1.0277                     \n",
      "[iter 1] loss=1.2450 val_loss=1.1794 scale=2.0000 norm=1.6424                     \n",
      "[iter 2] loss=1.1016 val_loss=1.0879 scale=2.0000 norm=1.4475                     \n",
      "[iter 3] loss=1.0116 val_loss=1.0118 scale=2.0000 norm=1.3685                     \n",
      "[iter 4] loss=0.9287 val_loss=0.9387 scale=2.0000 norm=1.3121                     \n",
      "[iter 5] loss=0.8495 val_loss=0.8779 scale=2.0000 norm=1.2651                     \n",
      "[iter 6] loss=0.7780 val_loss=0.8298 scale=2.0000 norm=1.2380                     \n",
      "[iter 7] loss=0.7105 val_loss=0.7763 scale=2.0000 norm=1.2121                     \n",
      "[iter 8] loss=0.6465 val_loss=0.7276 scale=2.0000 norm=1.1913                     \n",
      "[iter 9] loss=0.5842 val_loss=0.6926 scale=2.0000 norm=1.1709                     \n",
      "[iter 10] loss=0.5309 val_loss=0.6601 scale=2.0000 norm=1.1590                    \n",
      "[iter 11] loss=0.4768 val_loss=0.6381 scale=2.0000 norm=1.1420                    \n",
      "[iter 12] loss=0.4292 val_loss=0.6197 scale=2.0000 norm=1.1295                    \n",
      "[iter 13] loss=0.3909 val_loss=0.6103 scale=2.0000 norm=1.1246                    \n",
      "[iter 14] loss=0.3559 val_loss=0.6099 scale=2.0000 norm=1.1223                    \n",
      "[iter 15] loss=0.3267 val_loss=0.6038 scale=1.0000 norm=0.5629                    \n",
      "[iter 16] loss=0.3083 val_loss=0.6014 scale=1.0000 norm=0.5585                    \n",
      "[iter 17] loss=0.2940 val_loss=0.5986 scale=1.0000 norm=0.5579                    \n",
      "[iter 18] loss=0.2820 val_loss=0.5976 scale=2.0000 norm=1.1167                    \n",
      "[iter 19] loss=0.2617 val_loss=0.5944 scale=1.0000 norm=0.5611                    \n",
      "[iter 20] loss=0.2486 val_loss=0.5969 scale=1.0000 norm=0.5591                    \n",
      "[iter 21] loss=0.2388 val_loss=0.6012 scale=1.0000 norm=0.5594                    \n",
      "[iter 22] loss=0.2297 val_loss=0.6277 scale=2.0000 norm=1.1202                    \n",
      "[iter 23] loss=0.2106 val_loss=0.6333 scale=1.0000 norm=0.5591                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL19 (val_loss=0.5944)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features='sqrt', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.11768570725740737, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.3365 scale=1.0000 norm=1.0277                     \n",
      "[iter 1] loss=1.2432 val_loss=1.2379 scale=1.0000 norm=0.8195                     \n",
      "[iter 2] loss=1.1467 val_loss=1.1318 scale=2.0000 norm=1.5041                     \n",
      "[iter 3] loss=1.0434 val_loss=1.0490 scale=2.0000 norm=1.3966                     \n",
      "[iter 4] loss=0.9579 val_loss=0.9669 scale=2.0000 norm=1.3357                     \n",
      "[iter 5] loss=0.8709 val_loss=0.8962 scale=2.0000 norm=1.2794                     \n",
      "[iter 6] loss=0.7921 val_loss=0.8443 scale=2.0000 norm=1.2411                     \n",
      "[iter 7] loss=0.7261 val_loss=0.7932 scale=2.0000 norm=1.2202                     \n",
      "[iter 8] loss=0.6554 val_loss=0.7536 scale=2.0000 norm=1.1928                     \n",
      "[iter 9] loss=0.5947 val_loss=0.6837 scale=2.0000 norm=1.1734                     \n",
      "[iter 10] loss=0.5352 val_loss=0.6462 scale=2.0000 norm=1.1568                    \n",
      "[iter 11] loss=0.4798 val_loss=0.6301 scale=2.0000 norm=1.1399                    \n",
      "[iter 12] loss=0.4325 val_loss=0.6092 scale=2.0000 norm=1.1299                    \n",
      "[iter 13] loss=0.3871 val_loss=0.5935 scale=2.0000 norm=1.1197                    \n",
      "[iter 14] loss=0.3472 val_loss=0.5941 scale=2.0000 norm=1.1103                    \n",
      "[iter 15] loss=0.3129 val_loss=0.6022 scale=2.0000 norm=1.1054                    \n",
      "[iter 16] loss=0.2834 val_loss=0.5995 scale=1.0000 norm=0.5520                    \n",
      "[iter 17] loss=0.2699 val_loss=0.6006 scale=1.0000 norm=0.5516                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL13 (val_loss=0.5935)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.11505855882630425, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.2990 scale=1.0000 norm=1.0277                     \n",
      "[iter 1] loss=1.2124 val_loss=1.1693 scale=2.0000 norm=1.5859                     \n",
      "[iter 2] loss=1.0775 val_loss=1.0745 scale=2.0000 norm=1.4138                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 3] loss=0.9782 val_loss=0.9835 scale=2.0000 norm=1.3208                     \n",
      "[iter 4] loss=0.8875 val_loss=0.8834 scale=2.0000 norm=1.2583                     \n",
      "[iter 5] loss=0.8034 val_loss=0.8056 scale=2.0000 norm=1.2152                     \n",
      "[iter 6] loss=0.7247 val_loss=0.7417 scale=2.0000 norm=1.1849                     \n",
      "[iter 7] loss=0.6501 val_loss=0.6859 scale=2.0000 norm=1.1575                     \n",
      "[iter 8] loss=0.5817 val_loss=0.6407 scale=2.0000 norm=1.1377                     \n",
      "[iter 9] loss=0.5199 val_loss=0.6020 scale=2.0000 norm=1.1236                     \n",
      "[iter 10] loss=0.4647 val_loss=0.5720 scale=2.0000 norm=1.1130                    \n",
      "[iter 11] loss=0.4122 val_loss=0.5450 scale=2.0000 norm=1.0998                    \n",
      "[iter 12] loss=0.3656 val_loss=0.5325 scale=2.0000 norm=1.0928                    \n",
      "[iter 13] loss=0.3241 val_loss=0.5252 scale=2.0000 norm=1.0876                    \n",
      "[iter 14] loss=0.2863 val_loss=0.5183 scale=2.0000 norm=1.0798                    \n",
      "[iter 15] loss=0.2528 val_loss=0.5264 scale=2.0000 norm=1.0759                    \n",
      "[iter 16] loss=0.2230 val_loss=0.5310 scale=2.0000 norm=1.0747                    \n",
      "[iter 17] loss=0.1976 val_loss=0.5468 scale=2.0000 norm=1.0748                    \n",
      "[iter 18] loss=0.1731 val_loss=0.5721 scale=2.0000 norm=1.0739                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL14 (val_loss=0.5183)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.32199644585200377, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.1674 scale=1.0000 norm=1.0277                    \n",
      "[iter 1] loss=1.1456 val_loss=0.9482 scale=2.0000 norm=1.4289                    \n",
      "[iter 2] loss=0.8920 val_loss=0.8040 scale=2.0000 norm=1.2261                    \n",
      "[iter 3] loss=0.6823 val_loss=0.7214 scale=2.0000 norm=1.1668                    \n",
      "[iter 4] loss=0.5190 val_loss=0.7168 scale=2.0000 norm=1.1411                    \n",
      "[iter 5] loss=0.3941 val_loss=0.7698 scale=2.0000 norm=1.1263                    \n",
      "[iter 6] loss=0.3132 val_loss=0.8068 scale=2.0000 norm=1.1336                    \n",
      "[iter 7] loss=0.2569 val_loss=0.8187 scale=1.0000 norm=0.5697                    \n",
      "[iter 8] loss=0.2350 val_loss=0.9516 scale=2.0000 norm=1.1388                    \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL4 (val_loss=0.7168)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.121623580145979, 'minibatch_frac': 0.6, 'n_estimators': 2000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5178 val_loss=1.2773 scale=1.0000 norm=1.0617                    \n",
      "[iter 1] loss=1.2419 val_loss=1.1469 scale=2.0000 norm=1.6138                    \n",
      "[iter 2] loss=1.0959 val_loss=1.0607 scale=2.0000 norm=1.4186                    \n",
      "[iter 3] loss=0.9732 val_loss=0.9849 scale=2.0000 norm=1.3086                    \n",
      "[iter 4] loss=0.8903 val_loss=0.9105 scale=2.0000 norm=1.2691                    \n",
      "[iter 5] loss=0.8023 val_loss=0.8505 scale=2.0000 norm=1.2196                    \n",
      "[iter 6] loss=0.7161 val_loss=0.7772 scale=2.0000 norm=1.1786                    \n",
      "[iter 7] loss=0.6611 val_loss=0.7336 scale=2.0000 norm=1.1701                    \n",
      "[iter 8] loss=0.5876 val_loss=0.7067 scale=2.0000 norm=1.1610                    \n",
      "[iter 9] loss=0.5322 val_loss=0.6723 scale=2.0000 norm=1.1544                    \n",
      "[iter 10] loss=0.4687 val_loss=0.6566 scale=2.0000 norm=1.1228                   \n",
      "[iter 11] loss=0.4322 val_loss=0.6334 scale=2.0000 norm=1.1386                   \n",
      "[iter 12] loss=0.3800 val_loss=0.6310 scale=2.0000 norm=1.1194                   \n",
      "[iter 13] loss=0.3297 val_loss=0.6392 scale=2.0000 norm=1.0836                   \n",
      "[iter 14] loss=0.3084 val_loss=0.6378 scale=2.0000 norm=1.1194                   \n",
      "[iter 15] loss=0.2930 val_loss=0.6437 scale=2.0000 norm=1.1182                   \n",
      "[iter 16] loss=0.2567 val_loss=0.6443 scale=1.0000 norm=0.5519                   \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL12 (val_loss=0.6310)                                      \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=5,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.09732697014127678, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.3581 scale=1.0000 norm=1.0277                    \n",
      "[iter 1] loss=1.2412 val_loss=1.1991 scale=2.0000 norm=1.6356                    \n",
      "[iter 2] loss=1.1102 val_loss=1.1143 scale=2.0000 norm=1.4588                    \n",
      "[iter 3] loss=1.0263 val_loss=1.0277 scale=2.0000 norm=1.3745                    \n",
      "[iter 4] loss=0.9501 val_loss=0.9595 scale=2.0000 norm=1.3150                    \n",
      "[iter 5] loss=0.8778 val_loss=0.9002 scale=2.0000 norm=1.2702                    \n",
      "[iter 6] loss=0.8098 val_loss=0.8442 scale=2.0000 norm=1.2363                    \n",
      "[iter 7] loss=0.7476 val_loss=0.7939 scale=2.0000 norm=1.2126                    \n",
      "[iter 8] loss=0.6883 val_loss=0.7497 scale=2.0000 norm=1.1933                    \n",
      "[iter 9] loss=0.6354 val_loss=0.7147 scale=2.0000 norm=1.1796                    \n",
      "[iter 10] loss=0.5858 val_loss=0.6815 scale=2.0000 norm=1.1700                   \n",
      "[iter 11] loss=0.5375 val_loss=0.6541 scale=2.0000 norm=1.1577                   \n",
      "[iter 12] loss=0.4940 val_loss=0.6293 scale=2.0000 norm=1.1501                   \n",
      "[iter 13] loss=0.4552 val_loss=0.6133 scale=2.0000 norm=1.1453                   \n",
      "[iter 14] loss=0.4204 val_loss=0.6021 scale=2.0000 norm=1.1421                   \n",
      "[iter 15] loss=0.3906 val_loss=0.5958 scale=2.0000 norm=1.1426                   \n",
      "[iter 16] loss=0.3629 val_loss=0.5934 scale=2.0000 norm=1.1396                   \n",
      "[iter 17] loss=0.3378 val_loss=0.5948 scale=1.0000 norm=0.5696                   \n",
      "[iter 18] loss=0.3251 val_loss=0.5898 scale=2.0000 norm=1.1363                   \n",
      "[iter 19] loss=0.3031 val_loss=0.5969 scale=2.0000 norm=1.1355                   \n",
      "[iter 20] loss=0.2865 val_loss=0.6030 scale=2.0000 norm=1.1404                   \n",
      "[iter 21] loss=0.2708 val_loss=0.6103 scale=2.0000 norm=1.1431                   \n",
      "[iter 22] loss=0.2551 val_loss=0.6181 scale=2.0000 norm=1.1445                   \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL18 (val_loss=0.5898)                                      \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=3,\n",
      "                      max_features='sqrt', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.15668529518882468, 'minibatch_frac': 1.0, 'n_estimators': 2000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 10}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=1.4840 val_loss=1.4757 scale=1.0000 norm=1.0277                    \n",
      "[iter 1] loss=1.3296 val_loss=1.3814 scale=1.0000 norm=0.8874                    \n",
      "[iter 2] loss=1.2416 val_loss=1.3046 scale=2.0000 norm=1.6496                    \n",
      "[iter 3] loss=1.1618 val_loss=1.2230 scale=2.0000 norm=1.5760                    \n",
      "[iter 4] loss=1.0852 val_loss=1.1867 scale=2.0000 norm=1.5215                    \n",
      "[iter 5] loss=1.0210 val_loss=1.1432 scale=1.0000 norm=0.7469                    \n",
      "[iter 6] loss=0.9712 val_loss=1.0847 scale=2.0000 norm=1.4518                    \n",
      "[iter 7] loss=0.9177 val_loss=1.0351 scale=2.0000 norm=1.4348                    \n",
      "[iter 8] loss=0.8571 val_loss=1.0196 scale=2.0000 norm=1.4103                    \n",
      "[iter 9] loss=0.8211 val_loss=0.9978 scale=2.0000 norm=1.4140                    \n",
      "[iter 10] loss=0.7870 val_loss=0.9627 scale=1.0000 norm=0.7055                   \n",
      "[iter 11] loss=0.7633 val_loss=0.9556 scale=1.0000 norm=0.6973                   \n",
      "[iter 12] loss=0.7462 val_loss=0.9335 scale=2.0000 norm=1.3888                   \n",
      "[iter 13] loss=0.7243 val_loss=0.9285 scale=1.0000 norm=0.6964                   \n",
      "[iter 14] loss=0.7053 val_loss=0.9286 scale=1.0000 norm=0.6941                   \n",
      "[iter 15] loss=0.6942 val_loss=0.8976 scale=1.0000 norm=0.6957                   \n",
      "[iter 16] loss=0.6694 val_loss=0.9018 scale=1.0000 norm=0.6885                   \n",
      "[iter 17] loss=0.6595 val_loss=0.9048 scale=1.0000 norm=0.6887                   \n",
      "[iter 18] loss=0.6514 val_loss=0.8942 scale=1.0000 norm=0.6905                   \n",
      "[iter 19] loss=0.6431 val_loss=0.7817 scale=2.0000 norm=1.3828                   \n",
      "[iter 20] loss=0.6340 val_loss=0.7791 scale=1.0000 norm=0.6967                   \n",
      "[iter 21] loss=0.6247 val_loss=0.7790 scale=1.0000 norm=0.6951                   \n",
      "[iter 22] loss=0.6209 val_loss=0.7770 scale=1.0000 norm=0.6972                   \n",
      "[iter 23] loss=0.6143 val_loss=0.7774 scale=1.0000 norm=0.6976                   \n",
      "[iter 24] loss=0.6108 val_loss=0.7667 scale=1.0000 norm=0.6994                   \n",
      "[iter 25] loss=0.5975 val_loss=0.7632 scale=1.0000 norm=0.6976                   \n",
      "[iter 26] loss=0.5926 val_loss=0.7642 scale=1.0000 norm=0.6975                   \n",
      "[iter 27] loss=0.5886 val_loss=0.7616 scale=1.0000 norm=0.6976                   \n",
      "[iter 28] loss=0.5828 val_loss=0.7529 scale=1.0000 norm=0.6969                   \n",
      "[iter 29] loss=0.5787 val_loss=0.7450 scale=1.0000 norm=0.6974                   \n",
      "[iter 30] loss=0.5679 val_loss=0.7450 scale=1.0000 norm=0.6951                   \n",
      "[iter 31] loss=0.5661 val_loss=0.7423 scale=1.0000 norm=0.6970                   \n",
      "[iter 32] loss=0.5593 val_loss=0.7357 scale=1.0000 norm=0.6945                   \n",
      "[iter 33] loss=0.5547 val_loss=0.7337 scale=1.0000 norm=0.6939                   \n",
      "[iter 34] loss=0.5486 val_loss=0.7243 scale=1.0000 norm=0.6925                   \n",
      "[iter 35] loss=0.5423 val_loss=0.7202 scale=1.0000 norm=0.6919                   \n",
      "[iter 36] loss=0.5394 val_loss=0.7232 scale=1.0000 norm=0.6922                   \n",
      "[iter 37] loss=0.5344 val_loss=0.7262 scale=1.0000 norm=0.6908                   \n",
      "[iter 38] loss=0.5310 val_loss=0.7214 scale=1.0000 norm=0.6905                   \n",
      "[iter 39] loss=0.5227 val_loss=0.7220 scale=2.0000 norm=1.3746                   \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL35 (val_loss=0.7202)                                      \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=3,\n",
      "                      max_features='log2', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.009488661884598737, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.5734 scale=1.0000 norm=1.0277                    \n",
      "[iter 1] loss=1.4191 val_loss=1.5642 scale=1.0000 norm=0.9672                    \n",
      "[iter 2] loss=1.3957 val_loss=1.5498 scale=1.0000 norm=0.9459                    \n",
      "[iter 3] loss=1.3834 val_loss=1.5357 scale=1.0000 norm=0.9352                    \n",
      "[iter 4] loss=1.3731 val_loss=1.5221 scale=1.0000 norm=0.9265                    \n",
      "[iter 5] loss=1.3599 val_loss=1.5113 scale=1.0000 norm=0.9154                    \n",
      "[iter 6] loss=1.3522 val_loss=1.5055 scale=1.0000 norm=0.9093                    \n",
      "[iter 7] loss=1.3453 val_loss=1.4971 scale=1.0000 norm=0.9037                    \n",
      "[iter 8] loss=1.3389 val_loss=1.4891 scale=1.0000 norm=0.8987                    \n",
      "[iter 9] loss=1.3331 val_loss=1.4805 scale=1.0000 norm=0.8944                    \n",
      "[iter 10] loss=1.3277 val_loss=1.4729 scale=1.0000 norm=0.8904                   \n",
      "[iter 11] loss=1.3227 val_loss=1.4640 scale=1.0000 norm=0.8868                   \n",
      "[iter 12] loss=1.3147 val_loss=1.4576 scale=1.0000 norm=0.8805                   \n",
      "[iter 13] loss=1.3090 val_loss=1.4509 scale=1.0000 norm=0.8762                   \n",
      "[iter 14] loss=1.3047 val_loss=1.4451 scale=1.0000 norm=0.8732                   \n",
      "[iter 15] loss=1.3006 val_loss=1.4394 scale=1.0000 norm=0.8704                   \n",
      "[iter 16] loss=1.2957 val_loss=1.4349 scale=1.0000 norm=0.8669                   \n",
      "[iter 17] loss=1.2919 val_loss=1.4296 scale=1.0000 norm=0.8643                   \n",
      "[iter 18] loss=1.2876 val_loss=1.4247 scale=1.0000 norm=0.8613                   \n",
      "[iter 19] loss=1.2840 val_loss=1.4197 scale=1.0000 norm=0.8590                   \n",
      "[iter 20] loss=1.2781 val_loss=1.4151 scale=1.0000 norm=0.8548                   \n",
      "[iter 21] loss=1.2742 val_loss=1.4104 scale=1.0000 norm=0.8523                   \n",
      "[iter 22] loss=1.2710 val_loss=1.4068 scale=1.0000 norm=0.8502                   \n",
      "[iter 23] loss=1.2673 val_loss=1.4023 scale=1.0000 norm=0.8479                   \n",
      "[iter 24] loss=1.2642 val_loss=1.3989 scale=1.0000 norm=0.8460                   \n",
      "[iter 25] loss=1.2608 val_loss=1.3947 scale=1.0000 norm=0.8439                   \n",
      "[iter 26] loss=1.2578 val_loss=1.3911 scale=1.0000 norm=0.8422                   \n",
      "[iter 27] loss=1.2545 val_loss=1.3871 scale=1.0000 norm=0.8403                   \n",
      "[iter 28] loss=1.2516 val_loss=1.3844 scale=1.0000 norm=0.8386                   \n",
      "[iter 29] loss=1.2489 val_loss=1.3818 scale=1.0000 norm=0.8372                   \n",
      "[iter 30] loss=1.2465 val_loss=1.3789 scale=1.0000 norm=0.8360                   \n",
      "[iter 31] loss=1.2434 val_loss=1.3754 scale=1.0000 norm=0.8342                   \n",
      "[iter 32] loss=1.2392 val_loss=1.3722 scale=1.0000 norm=0.8316                   \n",
      "[iter 33] loss=1.2362 val_loss=1.3695 scale=1.0000 norm=0.8299                   \n",
      "[iter 34] loss=1.2339 val_loss=1.3670 scale=1.0000 norm=0.8289                   \n",
      "[iter 35] loss=1.2315 val_loss=1.3636 scale=1.0000 norm=0.8278                   \n",
      "[iter 36] loss=1.2289 val_loss=1.3616 scale=1.0000 norm=0.8264                   \n",
      "[iter 37] loss=1.2266 val_loss=1.3591 scale=1.0000 norm=0.8254                   \n",
      "[iter 38] loss=1.2244 val_loss=1.3564 scale=1.0000 norm=0.8245                   \n",
      "[iter 39] loss=1.2216 val_loss=1.3528 scale=2.0000 norm=1.6460                   \n",
      "[iter 40] loss=1.2185 val_loss=1.3500 scale=1.0000 norm=0.8222                   \n",
      "[iter 41] loss=1.2158 val_loss=1.3482 scale=1.0000 norm=0.8209                   \n",
      "[iter 42] loss=1.2136 val_loss=1.3458 scale=1.0000 norm=0.8200                   \n",
      "[iter 43] loss=1.2113 val_loss=1.3428 scale=2.0000 norm=1.6379                   \n",
      "[iter 44] loss=1.2080 val_loss=1.3389 scale=1.0000 norm=0.8181                   \n",
      "[iter 45] loss=1.2023 val_loss=1.3368 scale=1.0000 norm=0.8141                   \n",
      "[iter 46] loss=1.2002 val_loss=1.3353 scale=1.0000 norm=0.8132                   \n",
      "[iter 47] loss=1.1983 val_loss=1.3331 scale=1.0000 norm=0.8126                   \n",
      "[iter 48] loss=1.1960 val_loss=1.3304 scale=1.0000 norm=0.8116                   \n",
      "[iter 49] loss=1.1937 val_loss=1.3150 scale=2.0000 norm=1.6211                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 50] loss=1.1840 val_loss=1.3124 scale=2.0000 norm=1.6081                   \n",
      "[iter 51] loss=1.1802 val_loss=1.3104 scale=1.0000 norm=0.8027                   \n",
      "[iter 52] loss=1.1772 val_loss=1.3090 scale=1.0000 norm=0.8011                   \n",
      "[iter 53] loss=1.1730 val_loss=1.3071 scale=1.0000 norm=0.7987                   \n",
      "[iter 54] loss=1.1703 val_loss=1.3051 scale=2.0000 norm=1.5949                   \n",
      "[iter 55] loss=1.1666 val_loss=1.3033 scale=1.0000 norm=0.7966                   \n",
      "[iter 56] loss=1.1642 val_loss=1.3011 scale=1.0000 norm=0.7956                   \n",
      "[iter 57] loss=1.1616 val_loss=1.2953 scale=2.0000 norm=1.5888                   \n",
      "[iter 58] loss=1.1558 val_loss=1.2915 scale=1.0000 norm=0.7918                   \n",
      "[iter 59] loss=1.1527 val_loss=1.2889 scale=1.0000 norm=0.7902                   \n",
      "[iter 60] loss=1.1495 val_loss=1.2854 scale=2.0000 norm=1.5771                   \n",
      "[iter 61] loss=1.1444 val_loss=1.2836 scale=1.0000 norm=0.7863                   \n",
      "[iter 62] loss=1.1421 val_loss=1.2787 scale=2.0000 norm=1.5709                   \n",
      "[iter 63] loss=1.1389 val_loss=1.2739 scale=2.0000 norm=1.5696                   \n",
      "[iter 64] loss=1.1344 val_loss=1.2707 scale=1.0000 norm=0.7831                   \n",
      "[iter 65] loss=1.1316 val_loss=1.2682 scale=1.0000 norm=0.7818                   \n",
      "[iter 66] loss=1.1287 val_loss=1.2666 scale=2.0000 norm=1.5608                   \n",
      "[iter 67] loss=1.1258 val_loss=1.2641 scale=1.0000 norm=0.7801                   \n",
      "[iter 68] loss=1.1236 val_loss=1.2594 scale=2.0000 norm=1.5585                   \n",
      "[iter 69] loss=1.1192 val_loss=1.2562 scale=2.0000 norm=1.5553                   \n",
      "[iter 70] loss=1.1157 val_loss=1.2519 scale=2.0000 norm=1.5533                   \n",
      "[iter 71] loss=1.1115 val_loss=1.2493 scale=2.0000 norm=1.5505                   \n",
      "[iter 72] loss=1.1086 val_loss=1.2479 scale=1.0000 norm=0.7750                   \n",
      "[iter 73] loss=1.1067 val_loss=1.2455 scale=1.0000 norm=0.7744                   \n",
      "[iter 74] loss=1.1042 val_loss=1.2410 scale=2.0000 norm=1.5466                   \n",
      "[iter 75] loss=1.0998 val_loss=1.2382 scale=2.0000 norm=1.5433                   \n",
      "[iter 76] loss=1.0968 val_loss=1.2342 scale=2.0000 norm=1.5421                   \n",
      "[iter 77] loss=1.0928 val_loss=1.2313 scale=2.0000 norm=1.5395                   \n",
      "[iter 78] loss=1.0892 val_loss=1.2286 scale=2.0000 norm=1.5373                   \n",
      "[iter 79] loss=1.0862 val_loss=1.2243 scale=2.0000 norm=1.5363                   \n",
      "[iter 80] loss=1.0824 val_loss=1.2209 scale=2.0000 norm=1.5338                   \n",
      "[iter 81] loss=1.0775 val_loss=1.2183 scale=2.0000 norm=1.5300                   \n",
      "[iter 82] loss=1.0747 val_loss=1.2144 scale=2.0000 norm=1.5291                   \n",
      "[iter 83] loss=1.0710 val_loss=1.2117 scale=2.0000 norm=1.5270                   \n",
      "[iter 84] loss=1.0676 val_loss=1.2079 scale=2.0000 norm=1.5251                   \n",
      "[iter 85] loss=1.0636 val_loss=1.2052 scale=2.0000 norm=1.5225                   \n",
      "[iter 86] loss=1.0605 val_loss=1.2032 scale=2.0000 norm=1.5212                   \n",
      "[iter 87] loss=1.0580 val_loss=1.2010 scale=2.0000 norm=1.5212                   \n",
      "[iter 88] loss=1.0544 val_loss=1.1985 scale=1.0000 norm=0.7596                   \n",
      "[iter 89] loss=1.0518 val_loss=1.1963 scale=2.0000 norm=1.5173                   \n",
      "[iter 90] loss=1.0489 val_loss=1.1942 scale=1.0000 norm=0.7581                   \n",
      "[iter 91] loss=1.0467 val_loss=1.1923 scale=2.0000 norm=1.5147                   \n",
      "[iter 92] loss=1.0439 val_loss=1.1901 scale=2.0000 norm=1.5139                   \n",
      "[iter 93] loss=1.0410 val_loss=1.1857 scale=2.0000 norm=1.5130                   \n",
      "[iter 94] loss=1.0371 val_loss=1.1841 scale=2.0000 norm=1.5103                   \n",
      "[iter 95] loss=1.0344 val_loss=1.1823 scale=2.0000 norm=1.5097                   \n",
      "[iter 96] loss=1.0317 val_loss=1.1805 scale=2.0000 norm=1.5089                   \n",
      "[iter 97] loss=1.0291 val_loss=1.1789 scale=1.0000 norm=0.7542                   \n",
      "[iter 98] loss=1.0273 val_loss=1.1770 scale=2.0000 norm=1.5074                   \n",
      "[iter 99] loss=1.0247 val_loss=1.1751 scale=1.0000 norm=0.7534                   \n",
      "[iter 100] loss=1.0226 val_loss=1.1735 scale=2.0000 norm=1.5056                  \n",
      "[iter 101] loss=1.0201 val_loss=1.1707 scale=2.0000 norm=1.5052                  \n",
      "[iter 102] loss=1.0167 val_loss=1.1689 scale=2.0000 norm=1.5033                  \n",
      "[iter 103] loss=1.0141 val_loss=1.1674 scale=2.0000 norm=1.5027                  \n",
      "[iter 104] loss=1.0117 val_loss=1.1659 scale=1.0000 norm=0.7511                  \n",
      "[iter 105] loss=1.0098 val_loss=1.1641 scale=2.0000 norm=1.5011                  \n",
      "[iter 106] loss=1.0074 val_loss=1.1621 scale=1.0000 norm=0.7503                  \n",
      "[iter 107] loss=1.0052 val_loss=1.1605 scale=2.0000 norm=1.4991                  \n",
      "[iter 108] loss=1.0028 val_loss=1.1567 scale=2.0000 norm=1.4988                  \n",
      "[iter 109] loss=0.9991 val_loss=1.1552 scale=2.0000 norm=1.4962                  \n",
      "[iter 110] loss=0.9968 val_loss=1.1532 scale=1.0000 norm=0.7480                  \n",
      "[iter 111] loss=0.9946 val_loss=1.1470 scale=2.0000 norm=1.4946                  \n",
      "[iter 112] loss=0.9907 val_loss=1.1448 scale=2.0000 norm=1.4910                  \n",
      "[iter 113] loss=0.9879 val_loss=1.1430 scale=2.0000 norm=1.4902                  \n",
      "[iter 114] loss=0.9856 val_loss=1.1406 scale=1.0000 norm=0.7450                  \n",
      "[iter 115] loss=0.9828 val_loss=1.1350 scale=2.0000 norm=1.4876                  \n",
      "[iter 116] loss=0.9794 val_loss=1.1330 scale=2.0000 norm=1.4849                  \n",
      "[iter 117] loss=0.9768 val_loss=1.1315 scale=2.0000 norm=1.4842                  \n",
      "[iter 118] loss=0.9745 val_loss=1.1255 scale=2.0000 norm=1.4843                  \n",
      "[iter 119] loss=0.9707 val_loss=1.1233 scale=1.0000 norm=0.7404                  \n",
      "[iter 120] loss=0.9681 val_loss=1.1192 scale=2.0000 norm=1.4784                  \n",
      "[iter 121] loss=0.9631 val_loss=1.1165 scale=1.0000 norm=0.7371                  \n",
      "[iter 122] loss=0.9605 val_loss=1.1132 scale=2.0000 norm=1.4718                  \n",
      "[iter 123] loss=0.9562 val_loss=1.1107 scale=1.0000 norm=0.7343                  \n",
      "[iter 124] loss=0.9539 val_loss=1.1091 scale=1.0000 norm=0.7334                  \n",
      "[iter 125] loss=0.9524 val_loss=1.1072 scale=2.0000 norm=1.4661                  \n",
      "[iter 126] loss=0.9483 val_loss=1.1052 scale=1.0000 norm=0.7316                  \n",
      "[iter 127] loss=0.9464 val_loss=1.1038 scale=2.0000 norm=1.4618                  \n",
      "[iter 128] loss=0.9441 val_loss=1.1003 scale=2.0000 norm=1.4615                  \n",
      "[iter 129] loss=0.9402 val_loss=1.0967 scale=1.0000 norm=0.7295                  \n",
      "[iter 130] loss=0.9376 val_loss=1.0939 scale=2.0000 norm=1.4565                  \n",
      "[iter 131] loss=0.9341 val_loss=1.0895 scale=2.0000 norm=1.4544                  \n",
      "[iter 132] loss=0.9302 val_loss=1.0869 scale=2.0000 norm=1.4517                  \n",
      "[iter 133] loss=0.9269 val_loss=1.0831 scale=1.0000 norm=0.7251                  \n",
      "[iter 134] loss=0.9241 val_loss=1.0810 scale=2.0000 norm=1.4476                  \n",
      "[iter 135] loss=0.9209 val_loss=1.0788 scale=2.0000 norm=1.4463                  \n",
      "[iter 136] loss=0.9179 val_loss=1.0757 scale=1.0000 norm=0.7225                  \n",
      "[iter 137] loss=0.9158 val_loss=1.0727 scale=2.0000 norm=1.4431                  \n",
      "[iter 138] loss=0.9128 val_loss=1.0709 scale=1.0000 norm=0.7210                  \n",
      "[iter 139] loss=0.9107 val_loss=1.0694 scale=2.0000 norm=1.4402                  \n",
      "[iter 140] loss=0.9076 val_loss=1.0674 scale=1.0000 norm=0.7195                  \n",
      "[iter 141] loss=0.9050 val_loss=1.0656 scale=2.0000 norm=1.4370                  \n",
      "[iter 142] loss=0.9022 val_loss=1.0638 scale=2.0000 norm=1.4361                  \n",
      "[iter 143] loss=0.8992 val_loss=1.0620 scale=2.0000 norm=1.4351                  \n",
      "[iter 144] loss=0.8966 val_loss=1.0588 scale=1.0000 norm=0.7173                  \n",
      "[iter 145] loss=0.8941 val_loss=1.0578 scale=2.0000 norm=1.4325                  \n",
      "[iter 146] loss=0.8914 val_loss=1.0559 scale=2.0000 norm=1.4316                  \n",
      "[iter 147] loss=0.8887 val_loss=1.0541 scale=1.0000 norm=0.7154                  \n",
      "[iter 148] loss=0.8869 val_loss=1.0534 scale=1.0000 norm=0.7148                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 149] loss=0.8853 val_loss=1.0521 scale=2.0000 norm=1.4289                  \n",
      "[iter 150] loss=0.8828 val_loss=1.0477 scale=2.0000 norm=1.4284                  \n",
      "[iter 151] loss=0.8795 val_loss=1.0469 scale=2.0000 norm=1.4260                  \n",
      "[iter 152] loss=0.8774 val_loss=1.0459 scale=1.0000 norm=0.7130                  \n",
      "[iter 153] loss=0.8759 val_loss=1.0431 scale=2.0000 norm=1.4255                  \n",
      "[iter 154] loss=0.8731 val_loss=1.0422 scale=2.0000 norm=1.4245                  \n",
      "[iter 155] loss=0.8706 val_loss=1.0391 scale=1.0000 norm=0.7119                  \n",
      "[iter 156] loss=0.8683 val_loss=1.0379 scale=2.0000 norm=1.4220                  \n",
      "[iter 157] loss=0.8660 val_loss=1.0373 scale=1.0000 norm=0.7108                  \n",
      "[iter 158] loss=0.8645 val_loss=1.0349 scale=2.0000 norm=1.4211                  \n",
      "[iter 159] loss=0.8623 val_loss=1.0321 scale=2.0000 norm=1.4208                  \n",
      "[iter 160] loss=0.8598 val_loss=1.0306 scale=2.0000 norm=1.4200                  \n",
      "[iter 161] loss=0.8566 val_loss=1.0280 scale=2.0000 norm=1.4179                  \n",
      "[iter 162] loss=0.8542 val_loss=1.0252 scale=1.0000 norm=0.7086                  \n",
      "[iter 163] loss=0.8518 val_loss=1.0244 scale=2.0000 norm=1.4153                  \n",
      "[iter 164] loss=0.8496 val_loss=1.0216 scale=1.0000 norm=0.7076                  \n",
      "[iter 165] loss=0.8473 val_loss=1.0203 scale=2.0000 norm=1.4134                  \n",
      "[iter 166] loss=0.8451 val_loss=1.0195 scale=2.0000 norm=1.4132                  \n",
      "[iter 167] loss=0.8428 val_loss=1.0171 scale=2.0000 norm=1.4128                  \n",
      "[iter 168] loss=0.8404 val_loss=1.0143 scale=1.0000 norm=0.7060                  \n",
      "[iter 169] loss=0.8382 val_loss=1.0136 scale=1.0000 norm=0.7052                  \n",
      "[iter 170] loss=0.8365 val_loss=1.0135 scale=2.0000 norm=1.4094                  \n",
      "[iter 171] loss=0.8344 val_loss=1.0127 scale=1.0000 norm=0.7047                  \n",
      "[iter 172] loss=0.8332 val_loss=1.0101 scale=1.0000 norm=0.7045                  \n",
      "[iter 173] loss=0.8311 val_loss=1.0086 scale=2.0000 norm=1.4074                  \n",
      "[iter 174] loss=0.8286 val_loss=1.0079 scale=1.0000 norm=0.7034                  \n",
      "[iter 175] loss=0.8274 val_loss=1.0069 scale=2.0000 norm=1.4065                  \n",
      "[iter 176] loss=0.8254 val_loss=1.0064 scale=1.0000 norm=0.7032                  \n",
      "[iter 177] loss=0.8242 val_loss=1.0041 scale=2.0000 norm=1.4062                  \n",
      "[iter 178] loss=0.8218 val_loss=1.0036 scale=1.0000 norm=0.7027                  \n",
      "[iter 179] loss=0.8206 val_loss=1.0028 scale=1.0000 norm=0.7026                  \n",
      "[iter 180] loss=0.8191 val_loss=1.0025 scale=1.0000 norm=0.7022                  \n",
      "[iter 181] loss=0.8179 val_loss=0.9999 scale=1.0000 norm=0.7020                  \n",
      "[iter 182] loss=0.8158 val_loss=0.9995 scale=2.0000 norm=1.4026                  \n",
      "[iter 183] loss=0.8139 val_loss=0.9988 scale=1.0000 norm=0.7013                  \n",
      "[iter 184] loss=0.8128 val_loss=0.9982 scale=1.0000 norm=0.7012                  \n",
      "[iter 185] loss=0.8116 val_loss=0.9957 scale=1.0000 norm=0.7011                  \n",
      "[iter 186] loss=0.8097 val_loss=0.9952 scale=1.0000 norm=0.7004                  \n",
      "[iter 187] loss=0.8086 val_loss=0.9948 scale=1.0000 norm=0.7004                  \n",
      "[iter 188] loss=0.8075 val_loss=0.9943 scale=1.0000 norm=0.7003                  \n",
      "[iter 189] loss=0.8065 val_loss=0.9939 scale=1.0000 norm=0.7002                  \n",
      "[iter 190] loss=0.8054 val_loss=0.9934 scale=1.0000 norm=0.7001                  \n",
      "[iter 191] loss=0.8044 val_loss=0.9929 scale=2.0000 norm=1.4002                  \n",
      "[iter 192] loss=0.8023 val_loss=0.9908 scale=2.0000 norm=1.3999                  \n",
      "[iter 193] loss=0.8004 val_loss=0.9911 scale=2.0000 norm=1.3997                  \n",
      "[iter 194] loss=0.7987 val_loss=0.9905 scale=1.0000 norm=0.6999                  \n",
      "[iter 195] loss=0.7977 val_loss=0.9880 scale=1.0000 norm=0.6998                  \n",
      "[iter 196] loss=0.7957 val_loss=0.9874 scale=1.0000 norm=0.6991                  \n",
      "[iter 197] loss=0.7947 val_loss=0.9869 scale=2.0000 norm=1.3981                  \n",
      "[iter 198] loss=0.7931 val_loss=0.9861 scale=2.0000 norm=1.3982                  \n",
      "[iter 199] loss=0.7912 val_loss=0.9853 scale=2.0000 norm=1.3980                  \n",
      "[iter 200] loss=0.7895 val_loss=0.9852 scale=2.0000 norm=1.3980                  \n",
      "[iter 201] loss=0.7880 val_loss=0.9845 scale=2.0000 norm=1.3982                  \n",
      "[iter 202] loss=0.7864 val_loss=0.9834 scale=1.0000 norm=0.6992                  \n",
      "[iter 203] loss=0.7854 val_loss=0.9838 scale=2.0000 norm=1.3981                  \n",
      "[iter 204] loss=0.7839 val_loss=0.9830 scale=2.0000 norm=1.3982                  \n",
      "[iter 205] loss=0.7824 val_loss=0.9825 scale=1.0000 norm=0.6992                  \n",
      "[iter 206] loss=0.7814 val_loss=0.9820 scale=1.0000 norm=0.6992                  \n",
      "[iter 207] loss=0.7805 val_loss=0.9795 scale=1.0000 norm=0.6991                  \n",
      "[iter 208] loss=0.7787 val_loss=0.9796 scale=2.0000 norm=1.3970                  \n",
      "[iter 209] loss=0.7772 val_loss=0.9791 scale=1.0000 norm=0.6986                  \n",
      "[iter 210] loss=0.7763 val_loss=0.9787 scale=2.0000 norm=1.3971                  \n",
      "[iter 211] loss=0.7748 val_loss=0.9780 scale=2.0000 norm=1.3970                  \n",
      "[iter 212] loss=0.7733 val_loss=0.9778 scale=1.0000 norm=0.6986                  \n",
      "[iter 213] loss=0.7724 val_loss=0.9770 scale=1.0000 norm=0.6986                  \n",
      "[iter 214] loss=0.7714 val_loss=0.9759 scale=1.0000 norm=0.6985                  \n",
      "[iter 215] loss=0.7705 val_loss=0.9752 scale=1.0000 norm=0.6984                  \n",
      "[iter 216] loss=0.7697 val_loss=0.9754 scale=2.0000 norm=1.3968                  \n",
      "[iter 217] loss=0.7683 val_loss=0.9746 scale=1.0000 norm=0.6985                  \n",
      "[iter 218] loss=0.7674 val_loss=0.9721 scale=1.0000 norm=0.6985                  \n",
      "[iter 219] loss=0.7656 val_loss=0.9710 scale=2.0000 norm=1.3957                  \n",
      "[iter 220] loss=0.7639 val_loss=0.9708 scale=1.0000 norm=0.6978                  \n",
      "[iter 221] loss=0.7631 val_loss=0.9705 scale=1.0000 norm=0.6978                  \n",
      "[iter 222] loss=0.7623 val_loss=0.9699 scale=1.0000 norm=0.6977                  \n",
      "[iter 223] loss=0.7615 val_loss=0.9699 scale=1.0000 norm=0.6977                  \n",
      "[iter 224] loss=0.7608 val_loss=0.9689 scale=1.0000 norm=0.6978                  \n",
      "[iter 225] loss=0.7598 val_loss=0.9685 scale=1.0000 norm=0.6976                  \n",
      "[iter 226] loss=0.7589 val_loss=0.9685 scale=1.0000 norm=0.6976                  \n",
      "[iter 227] loss=0.7581 val_loss=0.9678 scale=1.0000 norm=0.6975                  \n",
      "[iter 228] loss=0.7572 val_loss=0.9674 scale=1.0000 norm=0.6974                  \n",
      "[iter 229] loss=0.7564 val_loss=0.9668 scale=1.0000 norm=0.6973                  \n",
      "[iter 230] loss=0.7555 val_loss=0.9666 scale=2.0000 norm=1.3945                  \n",
      "[iter 231] loss=0.7542 val_loss=0.9666 scale=1.0000 norm=0.6974                  \n",
      "[iter 232] loss=0.7535 val_loss=0.9664 scale=1.0000 norm=0.6973                  \n",
      "[iter 233] loss=0.7527 val_loss=0.9657 scale=1.0000 norm=0.6973                  \n",
      "[iter 234] loss=0.7517 val_loss=0.9653 scale=1.0000 norm=0.6972                  \n",
      "[iter 235] loss=0.7510 val_loss=0.9651 scale=1.0000 norm=0.6971                  \n",
      "[iter 236] loss=0.7501 val_loss=0.9644 scale=1.0000 norm=0.6970                  \n",
      "[iter 237] loss=0.7492 val_loss=0.9640 scale=2.0000 norm=1.3937                  \n",
      "[iter 238] loss=0.7478 val_loss=0.9643 scale=2.0000 norm=1.3937                  \n",
      "[iter 239] loss=0.7466 val_loss=0.9640 scale=1.0000 norm=0.6970                  \n",
      "[iter 240] loss=0.7458 val_loss=0.9634 scale=2.0000 norm=1.3939                  \n",
      "[iter 241] loss=0.7446 val_loss=0.9633 scale=1.0000 norm=0.6971                  \n",
      "[iter 242] loss=0.7437 val_loss=0.9631 scale=1.0000 norm=0.6970                  \n",
      "[iter 243] loss=0.7429 val_loss=0.9627 scale=1.0000 norm=0.6970                  \n",
      "[iter 244] loss=0.7420 val_loss=0.9621 scale=2.0000 norm=1.3937                  \n",
      "[iter 245] loss=0.7404 val_loss=0.9618 scale=1.0000 norm=0.6967                  \n",
      "[iter 246] loss=0.7395 val_loss=0.9616 scale=1.0000 norm=0.6965                  \n",
      "[iter 247] loss=0.7388 val_loss=0.9611 scale=1.0000 norm=0.6965                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 248] loss=0.7379 val_loss=0.9607 scale=2.0000 norm=1.3928                  \n",
      "[iter 249] loss=0.7367 val_loss=0.9609 scale=2.0000 norm=1.3928                  \n",
      "[iter 250] loss=0.7356 val_loss=0.9608 scale=1.0000 norm=0.6965                  \n",
      "[iter 251] loss=0.7347 val_loss=0.9567 scale=1.0000 norm=0.6964                  \n",
      "[iter 252] loss=0.7339 val_loss=0.9559 scale=1.0000 norm=0.6964                  \n",
      "[iter 253] loss=0.7326 val_loss=0.9517 scale=1.0000 norm=0.6961                  \n",
      "[iter 254] loss=0.7309 val_loss=0.9498 scale=1.0000 norm=0.6956                  \n",
      "[iter 255] loss=0.7294 val_loss=0.9497 scale=1.0000 norm=0.6952                  \n",
      "[iter 256] loss=0.7288 val_loss=0.9487 scale=1.0000 norm=0.6952                  \n",
      "[iter 257] loss=0.7280 val_loss=0.9468 scale=1.0000 norm=0.6951                  \n",
      "[iter 258] loss=0.7264 val_loss=0.9467 scale=1.0000 norm=0.6946                  \n",
      "[iter 259] loss=0.7258 val_loss=0.9454 scale=2.0000 norm=1.3893                  \n",
      "[iter 260] loss=0.7247 val_loss=0.9447 scale=1.0000 norm=0.6947                  \n",
      "[iter 261] loss=0.7235 val_loss=0.9444 scale=1.0000 norm=0.6945                  \n",
      "[iter 262] loss=0.7226 val_loss=0.9445 scale=1.0000 norm=0.6943                  \n",
      "[iter 263] loss=0.7219 val_loss=0.9438 scale=1.0000 norm=0.6943                  \n",
      "[iter 264] loss=0.7204 val_loss=0.9419 scale=1.0000 norm=0.6940                  \n",
      "[iter 265] loss=0.7189 val_loss=0.9421 scale=2.0000 norm=1.3871                  \n",
      "[iter 266] loss=0.7180 val_loss=0.9408 scale=2.0000 norm=1.3877                  \n",
      "[iter 267] loss=0.7168 val_loss=0.9407 scale=2.0000 norm=1.3877                  \n",
      "[iter 268] loss=0.7160 val_loss=0.9407 scale=1.0000 norm=0.6942                  \n",
      "[iter 269] loss=0.7152 val_loss=0.9406 scale=1.0000 norm=0.6941                  \n",
      "[iter 270] loss=0.7143 val_loss=0.9398 scale=1.0000 norm=0.6939                  \n",
      "[iter 271] loss=0.7137 val_loss=0.9391 scale=1.0000 norm=0.6939                  \n",
      "[iter 272] loss=0.7123 val_loss=0.9389 scale=1.0000 norm=0.6937                  \n",
      "[iter 273] loss=0.7115 val_loss=0.9388 scale=1.0000 norm=0.6936                  \n",
      "[iter 274] loss=0.7109 val_loss=0.9364 scale=1.0000 norm=0.6936                  \n",
      "[iter 275] loss=0.7103 val_loss=0.9352 scale=2.0000 norm=1.3873                  \n",
      "[iter 276] loss=0.7092 val_loss=0.9351 scale=1.0000 norm=0.6938                  \n",
      "[iter 277] loss=0.7086 val_loss=0.9335 scale=1.0000 norm=0.6937                  \n",
      "[iter 278] loss=0.7066 val_loss=0.9335 scale=1.0000 norm=0.6930                  \n",
      "[iter 279] loss=0.7061 val_loss=0.9330 scale=1.0000 norm=0.6931                  \n",
      "[iter 280] loss=0.7049 val_loss=0.9328 scale=1.0000 norm=0.6928                  \n",
      "[iter 281] loss=0.7042 val_loss=0.9327 scale=1.0000 norm=0.6929                  \n",
      "[iter 282] loss=0.7037 val_loss=0.9320 scale=1.0000 norm=0.6929                  \n",
      "[iter 283] loss=0.7031 val_loss=0.9319 scale=1.0000 norm=0.6929                  \n",
      "[iter 284] loss=0.7024 val_loss=0.9317 scale=1.0000 norm=0.6929                  \n",
      "[iter 285] loss=0.7018 val_loss=0.9312 scale=1.0000 norm=0.6929                  \n",
      "[iter 286] loss=0.7006 val_loss=0.9305 scale=1.0000 norm=0.6927                  \n",
      "[iter 287] loss=0.7001 val_loss=0.9309 scale=2.0000 norm=1.3854                  \n",
      "[iter 288] loss=0.6991 val_loss=0.9303 scale=1.0000 norm=0.6927                  \n",
      "[iter 289] loss=0.6978 val_loss=0.9302 scale=1.0000 norm=0.6925                  \n",
      "[iter 290] loss=0.6972 val_loss=0.9302 scale=1.0000 norm=0.6926                  \n",
      "[iter 291] loss=0.6967 val_loss=0.9288 scale=2.0000 norm=1.3854                  \n",
      "[iter 292] loss=0.6957 val_loss=0.9288 scale=1.0000 norm=0.6928                  \n",
      "[iter 293] loss=0.6952 val_loss=0.9287 scale=1.0000 norm=0.6928                  \n",
      "[iter 294] loss=0.6945 val_loss=0.9287 scale=1.0000 norm=0.6927                  \n",
      "[iter 295] loss=0.6938 val_loss=0.9283 scale=1.0000 norm=0.6928                  \n",
      "[iter 296] loss=0.6933 val_loss=0.9276 scale=1.0000 norm=0.6928                  \n",
      "[iter 297] loss=0.6921 val_loss=0.9280 scale=2.0000 norm=1.3853                  \n",
      "[iter 298] loss=0.6912 val_loss=0.9275 scale=1.0000 norm=0.6928                  \n",
      "[iter 299] loss=0.6905 val_loss=0.9276 scale=1.0000 norm=0.6928                  \n",
      "[iter 300] loss=0.6901 val_loss=0.9275 scale=1.0000 norm=0.6929                  \n",
      "[iter 301] loss=0.6895 val_loss=0.9271 scale=1.0000 norm=0.6929                  \n",
      "[iter 302] loss=0.6885 val_loss=0.9269 scale=1.0000 norm=0.6927                  \n",
      "[iter 303] loss=0.6879 val_loss=0.9266 scale=1.0000 norm=0.6928                  \n",
      "[iter 304] loss=0.6874 val_loss=0.9257 scale=2.0000 norm=1.3856                  \n",
      "[iter 305] loss=0.6865 val_loss=0.9257 scale=1.0000 norm=0.6929                  \n",
      "[iter 306] loss=0.6859 val_loss=0.9258 scale=1.0000 norm=0.6929                  \n",
      "[iter 307] loss=0.6852 val_loss=0.9254 scale=1.0000 norm=0.6928                  \n",
      "[iter 308] loss=0.6845 val_loss=0.9255 scale=1.0000 norm=0.6928                  \n",
      "[iter 309] loss=0.6841 val_loss=0.9248 scale=2.0000 norm=1.3859                  \n",
      "[iter 310] loss=0.6831 val_loss=0.9241 scale=1.0000 norm=0.6930                  \n",
      "[iter 311] loss=0.6821 val_loss=0.9242 scale=1.0000 norm=0.6928                  \n",
      "[iter 312] loss=0.6816 val_loss=0.9236 scale=1.0000 norm=0.6929                  \n",
      "[iter 313] loss=0.6811 val_loss=0.9234 scale=1.0000 norm=0.6930                  \n",
      "[iter 314] loss=0.6800 val_loss=0.9231 scale=1.0000 norm=0.6927                  \n",
      "[iter 315] loss=0.6796 val_loss=0.9214 scale=1.0000 norm=0.6928                  \n",
      "[iter 316] loss=0.6783 val_loss=0.9211 scale=1.0000 norm=0.6924                  \n",
      "[iter 317] loss=0.6778 val_loss=0.9208 scale=1.0000 norm=0.6924                  \n",
      "[iter 318] loss=0.6773 val_loss=0.9191 scale=1.0000 norm=0.6925                  \n",
      "[iter 319] loss=0.6760 val_loss=0.9184 scale=1.0000 norm=0.6921                  \n",
      "[iter 320] loss=0.6749 val_loss=0.9164 scale=1.0000 norm=0.6918                  \n",
      "[iter 321] loss=0.6735 val_loss=0.9165 scale=1.0000 norm=0.6914                  \n",
      "[iter 322] loss=0.6731 val_loss=0.9165 scale=1.0000 norm=0.6915                  \n",
      "[iter 323] loss=0.6728 val_loss=0.9150 scale=1.0000 norm=0.6917                  \n",
      "[iter 324] loss=0.6715 val_loss=0.9143 scale=1.0000 norm=0.6913                  \n",
      "[iter 325] loss=0.6703 val_loss=0.9127 scale=1.0000 norm=0.6910                  \n",
      "[iter 326] loss=0.6691 val_loss=0.9114 scale=1.0000 norm=0.6907                  \n",
      "[iter 327] loss=0.6679 val_loss=0.9105 scale=2.0000 norm=1.3807                  \n",
      "[iter 328] loss=0.6667 val_loss=0.9093 scale=1.0000 norm=0.6902                  \n",
      "[iter 329] loss=0.6657 val_loss=0.9078 scale=1.0000 norm=0.6900                  \n",
      "[iter 330] loss=0.6644 val_loss=0.9073 scale=1.0000 norm=0.6896                  \n",
      "[iter 331] loss=0.6634 val_loss=0.9074 scale=1.0000 norm=0.6894                  \n",
      "[iter 332] loss=0.6630 val_loss=0.9058 scale=1.0000 norm=0.6895                  \n",
      "[iter 333] loss=0.6618 val_loss=0.9050 scale=1.0000 norm=0.6891                  \n",
      "[iter 334] loss=0.6604 val_loss=0.9038 scale=1.0000 norm=0.6888                  \n",
      "[iter 335] loss=0.6594 val_loss=0.9035 scale=1.0000 norm=0.6886                  \n",
      "[iter 336] loss=0.6585 val_loss=0.9023 scale=1.0000 norm=0.6885                  \n",
      "[iter 337] loss=0.6575 val_loss=0.9015 scale=1.0000 norm=0.6882                  \n",
      "[iter 338] loss=0.6562 val_loss=0.9001 scale=1.0000 norm=0.6880                  \n",
      "[iter 339] loss=0.6551 val_loss=0.9001 scale=2.0000 norm=1.3754                  \n",
      "[iter 340] loss=0.6543 val_loss=0.9005 scale=1.0000 norm=0.6878                  \n",
      "[iter 341] loss=0.6539 val_loss=0.9006 scale=2.0000 norm=1.3759                  \n",
      "[iter 342] loss=0.6532 val_loss=0.9000 scale=1.0000 norm=0.6881                  \n",
      "[iter 343] loss=0.6519 val_loss=0.8987 scale=1.0000 norm=0.6879                  \n",
      "[iter 344] loss=0.6508 val_loss=0.8983 scale=1.0000 norm=0.6876                  \n",
      "[iter 345] loss=0.6498 val_loss=0.8970 scale=1.0000 norm=0.6874                  \n",
      "[iter 346] loss=0.6488 val_loss=0.8972 scale=2.0000 norm=1.3743                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 347] loss=0.6481 val_loss=0.8966 scale=1.0000 norm=0.6873                  \n",
      "[iter 348] loss=0.6469 val_loss=0.8955 scale=1.0000 norm=0.6872                  \n",
      "[iter 349] loss=0.6460 val_loss=0.8942 scale=1.0000 norm=0.6870                  \n",
      "[iter 350] loss=0.6451 val_loss=0.8944 scale=2.0000 norm=1.3737                  \n",
      "[iter 351] loss=0.6445 val_loss=0.8948 scale=1.0000 norm=0.6872                  \n",
      "[iter 352] loss=0.6440 val_loss=0.8945 scale=1.0000 norm=0.6873                  \n",
      "[iter 353] loss=0.6437 val_loss=0.8943 scale=1.0000 norm=0.6874                  \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL349 (val_loss=0.8942)                                     \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=3,\n",
      "                      max_features=0.75, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.31519569125993396, 'minibatch_frac': 0.6, 'n_estimators': 2000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5178 val_loss=1.3424 scale=1.0000 norm=1.0617                    \n",
      "[iter 1] loss=1.2301 val_loss=1.0333 scale=2.0000 norm=1.5447                    \n",
      "[iter 2] loss=1.0042 val_loss=0.8824 scale=2.0000 norm=1.3483                    \n",
      "[iter 3] loss=0.7895 val_loss=0.7800 scale=2.0000 norm=1.2695                    \n",
      "[iter 4] loss=0.6929 val_loss=0.7009 scale=2.0000 norm=1.2898                    \n",
      "[iter 5] loss=0.6104 val_loss=0.6657 scale=2.0000 norm=1.2787                    \n",
      "[iter 6] loss=0.5476 val_loss=0.6460 scale=1.0000 norm=0.6481                    \n",
      "[iter 7] loss=0.5850 val_loss=0.6429 scale=1.0000 norm=0.6646                    \n",
      "[iter 8] loss=0.5367 val_loss=0.6557 scale=1.0000 norm=0.6824                    \n",
      "[iter 9] loss=0.5298 val_loss=0.6470 scale=2.0000 norm=1.3672                    \n",
      "[iter 10] loss=0.5015 val_loss=0.6542 scale=1.0000 norm=0.6673                   \n",
      "[iter 11] loss=0.5090 val_loss=0.6536 scale=1.0000 norm=0.6864                   \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL7 (val_loss=0.6429)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=3,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.4989994078921883, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.2499 scale=1.0000 norm=1.0277                    \n",
      "[iter 1] loss=1.2048 val_loss=0.9388 scale=2.0000 norm=1.4948                    \n",
      "[iter 2] loss=0.9088 val_loss=0.7722 scale=2.0000 norm=1.3138                    \n",
      "[iter 3] loss=0.7161 val_loss=0.7643 scale=2.0000 norm=1.3294                    \n",
      "[iter 4] loss=0.6244 val_loss=0.7476 scale=1.0000 norm=0.6887                    \n",
      "[iter 5] loss=0.6042 val_loss=0.7407 scale=1.0000 norm=0.6923                    \n",
      "[iter 6] loss=0.5836 val_loss=0.7394 scale=1.0000 norm=0.6904                    \n",
      "[iter 7] loss=0.5686 val_loss=0.7383 scale=1.0000 norm=0.6906                    \n",
      "[iter 8] loss=0.5559 val_loss=0.7806 scale=2.0000 norm=1.3792                    \n",
      "[iter 9] loss=0.5401 val_loss=0.7188 scale=1.0000 norm=0.6937                    \n",
      "[iter 10] loss=0.5325 val_loss=0.7087 scale=1.0000 norm=0.6901                   \n",
      "[iter 11] loss=0.5218 val_loss=0.7150 scale=1.0000 norm=0.6839                   \n",
      "[iter 12] loss=0.5118 val_loss=0.7068 scale=1.0000 norm=0.6790                   \n",
      "[iter 13] loss=0.4995 val_loss=0.7010 scale=1.0000 norm=0.6743                   \n",
      "[iter 14] loss=0.4872 val_loss=0.7066 scale=1.0000 norm=0.6721                   \n",
      "[iter 15] loss=0.4816 val_loss=0.7068 scale=2.0000 norm=1.3435                   \n",
      "[iter 16] loss=0.4646 val_loss=0.7184 scale=1.0000 norm=0.6702                   \n",
      "[iter 17] loss=0.4555 val_loss=0.7161 scale=0.5000 norm=0.3331                   \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL13 (val_loss=0.7010)                                      \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=1,\n",
      "                      max_features=0.75, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.2184062676031326, 'minibatch_frac': 1.0, 'n_estimators': 2000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 5}}\n",
      "[iter 0] loss=1.4840 val_loss=1.5094 scale=1.0000 norm=1.0277                    \n",
      "[iter 1] loss=1.3961 val_loss=1.4090 scale=1.0000 norm=0.9390                    \n",
      "[iter 2] loss=1.2665 val_loss=1.3264 scale=2.0000 norm=1.6718                    \n",
      "[iter 3] loss=1.1566 val_loss=1.1539 scale=2.0000 norm=1.5696                    \n",
      "[iter 4] loss=1.0312 val_loss=1.0547 scale=2.0000 norm=1.4579                    \n",
      "[iter 5] loss=0.9211 val_loss=1.0121 scale=2.0000 norm=1.4104                    \n",
      "[iter 6] loss=0.8604 val_loss=0.9885 scale=1.0000 norm=0.7100                    \n",
      "[iter 7] loss=0.8289 val_loss=0.9199 scale=2.0000 norm=1.4200                    \n",
      "[iter 8] loss=0.7913 val_loss=0.8971 scale=1.0000 norm=0.7218                    \n",
      "[iter 9] loss=0.7660 val_loss=0.8753 scale=2.0000 norm=1.4420                    \n",
      "[iter 10] loss=0.7375 val_loss=0.8577 scale=1.0000 norm=0.7281                   \n",
      "[iter 11] loss=0.7178 val_loss=0.8477 scale=1.0000 norm=0.7261                   \n",
      "[iter 12] loss=0.6978 val_loss=0.8268 scale=1.0000 norm=0.7223                   \n",
      "[iter 13] loss=0.6802 val_loss=0.8168 scale=1.0000 norm=0.7214                   \n",
      "[iter 14] loss=0.6654 val_loss=0.8104 scale=1.0000 norm=0.7203                   \n",
      "[iter 15] loss=0.6514 val_loss=0.8087 scale=1.0000 norm=0.7210                   \n",
      "[iter 16] loss=0.6395 val_loss=0.8021 scale=1.0000 norm=0.7202                   \n",
      "[iter 17] loss=0.6265 val_loss=0.7533 scale=2.0000 norm=1.4412                   \n",
      "[iter 18] loss=0.6172 val_loss=0.7431 scale=1.0000 norm=0.7295                   \n",
      "[iter 19] loss=0.6097 val_loss=0.7427 scale=2.0000 norm=1.4563                   \n",
      "[iter 20] loss=0.5978 val_loss=0.7349 scale=1.0000 norm=0.7310                   \n",
      "[iter 21] loss=0.5921 val_loss=0.7311 scale=1.0000 norm=0.7295                   \n",
      "[iter 22] loss=0.5820 val_loss=0.7229 scale=1.0000 norm=0.7271                   \n",
      "[iter 23] loss=0.5740 val_loss=0.7109 scale=1.0000 norm=0.7259                   \n",
      "[iter 24] loss=0.5704 val_loss=0.7080 scale=1.0000 norm=0.7269                   \n",
      "[iter 25] loss=0.5654 val_loss=0.7021 scale=1.0000 norm=0.7269                   \n",
      "[iter 26] loss=0.5588 val_loss=0.6962 scale=2.0000 norm=1.4537                   \n",
      "[iter 27] loss=0.5551 val_loss=0.6813 scale=2.0000 norm=1.4590                   \n",
      "[iter 28] loss=0.5520 val_loss=0.6784 scale=2.0000 norm=1.4628                   \n",
      "[iter 29] loss=0.5485 val_loss=0.6794 scale=1.0000 norm=0.7314                   \n",
      "[iter 30] loss=0.5468 val_loss=0.6792 scale=0.5000 norm=0.3656                   \n",
      "[iter 31] loss=0.5452 val_loss=0.6797 scale=0.5000 norm=0.3651                   \n",
      "[iter 32] loss=0.5440 val_loss=0.6818 scale=1.0000 norm=0.7293                   \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL28 (val_loss=0.6784)                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.14684619891909373, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.2651 scale=1.0000 norm=1.0277                    \n",
      "[iter 1] loss=1.1967 val_loss=1.1147 scale=2.0000 norm=1.5554                    \n",
      "[iter 2] loss=1.0438 val_loss=0.9922 scale=2.0000 norm=1.3696                    \n",
      "[iter 3] loss=0.9245 val_loss=0.8872 scale=2.0000 norm=1.2751                    \n",
      "[iter 4] loss=0.8165 val_loss=0.7901 scale=2.0000 norm=1.2168                    \n",
      "[iter 5] loss=0.7171 val_loss=0.7166 scale=2.0000 norm=1.1771                    \n",
      "[iter 6] loss=0.6247 val_loss=0.6486 scale=2.0000 norm=1.1463                    \n",
      "[iter 7] loss=0.5418 val_loss=0.5966 scale=2.0000 norm=1.1269                    \n",
      "[iter 8] loss=0.4687 val_loss=0.5610 scale=2.0000 norm=1.1103                    \n",
      "[iter 9] loss=0.4020 val_loss=0.5474 scale=2.0000 norm=1.0958                    \n",
      "[iter 10] loss=0.3436 val_loss=0.5269 scale=2.0000 norm=1.0839                   \n",
      "[iter 11] loss=0.2938 val_loss=0.5303 scale=2.0000 norm=1.0767                   \n",
      "[iter 12] loss=0.2507 val_loss=0.5415 scale=2.0000 norm=1.0741                   \n",
      "[iter 13] loss=0.2133 val_loss=0.5675 scale=2.0000 norm=1.0742                   \n",
      "[iter 14] loss=0.1854 val_loss=0.6141 scale=2.0000 norm=1.0786                   \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL10 (val_loss=0.5269)                                      \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=1,\n",
      "                      max_features='sqrt', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.1908457063578154, 'minibatch_frac': 0.6, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 10}}\n",
      "[iter 0] loss=1.5178 val_loss=1.4772 scale=1.0000 norm=1.0617                    \n",
      "[iter 1] loss=1.4330 val_loss=1.4611 scale=1.0000 norm=0.9809                    \n",
      "[iter 2] loss=1.4367 val_loss=1.4571 scale=2.0000 norm=1.9872                    \n",
      "[iter 3] loss=1.3579 val_loss=1.4592 scale=1.0000 norm=0.9254                    \n",
      "[iter 4] loss=1.4197 val_loss=1.4464 scale=2.0000 norm=1.9982                    \n",
      "[iter 5] loss=1.3729 val_loss=1.4394 scale=1.0000 norm=0.9582                    \n",
      "[iter 6] loss=1.3000 val_loss=1.4454 scale=2.0000 norm=1.7880                    \n",
      "[iter 7] loss=1.3896 val_loss=1.4560 scale=1.0000 norm=1.0140                    \n",
      "[iter 8] loss=1.3631 val_loss=1.4650 scale=1.0000 norm=0.9872                    \n",
      "[iter 9] loss=1.3776 val_loss=1.4595 scale=2.0000 norm=1.9986                    \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL5 (val_loss=1.4394)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features='log2', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.14639587803545884, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.3051 scale=1.0000 norm=1.0277                    \n",
      "[iter 1] loss=1.2304 val_loss=1.1885 scale=1.0000 norm=0.8071                    \n",
      "[iter 2] loss=1.1267 val_loss=1.0666 scale=2.0000 norm=1.4741                    \n",
      "[iter 3] loss=1.0041 val_loss=0.9688 scale=2.0000 norm=1.3584                    \n",
      "[iter 4] loss=0.8976 val_loss=0.8922 scale=2.0000 norm=1.2856                    \n",
      "[iter 5] loss=0.8056 val_loss=0.8198 scale=2.0000 norm=1.2478                    \n",
      "[iter 6] loss=0.7204 val_loss=0.7694 scale=2.0000 norm=1.2163                    \n",
      "[iter 7] loss=0.6451 val_loss=0.7086 scale=2.0000 norm=1.1959                    \n",
      "[iter 8] loss=0.5708 val_loss=0.6625 scale=2.0000 norm=1.1724                    \n",
      "[iter 9] loss=0.5029 val_loss=0.5849 scale=2.0000 norm=1.1529                    \n",
      "[iter 10] loss=0.4402 val_loss=0.5671 scale=2.0000 norm=1.1357                   \n",
      "[iter 11] loss=0.3903 val_loss=0.5472 scale=2.0000 norm=1.1311                   \n",
      "[iter 12] loss=0.3494 val_loss=0.5497 scale=2.0000 norm=1.1298                   \n",
      "[iter 13] loss=0.3136 val_loss=0.5597 scale=2.0000 norm=1.1305                   \n",
      "[iter 14] loss=0.2849 val_loss=0.5522 scale=2.0000 norm=1.1333                   \n",
      "[iter 15] loss=0.2628 val_loss=0.5565 scale=1.0000 norm=0.5702                   \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL11 (val_loss=0.5472)                                      \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.08368802894190477, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 5}}\n",
      "[iter 0] loss=1.4840 val_loss=1.3167 scale=1.0000 norm=1.0277                    \n",
      "[iter 1] loss=1.2350 val_loss=1.1996 scale=2.0000 norm=1.6242                    \n",
      "[iter 2] loss=1.1158 val_loss=1.1236 scale=2.0000 norm=1.4639                    \n",
      "[iter 3] loss=1.0375 val_loss=1.0583 scale=2.0000 norm=1.3802                    \n",
      "[iter 4] loss=0.9683 val_loss=1.0001 scale=2.0000 norm=1.3212                    \n",
      "[iter 5] loss=0.9014 val_loss=0.9455 scale=2.0000 norm=1.2744                    \n",
      "[iter 6] loss=0.8380 val_loss=0.8875 scale=2.0000 norm=1.2381                    \n",
      "[iter 7] loss=0.7793 val_loss=0.8342 scale=2.0000 norm=1.2112                    \n",
      "[iter 8] loss=0.7232 val_loss=0.7856 scale=2.0000 norm=1.1888                    \n",
      "[iter 9] loss=0.6702 val_loss=0.7418 scale=2.0000 norm=1.1710                    \n",
      "[iter 10] loss=0.6199 val_loss=0.7067 scale=2.0000 norm=1.1549                   \n",
      "[iter 11] loss=0.5719 val_loss=0.6744 scale=2.0000 norm=1.1410                   \n",
      "[iter 12] loss=0.5273 val_loss=0.6460 scale=2.0000 norm=1.1300                   \n",
      "[iter 13] loss=0.4851 val_loss=0.6232 scale=2.0000 norm=1.1203                   \n",
      "[iter 14] loss=0.4457 val_loss=0.6030 scale=2.0000 norm=1.1107                   \n",
      "[iter 15] loss=0.4074 val_loss=0.5892 scale=2.0000 norm=1.1022                   \n",
      "[iter 16] loss=0.3747 val_loss=0.5764 scale=2.0000 norm=1.0977                   \n",
      "[iter 17] loss=0.3400 val_loss=0.5660 scale=2.0000 norm=1.0870                   \n",
      "[iter 18] loss=0.3117 val_loss=0.5619 scale=2.0000 norm=1.0838                   \n",
      "[iter 19] loss=0.2807 val_loss=0.5602 scale=2.0000 norm=1.0750                   \n",
      "[iter 20] loss=0.2554 val_loss=0.5647 scale=2.0000 norm=1.0710                   \n",
      "[iter 21] loss=0.2335 val_loss=0.5772 scale=2.0000 norm=1.0710                   \n",
      "[iter 22] loss=0.2124 val_loss=0.5815 scale=1.0000 norm=0.5350                   \n",
      "[iter 23] loss=0.2029 val_loss=0.5969 scale=2.0000 norm=1.0705                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL19 (val_loss=0.5602)                                      \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=3,\n",
      "                      max_features='sqrt', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.28651341609591735, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.4200 scale=1.0000 norm=1.0277                    \n",
      "[iter 1] loss=1.3016 val_loss=1.3714 scale=1.0000 norm=0.8552                    \n",
      "[iter 2] loss=1.2043 val_loss=1.3088 scale=1.0000 norm=0.7994                    \n",
      "[iter 3] loss=1.1088 val_loss=1.1565 scale=2.0000 norm=1.5077                    \n",
      "[iter 4] loss=0.9838 val_loss=1.1133 scale=1.0000 norm=0.7176                    \n",
      "[iter 5] loss=0.9142 val_loss=1.0872 scale=2.0000 norm=1.3980                    \n",
      "[iter 6] loss=0.8313 val_loss=1.0214 scale=1.0000 norm=0.6990                    \n",
      "[iter 7] loss=0.7836 val_loss=0.9366 scale=2.0000 norm=1.3766                    \n",
      "[iter 8] loss=0.7395 val_loss=0.9501 scale=1.0000 norm=0.7010                    \n",
      "[iter 9] loss=0.7253 val_loss=0.9451 scale=1.0000 norm=0.7043                    \n",
      "[iter 10] loss=0.7101 val_loss=0.9371 scale=2.0000 norm=1.4101                   \n",
      "[iter 11] loss=0.6889 val_loss=0.9187 scale=1.0000 norm=0.7074                   \n",
      "[iter 12] loss=0.6596 val_loss=0.9149 scale=1.0000 norm=0.7001                   \n",
      "[iter 13] loss=0.6502 val_loss=0.9109 scale=1.0000 norm=0.7022                   \n",
      "[iter 14] loss=0.6316 val_loss=0.8929 scale=1.0000 norm=0.7005                   \n",
      "[iter 15] loss=0.6236 val_loss=0.8988 scale=1.0000 norm=0.7014                   \n",
      "[iter 16] loss=0.6175 val_loss=0.8982 scale=1.0000 norm=0.7027                   \n",
      "[iter 17] loss=0.6098 val_loss=0.8959 scale=1.0000 norm=0.7026                   \n",
      "[iter 18] loss=0.5961 val_loss=0.9023 scale=1.0000 norm=0.6995                   \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL14 (val_loss=0.8929)                                      \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.1897047715319103, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.2308 scale=1.0000 norm=1.0277                    \n",
      "[iter 1] loss=1.1801 val_loss=1.0643 scale=2.0000 norm=1.5195                    \n",
      "[iter 2] loss=1.0017 val_loss=0.9117 scale=2.0000 norm=1.3186                    \n",
      "[iter 3] loss=0.8556 val_loss=0.7914 scale=2.0000 norm=1.2257                    \n",
      "[iter 4] loss=0.7256 val_loss=0.7028 scale=2.0000 norm=1.1753                    \n",
      "[iter 5] loss=0.6087 val_loss=0.6363 scale=2.0000 norm=1.1413                    \n",
      "[iter 6] loss=0.5049 val_loss=0.6010 scale=2.0000 norm=1.1143                    \n",
      "[iter 7] loss=0.4171 val_loss=0.5764 scale=2.0000 norm=1.0950                    \n",
      "[iter 8] loss=0.3471 val_loss=0.5627 scale=2.0000 norm=1.0869                    \n",
      "[iter 9] loss=0.2863 val_loss=0.5700 scale=2.0000 norm=1.0807                    \n",
      "[iter 10] loss=0.2372 val_loss=0.5991 scale=2.0000 norm=1.0815                   \n",
      "[iter 11] loss=0.2022 val_loss=0.6173 scale=1.0000 norm=0.5455                   \n",
      "[iter 12] loss=0.1860 val_loss=0.6446 scale=1.0000 norm=0.5454                   \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL8 (val_loss=0.5627)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=1,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.14636533850731537, 'minibatch_frac': 0.6, 'n_estimators': 1000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 10}}\n",
      "[iter 0] loss=1.5178 val_loss=1.5317 scale=1.0000 norm=1.0617                    \n",
      "[iter 1] loss=1.4267 val_loss=1.3980 scale=1.0000 norm=0.9612                    \n",
      "[iter 2] loss=1.3024 val_loss=1.2508 scale=2.0000 norm=1.7171                    \n",
      "[iter 3] loss=1.1840 val_loss=1.2084 scale=1.0000 norm=0.7797                    \n",
      "[iter 4] loss=1.1309 val_loss=1.1656 scale=2.0000 norm=1.5124                    \n",
      "[iter 5] loss=1.0649 val_loss=1.1332 scale=1.0000 norm=0.7315                    \n",
      "[iter 6] loss=1.0185 val_loss=1.1033 scale=1.0000 norm=0.7147                    \n",
      "[iter 7] loss=1.0078 val_loss=1.0671 scale=2.0000 norm=1.4364                    \n",
      "[iter 8] loss=0.9623 val_loss=1.0339 scale=2.0000 norm=1.4764                    \n",
      "[iter 9] loss=0.9042 val_loss=1.0218 scale=2.0000 norm=1.4343                    \n",
      "[iter 10] loss=0.8778 val_loss=1.0184 scale=2.0000 norm=1.4359                   \n",
      "[iter 11] loss=0.8882 val_loss=0.9571 scale=2.0000 norm=1.5366                   \n",
      "[iter 12] loss=0.8538 val_loss=0.9431 scale=1.0000 norm=0.7315                   \n",
      "[iter 13] loss=0.8059 val_loss=0.9391 scale=2.0000 norm=1.4188                   \n",
      "[iter 14] loss=0.8140 val_loss=0.9199 scale=1.0000 norm=0.7531                   \n",
      "[iter 15] loss=0.7879 val_loss=0.9085 scale=1.0000 norm=0.7273                   \n",
      "[iter 16] loss=0.7757 val_loss=0.9035 scale=2.0000 norm=1.4530                   \n",
      "[iter 17] loss=0.7810 val_loss=0.9009 scale=2.0000 norm=1.4922                   \n",
      "[iter 18] loss=0.7576 val_loss=0.8929 scale=1.0000 norm=0.7371                   \n",
      "[iter 19] loss=0.7552 val_loss=0.8872 scale=1.0000 norm=0.7489                   \n",
      "[iter 20] loss=0.7250 val_loss=0.8721 scale=1.0000 norm=0.7235                   \n",
      "[iter 21] loss=0.7488 val_loss=0.8552 scale=2.0000 norm=1.5172                   \n",
      "[iter 22] loss=0.7167 val_loss=0.8451 scale=2.0000 norm=1.4835                   \n",
      "[iter 23] loss=0.6723 val_loss=0.8351 scale=1.0000 norm=0.7007                   \n",
      "[iter 24] loss=0.6966 val_loss=0.8236 scale=1.0000 norm=0.7286                   \n",
      "[iter 25] loss=0.6953 val_loss=0.8178 scale=1.0000 norm=0.7244                   \n",
      "[iter 26] loss=0.7230 val_loss=0.8205 scale=1.0000 norm=0.7740                   \n",
      "[iter 27] loss=0.6461 val_loss=0.8145 scale=1.0000 norm=0.7051                   \n",
      "[iter 28] loss=0.6967 val_loss=0.8106 scale=1.0000 norm=0.7440                   \n",
      "[iter 29] loss=0.6603 val_loss=0.8168 scale=2.0000 norm=1.4522                   \n",
      "[iter 30] loss=0.6751 val_loss=0.8179 scale=1.0000 norm=0.7497                   \n",
      "[iter 31] loss=0.6627 val_loss=0.8216 scale=1.0000 norm=0.7404                   \n",
      "[iter 32] loss=0.6668 val_loss=0.8204 scale=1.0000 norm=0.7550                   \n",
      "[iter 33] loss=0.6758 val_loss=0.8106 scale=0.5000 norm=0.3789                   \n",
      "[iter 34] loss=0.6521 val_loss=0.8074 scale=0.5000 norm=0.3737                   \n",
      "[iter 35] loss=0.6673 val_loss=0.8061 scale=2.0000 norm=1.5089                   \n",
      "[iter 36] loss=0.6451 val_loss=0.8043 scale=2.0000 norm=1.4544                   \n",
      "[iter 37] loss=0.6474 val_loss=0.7906 scale=1.0000 norm=0.7376                   \n",
      "[iter 38] loss=0.6254 val_loss=0.7931 scale=1.0000 norm=0.7243                   \n",
      "[iter 39] loss=0.6166 val_loss=0.7822 scale=1.0000 norm=0.7175                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 40] loss=0.6574 val_loss=0.7799 scale=1.0000 norm=0.7686                   \n",
      "[iter 41] loss=0.6280 val_loss=0.7751 scale=1.0000 norm=0.7362                   \n",
      "[iter 42] loss=0.6095 val_loss=0.7723 scale=1.0000 norm=0.7264                   \n",
      "[iter 43] loss=0.6208 val_loss=0.7513 scale=2.0000 norm=1.4511                   \n",
      "[iter 44] loss=0.6502 val_loss=0.7423 scale=1.0000 norm=0.7530                   \n",
      "[iter 45] loss=0.6192 val_loss=0.7387 scale=1.0000 norm=0.7392                   \n",
      "[iter 46] loss=0.6379 val_loss=0.7397 scale=1.0000 norm=0.7664                   \n",
      "[iter 47] loss=0.6265 val_loss=0.7374 scale=0.5000 norm=0.3703                   \n",
      "[iter 48] loss=0.6122 val_loss=0.7328 scale=2.0000 norm=1.4637                   \n",
      "[iter 49] loss=0.6074 val_loss=0.7319 scale=0.5000 norm=0.3604                   \n",
      "[iter 50] loss=0.6171 val_loss=0.7299 scale=2.0000 norm=1.4977                   \n",
      "[iter 51] loss=0.6135 val_loss=0.7303 scale=1.0000 norm=0.7437                   \n",
      "[iter 52] loss=0.6001 val_loss=0.7356 scale=2.0000 norm=1.4191                   \n",
      "[iter 53] loss=0.6031 val_loss=0.7349 scale=1.0000 norm=0.7425                   \n",
      "[iter 54] loss=0.5581 val_loss=0.7392 scale=2.0000 norm=1.4217                   \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL50 (val_loss=0.7299)                                      \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=3,\n",
      "                      max_features=0.75, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.21385587890370544, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.3131 scale=1.0000 norm=1.0277                    \n",
      "[iter 1] loss=1.2082 val_loss=1.0783 scale=2.0000 norm=1.5722                    \n",
      "[iter 2] loss=1.0183 val_loss=0.9499 scale=2.0000 norm=1.3590                    \n",
      "[iter 3] loss=0.8783 val_loss=0.8374 scale=2.0000 norm=1.2895                    \n",
      "[iter 4] loss=0.7594 val_loss=0.7450 scale=2.0000 norm=1.2565                    \n",
      "[iter 5] loss=0.6625 val_loss=0.6938 scale=2.0000 norm=1.2487                    \n",
      "[iter 6] loss=0.5849 val_loss=0.6589 scale=2.0000 norm=1.2485                    \n",
      "[iter 7] loss=0.5258 val_loss=0.6228 scale=2.0000 norm=1.2600                    \n",
      "[iter 8] loss=0.4807 val_loss=0.6139 scale=2.0000 norm=1.2730                    \n",
      "[iter 9] loss=0.4478 val_loss=0.6211 scale=2.0000 norm=1.2861                    \n",
      "[iter 10] loss=0.4257 val_loss=0.6174 scale=1.0000 norm=0.6513                   \n",
      "[iter 11] loss=0.4110 val_loss=0.6061 scale=1.0000 norm=0.6487                   \n",
      "[iter 12] loss=0.4005 val_loss=0.6141 scale=2.0000 norm=1.2993                   \n",
      "[iter 13] loss=0.3879 val_loss=0.6179 scale=2.0000 norm=1.3111                   \n",
      "[iter 14] loss=0.3793 val_loss=0.6200 scale=1.0000 norm=0.6622                   \n",
      "[iter 15] loss=0.3727 val_loss=0.6244 scale=1.0000 norm=0.6605                   \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL11 (val_loss=0.6061)                                      \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=3,\n",
      "                      max_features='log2', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.27564748656651933, 'minibatch_frac': 0.8, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5200 val_loss=1.4174 scale=1.0000 norm=1.0640                    \n",
      "[iter 1] loss=1.3154 val_loss=1.3730 scale=2.0000 norm=1.7275                    \n",
      "[iter 2] loss=1.2124 val_loss=1.1793 scale=2.0000 norm=1.6447                    \n",
      "[iter 3] loss=1.0683 val_loss=1.0611 scale=1.0000 norm=0.7509                    \n",
      "[iter 4] loss=1.0056 val_loss=1.0333 scale=2.0000 norm=1.4321                    \n",
      "[iter 5] loss=0.9445 val_loss=1.0201 scale=1.0000 norm=0.7276                    \n",
      "[iter 6] loss=0.9014 val_loss=1.0130 scale=2.0000 norm=1.4432                    \n",
      "[iter 7] loss=0.8750 val_loss=0.9884 scale=1.0000 norm=0.7283                    \n",
      "[iter 8] loss=0.8688 val_loss=0.9729 scale=1.0000 norm=0.7315                    \n",
      "[iter 9] loss=0.8590 val_loss=0.9228 scale=1.0000 norm=0.7361                    \n",
      "[iter 10] loss=0.7895 val_loss=0.9162 scale=2.0000 norm=1.4029                   \n",
      "[iter 11] loss=0.7648 val_loss=0.9124 scale=1.0000 norm=0.7116                   \n",
      "[iter 12] loss=0.7357 val_loss=0.8913 scale=1.0000 norm=0.6964                   \n",
      "[iter 13] loss=0.6954 val_loss=0.8209 scale=1.0000 norm=0.6839                   \n",
      "[iter 14] loss=0.6437 val_loss=0.8201 scale=1.0000 norm=0.6680                   \n",
      "[iter 15] loss=0.6454 val_loss=0.8108 scale=1.0000 norm=0.6870                   \n",
      "[iter 16] loss=0.6198 val_loss=0.8098 scale=1.0000 norm=0.6817                   \n",
      "[iter 17] loss=0.6352 val_loss=0.8194 scale=1.0000 norm=0.6972                   \n",
      "[iter 18] loss=0.6243 val_loss=0.8158 scale=1.0000 norm=0.6884                   \n",
      "[iter 19] loss=0.6153 val_loss=0.8177 scale=1.0000 norm=0.6907                   \n",
      "[iter 20] loss=0.5940 val_loss=0.8196 scale=1.0000 norm=0.6907                   \n",
      "[iter 21] loss=0.6037 val_loss=0.8067 scale=1.0000 norm=0.6969                   \n",
      "[iter 22] loss=0.5889 val_loss=0.7944 scale=1.0000 norm=0.6962                   \n",
      "[iter 23] loss=0.5716 val_loss=0.7967 scale=1.0000 norm=0.6875                   \n",
      "[iter 24] loss=0.5789 val_loss=0.7989 scale=0.5000 norm=0.3413                   \n",
      "[iter 25] loss=0.5738 val_loss=0.7994 scale=1.0000 norm=0.6922                   \n",
      "[iter 26] loss=0.5797 val_loss=0.7999 scale=1.0000 norm=0.6968                   \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL22 (val_loss=0.7944)                                      \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=3,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.1834428459288963, 'minibatch_frac': 1.0, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 5}}\n",
      "[iter 0] loss=1.4840 val_loss=1.3394 scale=1.0000 norm=1.0277                    \n",
      "[iter 1] loss=1.2776 val_loss=1.1723 scale=2.0000 norm=1.6778                    \n",
      "[iter 2] loss=1.1003 val_loss=1.0338 scale=2.0000 norm=1.4470                    \n",
      "[iter 3] loss=0.9662 val_loss=0.9388 scale=2.0000 norm=1.3396                    \n",
      "[iter 4] loss=0.8640 val_loss=0.8477 scale=2.0000 norm=1.3059                    \n",
      "[iter 5] loss=0.7636 val_loss=0.7834 scale=2.0000 norm=1.2762                    \n",
      "[iter 6] loss=0.6862 val_loss=0.7456 scale=2.0000 norm=1.2721                    \n",
      "[iter 7] loss=0.6179 val_loss=0.7117 scale=2.0000 norm=1.2681                    \n",
      "[iter 8] loss=0.5600 val_loss=0.7011 scale=1.0000 norm=0.6342                    \n",
      "[iter 9] loss=0.5368 val_loss=0.6867 scale=2.0000 norm=1.2684                    \n",
      "[iter 10] loss=0.4996 val_loss=0.6878 scale=2.0000 norm=1.2816                   \n",
      "[iter 11] loss=0.4682 val_loss=0.6488 scale=1.0000 norm=0.6446                   \n",
      "[iter 12] loss=0.4536 val_loss=0.6387 scale=1.0000 norm=0.6452                   \n",
      "[iter 13] loss=0.4421 val_loss=0.6417 scale=1.0000 norm=0.6472                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 14] loss=0.4297 val_loss=0.6405 scale=1.0000 norm=0.6461                   \n",
      "[iter 15] loss=0.4227 val_loss=0.6419 scale=1.0000 norm=0.6494                   \n",
      "[iter 16] loss=0.4152 val_loss=0.6381 scale=1.0000 norm=0.6512                   \n",
      "[iter 17] loss=0.4073 val_loss=0.6434 scale=1.0000 norm=0.6522                   \n",
      "[iter 18] loss=0.4025 val_loss=0.6446 scale=1.0000 norm=0.6552                   \n",
      "[iter 19] loss=0.3970 val_loss=0.6377 scale=1.0000 norm=0.6568                   \n",
      "[iter 20] loss=0.3926 val_loss=0.6346 scale=1.0000 norm=0.6587                   \n",
      "[iter 21] loss=0.3890 val_loss=0.6370 scale=1.0000 norm=0.6610                   \n",
      "[iter 22] loss=0.3850 val_loss=0.6395 scale=1.0000 norm=0.6614                   \n",
      "[iter 23] loss=0.3805 val_loss=0.6405 scale=1.0000 norm=0.6616                   \n",
      "[iter 24] loss=0.3742 val_loss=0.6437 scale=1.0000 norm=0.6586                   \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL20 (val_loss=0.6346)                                      \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=5,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.09258589123218278, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.3654 scale=1.0000 norm=1.0277                    \n",
      "[iter 1] loss=1.2444 val_loss=1.2069 scale=2.0000 norm=1.6412                    \n",
      "[iter 2] loss=1.1159 val_loss=1.1199 scale=2.0000 norm=1.4667                    \n",
      "[iter 3] loss=1.0360 val_loss=1.0459 scale=2.0000 norm=1.3853                    \n",
      "[iter 4] loss=0.9631 val_loss=0.9782 scale=2.0000 norm=1.3253                    \n",
      "[iter 5] loss=0.8933 val_loss=0.9098 scale=2.0000 norm=1.2796                    \n",
      "[iter 6] loss=0.8275 val_loss=0.8573 scale=2.0000 norm=1.2460                    \n",
      "[iter 7] loss=0.7666 val_loss=0.8070 scale=2.0000 norm=1.2208                    \n",
      "[iter 8] loss=0.7091 val_loss=0.7667 scale=2.0000 norm=1.2000                    \n",
      "[iter 9] loss=0.6569 val_loss=0.7305 scale=2.0000 norm=1.1845                    \n",
      "[iter 10] loss=0.6062 val_loss=0.6972 scale=2.0000 norm=1.1706                   \n",
      "[iter 11] loss=0.5592 val_loss=0.6685 scale=2.0000 norm=1.1605                   \n",
      "[iter 12] loss=0.5169 val_loss=0.6429 scale=2.0000 norm=1.1520                   \n",
      "[iter 13] loss=0.4783 val_loss=0.6254 scale=2.0000 norm=1.1458                   \n",
      "[iter 14] loss=0.4440 val_loss=0.6101 scale=2.0000 norm=1.1420                   \n",
      "[iter 15] loss=0.4122 val_loss=0.6020 scale=2.0000 norm=1.1404                   \n",
      "[iter 16] loss=0.3837 val_loss=0.5950 scale=2.0000 norm=1.1380                   \n",
      "[iter 17] loss=0.3578 val_loss=0.5913 scale=2.0000 norm=1.1371                   \n",
      "[iter 18] loss=0.3344 val_loss=0.5921 scale=2.0000 norm=1.1382                   \n",
      "[iter 19] loss=0.3129 val_loss=0.5951 scale=1.0000 norm=0.5687                   \n",
      "[iter 20] loss=0.3022 val_loss=0.6052 scale=2.0000 norm=1.1361                   \n",
      "[iter 21] loss=0.2855 val_loss=0.6045 scale=2.0000 norm=1.1394                   \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL17 (val_loss=0.5913)                                      \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=1,\n",
      "                      max_features=0.75, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.16487456112056467, 'minibatch_frac': 0.8, 'n_estimators': 1000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 10}}\n",
      "[iter 0] loss=1.5200 val_loss=1.5271 scale=1.0000 norm=1.0640                    \n",
      "[iter 1] loss=1.4177 val_loss=1.3934 scale=1.0000 norm=0.9584                    \n",
      "[iter 2] loss=1.2612 val_loss=1.2923 scale=1.0000 norm=0.8341                    \n",
      "[iter 3] loss=1.1821 val_loss=1.2019 scale=2.0000 norm=1.5640                    \n",
      "[iter 4] loss=1.0982 val_loss=1.1571 scale=1.0000 norm=0.7465                    \n",
      "[iter 5] loss=1.0437 val_loss=1.0986 scale=2.0000 norm=1.4656                    \n",
      "[iter 6] loss=0.9780 val_loss=1.0617 scale=1.0000 norm=0.7127                    \n",
      "[iter 7] loss=0.9424 val_loss=1.0143 scale=2.0000 norm=1.4227                    \n",
      "[iter 8] loss=0.9014 val_loss=0.9890 scale=1.0000 norm=0.7191                    \n",
      "[iter 9] loss=0.8764 val_loss=0.9690 scale=2.0000 norm=1.4306                    \n",
      "[iter 10] loss=0.8176 val_loss=0.9548 scale=2.0000 norm=1.4167                   \n",
      "[iter 11] loss=0.8060 val_loss=0.9094 scale=1.0000 norm=0.7343                   \n",
      "[iter 12] loss=0.7726 val_loss=0.8956 scale=1.0000 norm=0.7141                   \n",
      "[iter 13] loss=0.7260 val_loss=0.8764 scale=2.0000 norm=1.3824                   \n",
      "[iter 14] loss=0.7325 val_loss=0.8648 scale=1.0000 norm=0.7317                   \n",
      "[iter 15] loss=0.7230 val_loss=0.8302 scale=1.0000 norm=0.7319                   \n",
      "[iter 16] loss=0.6839 val_loss=0.8184 scale=1.0000 norm=0.7128                   \n",
      "[iter 17] loss=0.7041 val_loss=0.8224 scale=2.0000 norm=1.4796                   \n",
      "[iter 18] loss=0.6751 val_loss=0.8145 scale=1.0000 norm=0.7226                   \n",
      "[iter 19] loss=0.6670 val_loss=0.7943 scale=1.0000 norm=0.7274                   \n",
      "[iter 20] loss=0.6468 val_loss=0.7783 scale=2.0000 norm=1.4285                   \n",
      "[iter 21] loss=0.6515 val_loss=0.7694 scale=2.0000 norm=1.4696                   \n",
      "[iter 22] loss=0.6447 val_loss=0.7594 scale=1.0000 norm=0.7401                   \n",
      "[iter 23] loss=0.6069 val_loss=0.7509 scale=1.0000 norm=0.7091                   \n",
      "[iter 24] loss=0.6250 val_loss=0.7446 scale=1.0000 norm=0.7322                   \n",
      "[iter 25] loss=0.6186 val_loss=0.7434 scale=0.5000 norm=0.3678                   \n",
      "[iter 26] loss=0.6234 val_loss=0.7392 scale=1.0000 norm=0.7380                   \n",
      "[iter 27] loss=0.5854 val_loss=0.7306 scale=1.0000 norm=0.7064                   \n",
      "[iter 28] loss=0.6001 val_loss=0.7296 scale=1.0000 norm=0.7283                   \n",
      "[iter 29] loss=0.5763 val_loss=0.7248 scale=1.0000 norm=0.7079                   \n",
      "[iter 30] loss=0.5830 val_loss=0.7306 scale=2.0000 norm=1.4547                   \n",
      "[iter 31] loss=0.5919 val_loss=0.7338 scale=1.0000 norm=0.7350                   \n",
      "[iter 32] loss=0.5840 val_loss=0.7269 scale=2.0000 norm=1.4761                   \n",
      "[iter 33] loss=0.5785 val_loss=0.7184 scale=1.0000 norm=0.7316                   \n",
      "[iter 34] loss=0.5716 val_loss=0.7172 scale=0.5000 norm=0.3672                   \n",
      "[iter 35] loss=0.5714 val_loss=0.7043 scale=1.0000 norm=0.7337                   \n",
      "[iter 36] loss=0.5568 val_loss=0.7040 scale=0.5000 norm=0.3621                   \n",
      "[iter 37] loss=0.5468 val_loss=0.6868 scale=1.0000 norm=0.7122                   \n",
      "[iter 38] loss=0.5390 val_loss=0.6883 scale=1.0000 norm=0.7162                   \n",
      "[iter 39] loss=0.5563 val_loss=0.6885 scale=0.5000 norm=0.3645                   \n",
      "[iter 40] loss=0.5696 val_loss=0.6864 scale=0.5000 norm=0.3722                   \n",
      "[iter 41] loss=0.5263 val_loss=0.6859 scale=2.0000 norm=1.4123                   \n",
      "[iter 42] loss=0.5307 val_loss=0.6751 scale=1.0000 norm=0.7241                   \n",
      "[iter 43] loss=0.5444 val_loss=0.6722 scale=0.5000 norm=0.3645                   \n",
      "[iter 44] loss=0.5501 val_loss=0.6664 scale=2.0000 norm=1.4668                   \n",
      "[iter 45] loss=0.5596 val_loss=0.6641 scale=1.0000 norm=0.7470                   \n",
      "[iter 46] loss=0.5453 val_loss=0.6658 scale=1.0000 norm=0.7357                   \n",
      "[iter 47] loss=0.5305 val_loss=0.6573 scale=2.0000 norm=1.4416                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 48] loss=0.5276 val_loss=0.6572 scale=2.0000 norm=1.4442                   \n",
      "[iter 49] loss=0.5288 val_loss=0.6568 scale=1.0000 norm=0.7284                   \n",
      "[iter 50] loss=0.5336 val_loss=0.6585 scale=1.0000 norm=0.7307                   \n",
      "[iter 51] loss=0.5319 val_loss=0.6585 scale=1.0000 norm=0.7285                   \n",
      "[iter 52] loss=0.5165 val_loss=0.6579 scale=1.0000 norm=0.7094                   \n",
      "[iter 53] loss=0.5181 val_loss=0.6512 scale=1.0000 norm=0.7181                   \n",
      "[iter 54] loss=0.5015 val_loss=0.6499 scale=1.0000 norm=0.7078                   \n",
      "[iter 55] loss=0.5181 val_loss=0.6483 scale=0.5000 norm=0.3629                   \n",
      "[iter 56] loss=0.5204 val_loss=0.6471 scale=1.0000 norm=0.7326                   \n",
      "[iter 57] loss=0.5226 val_loss=0.6453 scale=1.0000 norm=0.7262                   \n",
      "[iter 58] loss=0.5100 val_loss=0.6452 scale=2.0000 norm=1.4489                   \n",
      "[iter 59] loss=0.5114 val_loss=0.6434 scale=2.0000 norm=1.4392                   \n",
      "[iter 60] loss=0.5299 val_loss=0.6452 scale=2.0000 norm=1.4672                   \n",
      "[iter 61] loss=0.5146 val_loss=0.6420 scale=1.0000 norm=0.7223                   \n",
      "[iter 62] loss=0.5013 val_loss=0.6417 scale=1.0000 norm=0.7115                   \n",
      "[iter 63] loss=0.4953 val_loss=0.6415 scale=0.2500 norm=0.1786                   \n",
      "[iter 64] loss=0.4984 val_loss=0.6427 scale=0.5000 norm=0.3594                   \n",
      "[iter 65] loss=0.5176 val_loss=0.6421 scale=0.2500 norm=0.1829                   \n",
      "[iter 66] loss=0.4988 val_loss=0.6419 scale=1.0000 norm=0.7204                   \n",
      "[iter 67] loss=0.5034 val_loss=0.6390 scale=1.0000 norm=0.7249                   \n",
      "[iter 68] loss=0.4993 val_loss=0.6393 scale=1.0000 norm=0.7183                   \n",
      "[iter 69] loss=0.5096 val_loss=0.6362 scale=0.2500 norm=0.1814                   \n",
      "[iter 70] loss=0.4994 val_loss=0.6370 scale=1.0000 norm=0.7142                   \n",
      "[iter 71] loss=0.5144 val_loss=0.6367 scale=0.2500 norm=0.1827                   \n",
      "[iter 72] loss=0.4976 val_loss=0.6331 scale=2.0000 norm=1.4386                   \n",
      "[iter 73] loss=0.4912 val_loss=0.6318 scale=2.0000 norm=1.4292                   \n",
      "[iter 74] loss=0.5036 val_loss=0.6316 scale=0.2500 norm=0.1824                   \n",
      "[iter 75] loss=0.5034 val_loss=0.6267 scale=1.0000 norm=0.7253                   \n",
      "[iter 76] loss=0.5129 val_loss=0.6266 scale=0.0625 norm=0.0460                   \n",
      "[iter 77] loss=0.4994 val_loss=0.6252 scale=2.0000 norm=1.4397                   \n",
      "[iter 78] loss=0.5035 val_loss=0.6231 scale=1.0000 norm=0.7249                   \n",
      "[iter 79] loss=0.4879 val_loss=0.6207 scale=1.0000 norm=0.7149                   \n",
      "[iter 80] loss=0.4814 val_loss=0.6208 scale=0.2500 norm=0.1767                   \n",
      "[iter 81] loss=0.4979 val_loss=0.6203 scale=0.2500 norm=0.1818                   \n",
      "[iter 82] loss=0.4980 val_loss=0.6192 scale=0.5000 norm=0.3633                   \n",
      "[iter 83] loss=0.4832 val_loss=0.6186 scale=1.0000 norm=0.7101                   \n",
      "[iter 84] loss=0.4852 val_loss=0.6170 scale=1.0000 norm=0.7112                   \n",
      "[iter 85] loss=0.4619 val_loss=0.6154 scale=1.0000 norm=0.6880                   \n",
      "[iter 86] loss=0.4896 val_loss=0.6154 scale=0.0020 norm=0.0014                   \n",
      "[iter 87] loss=0.4771 val_loss=0.6162 scale=1.0000 norm=0.7130                   \n",
      "[iter 88] loss=0.4660 val_loss=0.6162 scale=0.0625 norm=0.0441                   \n",
      "[iter 89] loss=0.4883 val_loss=0.6198 scale=2.0000 norm=1.4459                   \n",
      "[iter 90] loss=0.4909 val_loss=0.6198 scale=0.0010 norm=0.0007                   \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL86 (val_loss=0.6154)                                      \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=5,\n",
      "                      max_features='sqrt', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.026092561543443846, 'minibatch_frac': 0.6, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5178 val_loss=1.5096 scale=1.0000 norm=1.0617                    \n",
      "[iter 1] loss=1.3401 val_loss=1.4514 scale=1.0000 norm=0.9011                    \n",
      "[iter 2] loss=1.3053 val_loss=1.4132 scale=1.0000 norm=0.8758                    \n",
      "[iter 3] loss=1.2741 val_loss=1.3674 scale=1.0000 norm=0.8425                    \n",
      "[iter 4] loss=1.2475 val_loss=1.3464 scale=1.0000 norm=0.8300                    \n",
      "[iter 5] loss=1.2058 val_loss=1.3174 scale=1.0000 norm=0.7965                    \n",
      "[iter 6] loss=1.1871 val_loss=1.2999 scale=1.0000 norm=0.7745                    \n",
      "[iter 7] loss=1.1891 val_loss=1.2869 scale=1.0000 norm=0.7919                    \n",
      "[iter 8] loss=1.1734 val_loss=1.2658 scale=2.0000 norm=1.5620                    \n",
      "[iter 9] loss=1.1570 val_loss=1.2527 scale=1.0000 norm=0.7712                    \n",
      "[iter 10] loss=1.1335 val_loss=1.2310 scale=2.0000 norm=1.5137                   \n",
      "[iter 11] loss=1.1258 val_loss=1.2190 scale=1.0000 norm=0.7622                   \n",
      "[iter 12] loss=1.1073 val_loss=1.2057 scale=1.0000 norm=0.7422                   \n",
      "[iter 13] loss=1.0803 val_loss=1.1829 scale=2.0000 norm=1.4531                   \n",
      "[iter 14] loss=1.0781 val_loss=1.1634 scale=2.0000 norm=1.4583                   \n",
      "[iter 15] loss=1.0520 val_loss=1.1431 scale=2.0000 norm=1.4293                   \n",
      "[iter 16] loss=1.0375 val_loss=1.1229 scale=2.0000 norm=1.4251                   \n",
      "[iter 17] loss=1.0229 val_loss=1.1058 scale=2.0000 norm=1.4283                   \n",
      "[iter 18] loss=0.9982 val_loss=1.0963 scale=1.0000 norm=0.6999                   \n",
      "[iter 19] loss=0.9899 val_loss=1.0816 scale=2.0000 norm=1.3925                   \n",
      "[iter 20] loss=0.9731 val_loss=1.0653 scale=2.0000 norm=1.3696                   \n",
      "[iter 21] loss=0.9655 val_loss=1.0510 scale=2.0000 norm=1.3878                   \n",
      "[iter 22] loss=0.9438 val_loss=1.0335 scale=2.0000 norm=1.3628                   \n",
      "[iter 23] loss=0.9136 val_loss=1.0184 scale=2.0000 norm=1.3130                   \n",
      "[iter 24] loss=0.9046 val_loss=1.0039 scale=2.0000 norm=1.3268                   \n",
      "[iter 25] loss=0.8898 val_loss=0.9866 scale=2.0000 norm=1.3257                   \n",
      "[iter 26] loss=0.8900 val_loss=0.9719 scale=2.0000 norm=1.3480                   \n",
      "[iter 27] loss=0.8522 val_loss=0.9590 scale=2.0000 norm=1.2852                   \n",
      "[iter 28] loss=0.8532 val_loss=0.9444 scale=2.0000 norm=1.3171                   \n",
      "[iter 29] loss=0.8326 val_loss=0.9299 scale=2.0000 norm=1.3015                   \n",
      "[iter 30] loss=0.8195 val_loss=0.9205 scale=2.0000 norm=1.2945                   \n",
      "[iter 31] loss=0.8039 val_loss=0.9078 scale=2.0000 norm=1.2892                   \n",
      "[iter 32] loss=0.7850 val_loss=0.8946 scale=2.0000 norm=1.2798                   \n",
      "[iter 33] loss=0.7754 val_loss=0.8874 scale=1.0000 norm=0.6398                   \n",
      "[iter 34] loss=0.7678 val_loss=0.8815 scale=1.0000 norm=0.6347                   \n",
      "[iter 35] loss=0.7631 val_loss=0.8674 scale=2.0000 norm=1.2733                   \n",
      "[iter 36] loss=0.7413 val_loss=0.8547 scale=2.0000 norm=1.2522                   \n",
      "[iter 37] loss=0.7229 val_loss=0.8454 scale=2.0000 norm=1.2405                   \n",
      "[iter 38] loss=0.7091 val_loss=0.8350 scale=2.0000 norm=1.2254                   \n",
      "[iter 39] loss=0.6939 val_loss=0.8228 scale=2.0000 norm=1.2259                   \n",
      "[iter 40] loss=0.6917 val_loss=0.8172 scale=1.0000 norm=0.6260                   \n",
      "[iter 41] loss=0.6773 val_loss=0.8049 scale=2.0000 norm=1.2272                   \n",
      "[iter 42] loss=0.6538 val_loss=0.7944 scale=2.0000 norm=1.2120                   \n",
      "[iter 43] loss=0.6465 val_loss=0.7845 scale=2.0000 norm=1.2151                   \n",
      "[iter 44] loss=0.6448 val_loss=0.7744 scale=2.0000 norm=1.2215                   \n",
      "[iter 45] loss=0.6351 val_loss=0.7654 scale=2.0000 norm=1.2290                   \n",
      "[iter 46] loss=0.6215 val_loss=0.7558 scale=2.0000 norm=1.2306                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 47] loss=0.6144 val_loss=0.7470 scale=2.0000 norm=1.2236                   \n",
      "[iter 48] loss=0.5853 val_loss=0.7395 scale=2.0000 norm=1.1905                   \n",
      "[iter 49] loss=0.5780 val_loss=0.7251 scale=2.0000 norm=1.2004                   \n",
      "[iter 50] loss=0.5707 val_loss=0.7167 scale=2.0000 norm=1.2094                   \n",
      "[iter 51] loss=0.5600 val_loss=0.7099 scale=2.0000 norm=1.2041                   \n",
      "[iter 52] loss=0.5367 val_loss=0.7029 scale=2.0000 norm=1.1672                   \n",
      "[iter 53] loss=0.5345 val_loss=0.6991 scale=1.0000 norm=0.5955                   \n",
      "[iter 54] loss=0.5219 val_loss=0.6959 scale=1.0000 norm=0.5887                   \n",
      "[iter 55] loss=0.5251 val_loss=0.6878 scale=2.0000 norm=1.1928                   \n",
      "[iter 56] loss=0.5155 val_loss=0.6820 scale=2.0000 norm=1.1958                   \n",
      "[iter 57] loss=0.5085 val_loss=0.6781 scale=1.0000 norm=0.5935                   \n",
      "[iter 58] loss=0.5007 val_loss=0.6742 scale=2.0000 norm=1.1887                   \n",
      "[iter 59] loss=0.4992 val_loss=0.6708 scale=1.0000 norm=0.5965                   \n",
      "[iter 60] loss=0.4952 val_loss=0.6654 scale=1.0000 norm=0.5943                   \n",
      "[iter 61] loss=0.5009 val_loss=0.6595 scale=2.0000 norm=1.2097                   \n",
      "[iter 62] loss=0.4830 val_loss=0.6568 scale=1.0000 norm=0.5922                   \n",
      "[iter 63] loss=0.4749 val_loss=0.6546 scale=1.0000 norm=0.5922                   \n",
      "[iter 64] loss=0.4578 val_loss=0.6490 scale=2.0000 norm=1.1667                   \n",
      "[iter 65] loss=0.4598 val_loss=0.6453 scale=1.0000 norm=0.5867                   \n",
      "[iter 66] loss=0.4510 val_loss=0.6420 scale=2.0000 norm=1.1708                   \n",
      "[iter 67] loss=0.4505 val_loss=0.6401 scale=1.0000 norm=0.5918                   \n",
      "[iter 68] loss=0.4389 val_loss=0.6358 scale=2.0000 norm=1.1644                   \n",
      "[iter 69] loss=0.4521 val_loss=0.6331 scale=1.0000 norm=0.5983                   \n",
      "[iter 70] loss=0.4368 val_loss=0.6261 scale=2.0000 norm=1.1856                   \n",
      "[iter 71] loss=0.4246 val_loss=0.6194 scale=2.0000 norm=1.1829                   \n",
      "[iter 72] loss=0.4058 val_loss=0.6158 scale=1.0000 norm=0.5802                   \n",
      "[iter 73] loss=0.4138 val_loss=0.6129 scale=1.0000 norm=0.5877                   \n",
      "[iter 74] loss=0.4102 val_loss=0.6102 scale=2.0000 norm=1.1679                   \n",
      "[iter 75] loss=0.4079 val_loss=0.6072 scale=1.0000 norm=0.5929                   \n",
      "[iter 76] loss=0.4054 val_loss=0.6039 scale=1.0000 norm=0.5946                   \n",
      "[iter 77] loss=0.3904 val_loss=0.5998 scale=2.0000 norm=1.1567                   \n",
      "[iter 78] loss=0.4058 val_loss=0.5972 scale=1.0000 norm=0.5968                   \n",
      "[iter 79] loss=0.3825 val_loss=0.5965 scale=1.0000 norm=0.5839                   \n",
      "[iter 80] loss=0.3786 val_loss=0.5945 scale=1.0000 norm=0.5798                   \n",
      "[iter 81] loss=0.3794 val_loss=0.5938 scale=1.0000 norm=0.5916                   \n",
      "[iter 82] loss=0.3731 val_loss=0.5918 scale=1.0000 norm=0.5862                   \n",
      "[iter 83] loss=0.3643 val_loss=0.5924 scale=1.0000 norm=0.5766                   \n",
      "[iter 84] loss=0.3600 val_loss=0.5889 scale=2.0000 norm=1.1585                   \n",
      "[iter 85] loss=0.3534 val_loss=0.5881 scale=2.0000 norm=1.1581                   \n",
      "[iter 86] loss=0.3683 val_loss=0.5864 scale=1.0000 norm=0.5931                   \n",
      "[iter 87] loss=0.3581 val_loss=0.5851 scale=1.0000 norm=0.5869                   \n",
      "[iter 88] loss=0.3428 val_loss=0.5817 scale=2.0000 norm=1.1615                   \n",
      "[iter 89] loss=0.3534 val_loss=0.5809 scale=1.0000 norm=0.5934                   \n",
      "[iter 90] loss=0.3435 val_loss=0.5795 scale=1.0000 norm=0.5831                   \n",
      "[iter 91] loss=0.3402 val_loss=0.5787 scale=1.0000 norm=0.5803                   \n",
      "[iter 92] loss=0.3423 val_loss=0.5774 scale=1.0000 norm=0.5829                   \n",
      "[iter 93] loss=0.3473 val_loss=0.5773 scale=1.0000 norm=0.5944                   \n",
      "[iter 94] loss=0.3293 val_loss=0.5723 scale=2.0000 norm=1.1629                   \n",
      "[iter 95] loss=0.3321 val_loss=0.5718 scale=1.0000 norm=0.5890                   \n",
      "[iter 96] loss=0.3452 val_loss=0.5713 scale=1.0000 norm=0.5937                   \n",
      "[iter 97] loss=0.3253 val_loss=0.5709 scale=1.0000 norm=0.5865                   \n",
      "[iter 98] loss=0.3075 val_loss=0.5692 scale=1.0000 norm=0.5718                   \n",
      "[iter 99] loss=0.3298 val_loss=0.5683 scale=1.0000 norm=0.5925                   \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=5,\n",
      "                      max_features='log2', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.2278458343948816, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.3159 scale=1.0000 norm=1.0277                    \n",
      "[iter 1] loss=1.2012 val_loss=1.0730 scale=2.0000 norm=1.5552                    \n",
      "[iter 2] loss=1.0201 val_loss=0.9657 scale=2.0000 norm=1.3643                    \n",
      "[iter 3] loss=0.8964 val_loss=0.8367 scale=2.0000 norm=1.3173                    \n",
      "[iter 4] loss=0.7594 val_loss=0.7634 scale=2.0000 norm=1.2527                    \n",
      "[iter 5] loss=0.6554 val_loss=0.7032 scale=2.0000 norm=1.2304                    \n",
      "[iter 6] loss=0.5604 val_loss=0.6808 scale=2.0000 norm=1.2068                    \n",
      "[iter 7] loss=0.4951 val_loss=0.6729 scale=2.0000 norm=1.2106                    \n",
      "[iter 8] loss=0.4450 val_loss=0.6661 scale=1.0000 norm=0.6095                    \n",
      "[iter 9] loss=0.4270 val_loss=0.6670 scale=1.0000 norm=0.6121                    \n",
      "[iter 10] loss=0.4084 val_loss=0.6662 scale=1.0000 norm=0.6137                   \n",
      "[iter 11] loss=0.3942 val_loss=0.6648 scale=1.0000 norm=0.6162                   \n",
      "[iter 12] loss=0.3801 val_loss=0.6617 scale=1.0000 norm=0.6178                   \n",
      "[iter 13] loss=0.3647 val_loss=0.6547 scale=1.0000 norm=0.6171                   \n",
      "[iter 14] loss=0.3524 val_loss=0.6547 scale=1.0000 norm=0.6175                   \n",
      "[iter 15] loss=0.3394 val_loss=0.6664 scale=1.0000 norm=0.6174                   \n",
      "[iter 16] loss=0.3256 val_loss=0.6656 scale=1.0000 norm=0.6142                   \n",
      "[iter 17] loss=0.3141 val_loss=0.6748 scale=1.0000 norm=0.6126                   \n",
      "[iter 18] loss=0.3037 val_loss=0.6790 scale=1.0000 norm=0.6111                   \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL14 (val_loss=0.6547)                                      \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=1,\n",
      "                      max_features='sqrt', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.2077984990319478, 'minibatch_frac': 0.7, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 5}}\n",
      "[iter 0] loss=1.5211 val_loss=1.4687 scale=1.0000 norm=1.0651                    \n",
      "[iter 1] loss=1.4286 val_loss=1.4552 scale=1.0000 norm=0.9764                    \n",
      "[iter 2] loss=1.4141 val_loss=1.4521 scale=2.0000 norm=1.9450                    \n",
      "[iter 3] loss=1.3433 val_loss=1.4604 scale=2.0000 norm=1.8363                    \n",
      "[iter 4] loss=1.4077 val_loss=1.4452 scale=1.0000 norm=1.0083                    \n",
      "[iter 5] loss=1.3735 val_loss=1.4393 scale=1.0000 norm=0.9782                    \n",
      "[iter 6] loss=1.3057 val_loss=1.4203 scale=2.0000 norm=1.8350                    \n",
      "[iter 7] loss=1.3827 val_loss=1.4325 scale=1.0000 norm=1.0147                    \n",
      "[iter 8] loss=1.3677 val_loss=1.4413 scale=1.0000 norm=0.9982                    \n",
      "[iter 9] loss=1.3520 val_loss=1.4424 scale=1.0000 norm=0.9773                    \n",
      "[iter 10] loss=1.3358 val_loss=1.4432 scale=1.0000 norm=0.9652                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL6 (val_loss=1.4203)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.13113660889860998, 'minibatch_frac': 1.0, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.2807 scale=1.0000 norm=1.0277                    \n",
      "[iter 1] loss=1.2040 val_loss=1.1485 scale=2.0000 norm=1.5699                    \n",
      "[iter 2] loss=1.0598 val_loss=1.0455 scale=2.0000 norm=1.3906                    \n",
      "[iter 3] loss=0.9506 val_loss=0.9449 scale=2.0000 norm=1.2963                    \n",
      "[iter 4] loss=0.8514 val_loss=0.8414 scale=2.0000 norm=1.2361                    \n",
      "[iter 5] loss=0.7591 val_loss=0.7702 scale=2.0000 norm=1.1943                    \n",
      "[iter 6] loss=0.6731 val_loss=0.7078 scale=2.0000 norm=1.1634                    \n",
      "[iter 7] loss=0.5943 val_loss=0.6497 scale=2.0000 norm=1.1395                    \n",
      "[iter 8] loss=0.5236 val_loss=0.6057 scale=2.0000 norm=1.1227                    \n",
      "[iter 9] loss=0.4588 val_loss=0.5767 scale=2.0000 norm=1.1072                    \n",
      "[iter 10] loss=0.4026 val_loss=0.5529 scale=2.0000 norm=1.0968                   \n",
      "[iter 11] loss=0.3511 val_loss=0.5374 scale=2.0000 norm=1.0886                   \n",
      "[iter 12] loss=0.3074 val_loss=0.5455 scale=2.0000 norm=1.0848                   \n",
      "[iter 13] loss=0.2670 val_loss=0.5523 scale=2.0000 norm=1.0801                   \n",
      "[iter 14] loss=0.2311 val_loss=0.5547 scale=1.0000 norm=0.5368                   \n",
      "[iter 15] loss=0.2167 val_loss=0.5577 scale=1.0000 norm=0.5367                   \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL11 (val_loss=0.5374)                                      \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=5,\n",
      "                      max_features=0.75, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.2998080894526426, 'minibatch_frac': 0.8, 'n_estimators': 1000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 10}}\n",
      "[iter 0] loss=1.5200 val_loss=1.2303 scale=1.0000 norm=1.0640                    \n",
      "[iter 1] loss=1.1656 val_loss=0.9969 scale=2.0000 norm=1.4589                    \n",
      "[iter 2] loss=0.9327 val_loss=0.8343 scale=2.0000 norm=1.2606                    \n",
      "[iter 3] loss=0.7263 val_loss=0.7255 scale=2.0000 norm=1.2023                    \n",
      "[iter 4] loss=0.5980 val_loss=0.6725 scale=2.0000 norm=1.1857                    \n",
      "[iter 5] loss=0.4863 val_loss=0.6244 scale=2.0000 norm=1.1821                    \n",
      "[iter 6] loss=0.3880 val_loss=0.6328 scale=2.0000 norm=1.1775                    \n",
      "[iter 7] loss=0.3770 val_loss=0.6876 scale=2.0000 norm=1.2172                    \n",
      "[iter 8] loss=0.3603 val_loss=0.7431 scale=2.0000 norm=1.2389                    \n",
      "[iter 9] loss=0.3428 val_loss=0.7547 scale=1.0000 norm=0.6286                    \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL5 (val_loss=0.6244)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features='log2', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.07006469095882442, 'minibatch_frac': 0.7, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5211 val_loss=1.3848 scale=1.0000 norm=1.0651                    \n",
      "[iter 1] loss=1.2618 val_loss=1.2873 scale=1.0000 norm=0.8355                    \n",
      "[iter 2] loss=1.1974 val_loss=1.2215 scale=2.0000 norm=1.5848                    \n",
      "[iter 3] loss=1.1221 val_loss=1.1601 scale=2.0000 norm=1.4874                    \n",
      "[iter 4] loss=1.0755 val_loss=1.1075 scale=2.0000 norm=1.4505                    \n",
      "[iter 5] loss=1.0114 val_loss=1.0553 scale=2.0000 norm=1.3867                    \n",
      "[iter 6] loss=0.9511 val_loss=1.0123 scale=2.0000 norm=1.3280                    \n",
      "[iter 7] loss=0.9088 val_loss=0.9705 scale=2.0000 norm=1.3071                    \n",
      "[iter 8] loss=0.8662 val_loss=0.9319 scale=2.0000 norm=1.2910                    \n",
      "[iter 9] loss=0.8195 val_loss=0.8960 scale=2.0000 norm=1.2616                    \n",
      "[iter 10] loss=0.7735 val_loss=0.8593 scale=2.0000 norm=1.2384                   \n",
      "[iter 11] loss=0.7385 val_loss=0.8232 scale=2.0000 norm=1.2389                   \n",
      "[iter 12] loss=0.6880 val_loss=0.7995 scale=2.0000 norm=1.1995                   \n",
      "[iter 13] loss=0.6467 val_loss=0.7733 scale=2.0000 norm=1.1813                   \n",
      "[iter 14] loss=0.6111 val_loss=0.7479 scale=2.0000 norm=1.1761                   \n",
      "[iter 15] loss=0.5738 val_loss=0.7273 scale=2.0000 norm=1.1668                   \n",
      "[iter 16] loss=0.5406 val_loss=0.7091 scale=2.0000 norm=1.1595                   \n",
      "[iter 17] loss=0.5164 val_loss=0.6930 scale=2.0000 norm=1.1515                   \n",
      "[iter 18] loss=0.4829 val_loss=0.6785 scale=2.0000 norm=1.1434                   \n",
      "[iter 19] loss=0.4500 val_loss=0.6666 scale=2.0000 norm=1.1334                   \n",
      "[iter 20] loss=0.4200 val_loss=0.6534 scale=2.0000 norm=1.1276                   \n",
      "[iter 21] loss=0.4071 val_loss=0.6472 scale=1.0000 norm=0.5684                   \n",
      "[iter 22] loss=0.3888 val_loss=0.6413 scale=1.0000 norm=0.5660                   \n",
      "[iter 23] loss=0.3621 val_loss=0.6359 scale=2.0000 norm=1.1044                   \n",
      "[iter 24] loss=0.3514 val_loss=0.6318 scale=1.0000 norm=0.5560                   \n",
      "[iter 25] loss=0.3431 val_loss=0.6414 scale=2.0000 norm=1.1177                   \n",
      "[iter 26] loss=0.3335 val_loss=0.6381 scale=1.0000 norm=0.5656                   \n",
      "[iter 27] loss=0.3136 val_loss=0.6393 scale=1.0000 norm=0.5528                   \n",
      "[iter 28] loss=0.3062 val_loss=0.6396 scale=1.0000 norm=0.5540                   \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL24 (val_loss=0.6318)                                      \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.3522472423433085, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 5}}\n",
      "[iter 0] loss=1.4840 val_loss=1.1813 scale=1.0000 norm=1.0277                    \n",
      "[iter 1] loss=1.1411 val_loss=0.9385 scale=2.0000 norm=1.4130                    \n",
      "[iter 2] loss=0.8669 val_loss=0.7727 scale=2.0000 norm=1.2118                    \n",
      "[iter 3] loss=0.6469 val_loss=0.6885 scale=2.0000 norm=1.1485                    \n",
      "[iter 4] loss=0.4840 val_loss=0.7023 scale=2.0000 norm=1.1294                    \n",
      "[iter 5] loss=0.3666 val_loss=0.7494 scale=2.0000 norm=1.1198                    \n",
      "[iter 6] loss=0.3005 val_loss=0.7633 scale=1.0000 norm=0.5710                    \n",
      "[iter 7] loss=0.2655 val_loss=0.8539 scale=2.0000 norm=1.1239                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL3 (val_loss=0.6885)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.75, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.026450537233589103, 'minibatch_frac': 0.6, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5178 val_loss=1.4697 scale=1.0000 norm=1.0617                    \n",
      "[iter 1] loss=1.3167 val_loss=1.3673 scale=2.0000 norm=1.7612                    \n",
      "[iter 2] loss=1.2346 val_loss=1.2705 scale=2.0000 norm=1.6369                    \n",
      "[iter 3] loss=1.1755 val_loss=1.2353 scale=2.0000 norm=1.5377                    \n",
      "[iter 4] loss=1.1525 val_loss=1.2018 scale=2.0000 norm=1.5271                    \n",
      "[iter 5] loss=1.1108 val_loss=1.1744 scale=2.0000 norm=1.4672                    \n",
      "[iter 6] loss=1.0814 val_loss=1.1465 scale=2.0000 norm=1.4122                    \n",
      "[iter 7] loss=1.0668 val_loss=1.1229 scale=2.0000 norm=1.4233                    \n",
      "[iter 8] loss=1.0405 val_loss=1.0988 scale=2.0000 norm=1.4006                    \n",
      "[iter 9] loss=1.0174 val_loss=1.0771 scale=2.0000 norm=1.3729                    \n",
      "[iter 10] loss=0.9913 val_loss=1.0521 scale=2.0000 norm=1.3470                   \n",
      "[iter 11] loss=0.9762 val_loss=1.0311 scale=2.0000 norm=1.3538                   \n",
      "[iter 12] loss=0.9487 val_loss=1.0121 scale=2.0000 norm=1.3064                   \n",
      "[iter 13] loss=0.9184 val_loss=0.9920 scale=2.0000 norm=1.2819                   \n",
      "[iter 14] loss=0.9084 val_loss=0.9729 scale=2.0000 norm=1.2839                   \n",
      "[iter 15] loss=0.8816 val_loss=0.9565 scale=2.0000 norm=1.2597                   \n",
      "[iter 16] loss=0.8656 val_loss=0.9389 scale=2.0000 norm=1.2572                   \n",
      "[iter 17] loss=0.8520 val_loss=0.9215 scale=2.0000 norm=1.2578                   \n",
      "[iter 18] loss=0.8260 val_loss=0.9050 scale=2.0000 norm=1.2395                   \n",
      "[iter 19] loss=0.8051 val_loss=0.8891 scale=2.0000 norm=1.2251                   \n",
      "[iter 20] loss=0.7880 val_loss=0.8743 scale=2.0000 norm=1.2114                   \n",
      "[iter 21] loss=0.7758 val_loss=0.8536 scale=2.0000 norm=1.2271                   \n",
      "[iter 22] loss=0.7501 val_loss=0.8315 scale=2.0000 norm=1.2055                   \n",
      "[iter 23] loss=0.7231 val_loss=0.8157 scale=2.0000 norm=1.1712                   \n",
      "[iter 24] loss=0.7164 val_loss=0.8024 scale=2.0000 norm=1.1834                   \n",
      "[iter 25] loss=0.7000 val_loss=0.7870 scale=2.0000 norm=1.1823                   \n",
      "[iter 26] loss=0.6934 val_loss=0.7715 scale=2.0000 norm=1.1946                   \n",
      "[iter 27] loss=0.6592 val_loss=0.7586 scale=2.0000 norm=1.1527                   \n",
      "[iter 28] loss=0.6543 val_loss=0.7469 scale=2.0000 norm=1.1672                   \n",
      "[iter 29] loss=0.6353 val_loss=0.7333 scale=2.0000 norm=1.1625                   \n",
      "[iter 30] loss=0.6218 val_loss=0.7218 scale=2.0000 norm=1.1550                   \n",
      "[iter 31] loss=0.6061 val_loss=0.7114 scale=2.0000 norm=1.1480                   \n",
      "[iter 32] loss=0.5838 val_loss=0.7030 scale=2.0000 norm=1.1418                   \n",
      "[iter 33] loss=0.5749 val_loss=0.6913 scale=2.0000 norm=1.1425                   \n",
      "[iter 34] loss=0.5594 val_loss=0.6790 scale=2.0000 norm=1.1384                   \n",
      "[iter 35] loss=0.5456 val_loss=0.6681 scale=2.0000 norm=1.1351                   \n",
      "[iter 36] loss=0.5309 val_loss=0.6584 scale=2.0000 norm=1.1246                   \n",
      "[iter 37] loss=0.5125 val_loss=0.6506 scale=2.0000 norm=1.1182                   \n",
      "[iter 38] loss=0.5012 val_loss=0.6420 scale=2.0000 norm=1.1094                   \n",
      "[iter 39] loss=0.4865 val_loss=0.6323 scale=2.0000 norm=1.1144                   \n",
      "[iter 40] loss=0.4793 val_loss=0.6223 scale=2.0000 norm=1.1274                   \n",
      "[iter 41] loss=0.4594 val_loss=0.6148 scale=2.0000 norm=1.1046                   \n",
      "[iter 42] loss=0.4384 val_loss=0.6066 scale=2.0000 norm=1.0946                   \n",
      "[iter 43] loss=0.4312 val_loss=0.5990 scale=2.0000 norm=1.0989                   \n",
      "[iter 44] loss=0.4311 val_loss=0.5912 scale=2.0000 norm=1.1080                   \n",
      "[iter 45] loss=0.4247 val_loss=0.5846 scale=2.0000 norm=1.1106                   \n",
      "[iter 46] loss=0.4042 val_loss=0.5781 scale=2.0000 norm=1.1119                   \n",
      "[iter 47] loss=0.4004 val_loss=0.5718 scale=2.0000 norm=1.0970                   \n",
      "[iter 48] loss=0.3770 val_loss=0.5644 scale=2.0000 norm=1.0858                   \n",
      "[iter 49] loss=0.3695 val_loss=0.5566 scale=2.0000 norm=1.0840                   \n",
      "[iter 50] loss=0.3565 val_loss=0.5521 scale=2.0000 norm=1.0865                   \n",
      "[iter 51] loss=0.3508 val_loss=0.5469 scale=2.0000 norm=1.0863                   \n",
      "[iter 52] loss=0.3361 val_loss=0.5447 scale=2.0000 norm=1.0665                   \n",
      "[iter 53] loss=0.3276 val_loss=0.5392 scale=2.0000 norm=1.0846                   \n",
      "[iter 54] loss=0.3144 val_loss=0.5357 scale=2.0000 norm=1.0730                   \n",
      "[iter 55] loss=0.3064 val_loss=0.5302 scale=2.0000 norm=1.0778                   \n",
      "[iter 56] loss=0.2999 val_loss=0.5281 scale=2.0000 norm=1.0795                   \n",
      "[iter 57] loss=0.2962 val_loss=0.5242 scale=2.0000 norm=1.0751                   \n",
      "[iter 58] loss=0.2890 val_loss=0.5196 scale=2.0000 norm=1.0793                   \n",
      "[iter 59] loss=0.2771 val_loss=0.5182 scale=2.0000 norm=1.0786                   \n",
      "[iter 60] loss=0.2766 val_loss=0.5152 scale=2.0000 norm=1.0733                   \n",
      "[iter 61] loss=0.2748 val_loss=0.5144 scale=1.0000 norm=0.5443                   \n",
      "[iter 62] loss=0.2729 val_loss=0.5144 scale=2.0000 norm=1.0830                   \n",
      "[iter 63] loss=0.2508 val_loss=0.5131 scale=2.0000 norm=1.0686                   \n",
      "[iter 64] loss=0.2349 val_loss=0.5123 scale=2.0000 norm=1.0597                   \n",
      "[iter 65] loss=0.2365 val_loss=0.5092 scale=2.0000 norm=1.0661                   \n",
      "[iter 66] loss=0.2296 val_loss=0.5086 scale=2.0000 norm=1.0607                   \n",
      "[iter 67] loss=0.2243 val_loss=0.5057 scale=2.0000 norm=1.0706                   \n",
      "[iter 68] loss=0.2210 val_loss=0.5066 scale=2.0000 norm=1.0676                   \n",
      "[iter 69] loss=0.2271 val_loss=0.5072 scale=1.0000 norm=0.5398                   \n",
      "[iter 70] loss=0.2136 val_loss=0.5063 scale=1.0000 norm=0.5361                   \n",
      "[iter 71] loss=0.2098 val_loss=0.5069 scale=2.0000 norm=1.0753                   \n",
      "== Early stopping achieved.                                                      \n",
      "== Best iteration / VAL67 (val_loss=0.5057)                                      \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.75, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.013528566507246558, 'minibatch_frac': 0.6, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5178 val_loss=1.4992 scale=1.0000 norm=1.0617                     \n",
      "[iter 1] loss=1.3474 val_loss=1.3800 scale=2.0000 norm=1.8099                     \n",
      "[iter 2] loss=1.2710 val_loss=1.3376 scale=2.0000 norm=1.6894                     \n",
      "[iter 3] loss=1.2314 val_loss=1.3069 scale=2.0000 norm=1.6120                     \n",
      "[iter 4] loss=1.2140 val_loss=1.2818 scale=2.0000 norm=1.6075                     \n",
      "[iter 5] loss=1.1798 val_loss=1.2614 scale=2.0000 norm=1.5533                     \n",
      "[iter 6] loss=1.1544 val_loss=1.2405 scale=2.0000 norm=1.4969                     \n",
      "[iter 7] loss=1.1527 val_loss=1.2235 scale=2.0000 norm=1.5267                     \n",
      "[iter 8] loss=1.1331 val_loss=1.2066 scale=2.0000 norm=1.5026                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 9] loss=1.1188 val_loss=1.1913 scale=2.0000 norm=1.4808                     \n",
      "[iter 10] loss=1.1008 val_loss=1.1765 scale=2.0000 norm=1.4594                    \n",
      "[iter 11] loss=1.0964 val_loss=1.1608 scale=2.0000 norm=1.4699                    \n",
      "[iter 12] loss=1.0753 val_loss=1.1479 scale=2.0000 norm=1.4252                    \n",
      "[iter 13] loss=1.0523 val_loss=1.1352 scale=2.0000 norm=1.4018                    \n",
      "[iter 14] loss=1.0532 val_loss=1.1223 scale=2.0000 norm=1.4059                    \n",
      "[iter 15] loss=1.0324 val_loss=1.1111 scale=2.0000 norm=1.3791                    \n",
      "[iter 16] loss=1.0275 val_loss=1.0993 scale=2.0000 norm=1.3833                    \n",
      "[iter 17] loss=1.0183 val_loss=1.0880 scale=2.0000 norm=1.3873                    \n",
      "[iter 18] loss=0.9997 val_loss=1.0764 scale=2.0000 norm=1.3644                    \n",
      "[iter 19] loss=0.9875 val_loss=1.0644 scale=2.0000 norm=1.3510                    \n",
      "[iter 20] loss=0.9760 val_loss=1.0541 scale=2.0000 norm=1.3275                    \n",
      "[iter 21] loss=0.9725 val_loss=1.0433 scale=2.0000 norm=1.3523                    \n",
      "[iter 22] loss=0.9548 val_loss=1.0321 scale=2.0000 norm=1.3225                    \n",
      "[iter 23] loss=0.9357 val_loss=1.0220 scale=2.0000 norm=1.2840                    \n",
      "[iter 24] loss=0.9325 val_loss=1.0118 scale=2.0000 norm=1.3012                    \n",
      "[iter 25] loss=0.9230 val_loss=1.0014 scale=2.0000 norm=1.3001                    \n",
      "[iter 26] loss=0.9222 val_loss=0.9908 scale=2.0000 norm=1.3136                    \n",
      "[iter 27] loss=0.8943 val_loss=0.9815 scale=2.0000 norm=1.2608                    \n",
      "[iter 28] loss=0.8970 val_loss=0.9706 scale=2.0000 norm=1.2832                    \n",
      "[iter 29] loss=0.8806 val_loss=0.9614 scale=2.0000 norm=1.2708                    \n",
      "[iter 30] loss=0.8740 val_loss=0.9523 scale=2.0000 norm=1.2666                    \n",
      "[iter 31] loss=0.8639 val_loss=0.9436 scale=2.0000 norm=1.2624                    \n",
      "[iter 32] loss=0.8475 val_loss=0.9350 scale=2.0000 norm=1.2499                    \n",
      "[iter 33] loss=0.8424 val_loss=0.9264 scale=2.0000 norm=1.2513                    \n",
      "[iter 34] loss=0.8341 val_loss=0.9178 scale=2.0000 norm=1.2427                    \n",
      "[iter 35] loss=0.8238 val_loss=0.9094 scale=2.0000 norm=1.2389                    \n",
      "[iter 36] loss=0.8113 val_loss=0.9015 scale=2.0000 norm=1.2257                    \n",
      "[iter 37] loss=0.7988 val_loss=0.8927 scale=2.0000 norm=1.2184                    \n",
      "[iter 38] loss=0.7902 val_loss=0.8844 scale=2.0000 norm=1.2056                    \n",
      "[iter 39] loss=0.7812 val_loss=0.8759 scale=2.0000 norm=1.2164                    \n",
      "[iter 40] loss=0.7754 val_loss=0.8677 scale=2.0000 norm=1.2238                    \n",
      "[iter 41] loss=0.7597 val_loss=0.8596 scale=2.0000 norm=1.2003                    \n",
      "[iter 42] loss=0.7439 val_loss=0.8510 scale=2.0000 norm=1.1896                    \n",
      "[iter 43] loss=0.7408 val_loss=0.8439 scale=2.0000 norm=1.1934                    \n",
      "[iter 44] loss=0.7357 val_loss=0.8355 scale=2.0000 norm=1.1946                    \n",
      "[iter 45] loss=0.7327 val_loss=0.8284 scale=2.0000 norm=1.2015                    \n",
      "[iter 46] loss=0.7205 val_loss=0.8205 scale=2.0000 norm=1.1981                    \n",
      "[iter 47] loss=0.7154 val_loss=0.8131 scale=2.0000 norm=1.1938                    \n",
      "[iter 48] loss=0.6958 val_loss=0.8061 scale=2.0000 norm=1.1707                    \n",
      "[iter 49] loss=0.6881 val_loss=0.7996 scale=2.0000 norm=1.1740                    \n",
      "[iter 50] loss=0.6796 val_loss=0.7926 scale=2.0000 norm=1.1811                    \n",
      "[iter 51] loss=0.6729 val_loss=0.7847 scale=2.0000 norm=1.1746                    \n",
      "[iter 52] loss=0.6582 val_loss=0.7783 scale=2.0000 norm=1.1506                    \n",
      "[iter 53] loss=0.6539 val_loss=0.7712 scale=2.0000 norm=1.1701                    \n",
      "[iter 54] loss=0.6392 val_loss=0.7649 scale=2.0000 norm=1.1536                    \n",
      "[iter 55] loss=0.6364 val_loss=0.7577 scale=2.0000 norm=1.1619                    \n",
      "[iter 56] loss=0.6251 val_loss=0.7503 scale=2.0000 norm=1.1577                    \n",
      "[iter 57] loss=0.6198 val_loss=0.7434 scale=2.0000 norm=1.1537                    \n",
      "[iter 58] loss=0.6127 val_loss=0.7374 scale=2.0000 norm=1.1553                    \n",
      "[iter 59] loss=0.6053 val_loss=0.7311 scale=2.0000 norm=1.1501                    \n",
      "[iter 60] loss=0.6016 val_loss=0.7238 scale=2.0000 norm=1.1484                    \n",
      "[iter 61] loss=0.5978 val_loss=0.7184 scale=2.0000 norm=1.1581                    \n",
      "[iter 62] loss=0.5868 val_loss=0.7124 scale=2.0000 norm=1.1441                    \n",
      "[iter 63] loss=0.5726 val_loss=0.7068 scale=2.0000 norm=1.1374                    \n",
      "[iter 64] loss=0.5581 val_loss=0.7026 scale=2.0000 norm=1.1232                    \n",
      "[iter 65] loss=0.5580 val_loss=0.6973 scale=2.0000 norm=1.1314                    \n",
      "[iter 66] loss=0.5493 val_loss=0.6923 scale=2.0000 norm=1.1288                    \n",
      "[iter 67] loss=0.5460 val_loss=0.6864 scale=2.0000 norm=1.1386                    \n",
      "[iter 68] loss=0.5352 val_loss=0.6802 scale=2.0000 norm=1.1212                    \n",
      "[iter 69] loss=0.5381 val_loss=0.6745 scale=2.0000 norm=1.1400                    \n",
      "[iter 70] loss=0.5241 val_loss=0.6698 scale=2.0000 norm=1.1298                    \n",
      "[iter 71] loss=0.5149 val_loss=0.6639 scale=2.0000 norm=1.1281                    \n",
      "[iter 72] loss=0.4992 val_loss=0.6577 scale=2.0000 norm=1.1082                    \n",
      "[iter 73] loss=0.5006 val_loss=0.6531 scale=2.0000 norm=1.1184                    \n",
      "[iter 74] loss=0.4936 val_loss=0.6483 scale=2.0000 norm=1.1128                    \n",
      "[iter 75] loss=0.4893 val_loss=0.6431 scale=2.0000 norm=1.1240                    \n",
      "[iter 76] loss=0.4849 val_loss=0.6382 scale=2.0000 norm=1.1287                    \n",
      "[iter 77] loss=0.4737 val_loss=0.6331 scale=2.0000 norm=1.1075                    \n",
      "[iter 78] loss=0.4778 val_loss=0.6277 scale=2.0000 norm=1.1226                    \n",
      "[iter 79] loss=0.4576 val_loss=0.6235 scale=2.0000 norm=1.1069                    \n",
      "[iter 80] loss=0.4520 val_loss=0.6201 scale=2.0000 norm=1.0974                    \n",
      "[iter 81] loss=0.4443 val_loss=0.6170 scale=2.0000 norm=1.1075                    \n",
      "[iter 82] loss=0.4386 val_loss=0.6125 scale=2.0000 norm=1.1003                    \n",
      "[iter 83] loss=0.4304 val_loss=0.6085 scale=2.0000 norm=1.0918                    \n",
      "[iter 84] loss=0.4237 val_loss=0.6050 scale=2.0000 norm=1.0985                    \n",
      "[iter 85] loss=0.4167 val_loss=0.6022 scale=2.0000 norm=1.0933                    \n",
      "[iter 86] loss=0.4238 val_loss=0.5984 scale=2.0000 norm=1.1067                    \n",
      "[iter 87] loss=0.4148 val_loss=0.5950 scale=2.0000 norm=1.1029                    \n",
      "[iter 88] loss=0.3964 val_loss=0.5909 scale=2.0000 norm=1.0902                    \n",
      "[iter 89] loss=0.4043 val_loss=0.5876 scale=2.0000 norm=1.1064                    \n",
      "[iter 90] loss=0.3920 val_loss=0.5852 scale=2.0000 norm=1.0880                    \n",
      "[iter 91] loss=0.3869 val_loss=0.5830 scale=2.0000 norm=1.0879                    \n",
      "[iter 92] loss=0.3849 val_loss=0.5809 scale=2.0000 norm=1.0844                    \n",
      "[iter 93] loss=0.3805 val_loss=0.5778 scale=2.0000 norm=1.0968                    \n",
      "[iter 94] loss=0.3695 val_loss=0.5753 scale=2.0000 norm=1.0839                    \n",
      "[iter 95] loss=0.3639 val_loss=0.5724 scale=2.0000 norm=1.0852                    \n",
      "[iter 96] loss=0.3714 val_loss=0.5701 scale=2.0000 norm=1.0929                    \n",
      "[iter 97] loss=0.3586 val_loss=0.5680 scale=2.0000 norm=1.0904                    \n",
      "[iter 98] loss=0.3413 val_loss=0.5655 scale=2.0000 norm=1.0705                    \n",
      "[iter 99] loss=0.3507 val_loss=0.5634 scale=2.0000 norm=1.0873                    \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.75, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.06765092725878741, 'minibatch_frac': 0.6, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 10}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=1.5178 val_loss=1.3114 scale=1.0000 norm=1.0617                     \n",
      "[iter 1] loss=1.2555 val_loss=1.2314 scale=2.0000 norm=1.6556                     \n",
      "[iter 2] loss=1.1663 val_loss=1.1665 scale=2.0000 norm=1.5240                     \n",
      "[iter 3] loss=1.0793 val_loss=1.1035 scale=2.0000 norm=1.4093                     \n",
      "[iter 4] loss=1.0282 val_loss=1.0541 scale=2.0000 norm=1.3702                     \n",
      "[iter 5] loss=0.9645 val_loss=1.0058 scale=2.0000 norm=1.3065                     \n",
      "[iter 6] loss=0.9058 val_loss=0.9626 scale=2.0000 norm=1.2569                     \n",
      "[iter 7] loss=0.8686 val_loss=0.9221 scale=2.0000 norm=1.2499                     \n",
      "[iter 8] loss=0.8100 val_loss=0.8863 scale=2.0000 norm=1.2329                     \n",
      "[iter 9] loss=0.7630 val_loss=0.8506 scale=2.0000 norm=1.2087                     \n",
      "[iter 10] loss=0.7128 val_loss=0.8132 scale=2.0000 norm=1.1806                    \n",
      "[iter 11] loss=0.6835 val_loss=0.7823 scale=2.0000 norm=1.1926                    \n",
      "[iter 12] loss=0.6323 val_loss=0.7510 scale=2.0000 norm=1.1563                    \n",
      "[iter 13] loss=0.5919 val_loss=0.7315 scale=2.0000 norm=1.1318                    \n",
      "[iter 14] loss=0.5508 val_loss=0.7123 scale=2.0000 norm=1.1410                    \n",
      "[iter 15] loss=0.5229 val_loss=0.6968 scale=2.0000 norm=1.1285                    \n",
      "[iter 16] loss=0.4936 val_loss=0.6782 scale=2.0000 norm=1.1240                    \n",
      "[iter 17] loss=0.4653 val_loss=0.6592 scale=2.0000 norm=1.1168                    \n",
      "[iter 18] loss=0.4377 val_loss=0.6478 scale=2.0000 norm=1.1150                    \n",
      "[iter 19] loss=0.4036 val_loss=0.6311 scale=2.0000 norm=1.1057                    \n",
      "[iter 20] loss=0.3700 val_loss=0.6239 scale=2.0000 norm=1.0906                    \n",
      "[iter 21] loss=0.3555 val_loss=0.6168 scale=2.0000 norm=1.1037                    \n",
      "[iter 22] loss=0.3195 val_loss=0.6104 scale=2.0000 norm=1.0925                    \n",
      "[iter 23] loss=0.2929 val_loss=0.6066 scale=2.0000 norm=1.0712                    \n",
      "[iter 24] loss=0.2924 val_loss=0.6032 scale=2.0000 norm=1.0863                    \n",
      "[iter 25] loss=0.2676 val_loss=0.6049 scale=2.0000 norm=1.0813                    \n",
      "[iter 26] loss=0.2691 val_loss=0.6176 scale=2.0000 norm=1.1073                    \n",
      "[iter 27] loss=0.2311 val_loss=0.6154 scale=2.0000 norm=1.0735                    \n",
      "[iter 28] loss=0.2329 val_loss=0.6180 scale=1.0000 norm=0.5452                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL24 (val_loss=0.6032)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.75, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.047706402175998724, 'minibatch_frac': 0.6, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5178 val_loss=1.4373 scale=1.0000 norm=1.0617                     \n",
      "[iter 1] loss=1.2881 val_loss=1.2780 scale=2.0000 norm=1.7117                     \n",
      "[iter 2] loss=1.1870 val_loss=1.2139 scale=2.0000 norm=1.5622                     \n",
      "[iter 3] loss=1.1126 val_loss=1.1531 scale=2.0000 norm=1.4577                     \n",
      "[iter 4] loss=1.0762 val_loss=1.1098 scale=2.0000 norm=1.4327                     \n",
      "[iter 5] loss=1.0241 val_loss=1.0738 scale=2.0000 norm=1.3712                     \n",
      "[iter 6] loss=0.9818 val_loss=1.0402 scale=2.0000 norm=1.3165                     \n",
      "[iter 7] loss=0.9532 val_loss=1.0051 scale=2.0000 norm=1.3119                     \n",
      "[iter 8] loss=0.9131 val_loss=0.9732 scale=2.0000 norm=1.2944                     \n",
      "[iter 9] loss=0.8764 val_loss=0.9436 scale=2.0000 norm=1.2639                     \n",
      "[iter 10] loss=0.8386 val_loss=0.9140 scale=2.0000 norm=1.2388                    \n",
      "[iter 11] loss=0.8109 val_loss=0.8853 scale=2.0000 norm=1.2464                    \n",
      "[iter 12] loss=0.7728 val_loss=0.8597 scale=2.0000 norm=1.2057                    \n",
      "[iter 13] loss=0.7330 val_loss=0.8344 scale=2.0000 norm=1.1831                    \n",
      "[iter 14] loss=0.7073 val_loss=0.8118 scale=2.0000 norm=1.1839                    \n",
      "[iter 15] loss=0.6753 val_loss=0.7892 scale=2.0000 norm=1.1676                    \n",
      "[iter 16] loss=0.6478 val_loss=0.7685 scale=2.0000 norm=1.1615                    \n",
      "[iter 17] loss=0.6256 val_loss=0.7482 scale=2.0000 norm=1.1557                    \n",
      "[iter 18] loss=0.5941 val_loss=0.7275 scale=2.0000 norm=1.1469                    \n",
      "[iter 19] loss=0.5631 val_loss=0.7143 scale=2.0000 norm=1.1358                    \n",
      "[iter 20] loss=0.5344 val_loss=0.6993 scale=2.0000 norm=1.1256                    \n",
      "[iter 21] loss=0.5182 val_loss=0.6842 scale=2.0000 norm=1.1381                    \n",
      "[iter 22] loss=0.4890 val_loss=0.6730 scale=2.0000 norm=1.1299                    \n",
      "[iter 23] loss=0.4543 val_loss=0.6608 scale=2.0000 norm=1.0948                    \n",
      "[iter 24] loss=0.4482 val_loss=0.6512 scale=2.0000 norm=1.1102                    \n",
      "[iter 25] loss=0.4209 val_loss=0.6405 scale=2.0000 norm=1.0989                    \n",
      "[iter 26] loss=0.4169 val_loss=0.6320 scale=2.0000 norm=1.1167                    \n",
      "[iter 27] loss=0.3772 val_loss=0.6247 scale=2.0000 norm=1.0786                    \n",
      "[iter 28] loss=0.3670 val_loss=0.6202 scale=2.0000 norm=1.0921                    \n",
      "[iter 29] loss=0.3503 val_loss=0.6155 scale=2.0000 norm=1.0912                    \n",
      "[iter 30] loss=0.3361 val_loss=0.6152 scale=2.0000 norm=1.0846                    \n",
      "[iter 31] loss=0.3223 val_loss=0.6117 scale=2.0000 norm=1.0814                    \n",
      "[iter 32] loss=0.2957 val_loss=0.6221 scale=2.0000 norm=1.0735                    \n",
      "[iter 33] loss=0.2879 val_loss=0.6236 scale=2.0000 norm=1.0738                    \n",
      "[iter 34] loss=0.2708 val_loss=0.6297 scale=2.0000 norm=1.0789                    \n",
      "[iter 35] loss=0.2623 val_loss=0.6256 scale=2.0000 norm=1.0805                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL31 (val_loss=0.6117)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=1,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.24512202748799491, 'minibatch_frac': 0.6, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 5}}\n",
      "[iter 0] loss=1.5178 val_loss=1.4964 scale=1.0000 norm=1.0617                     \n",
      "[iter 1] loss=1.4132 val_loss=1.3921 scale=1.0000 norm=0.9368                     \n",
      "[iter 2] loss=1.3035 val_loss=1.2091 scale=2.0000 norm=1.7120                     \n",
      "[iter 3] loss=1.1358 val_loss=1.1516 scale=1.0000 norm=0.7537                     \n",
      "[iter 4] loss=1.0685 val_loss=1.1099 scale=2.0000 norm=1.4581                     \n",
      "[iter 5] loss=0.9925 val_loss=1.0813 scale=2.0000 norm=1.4475                     \n",
      "[iter 6] loss=0.9514 val_loss=1.0742 scale=2.0000 norm=1.4866                     \n",
      "[iter 7] loss=0.9375 val_loss=1.0459 scale=1.0000 norm=0.7459                     \n",
      "[iter 8] loss=0.8994 val_loss=1.0337 scale=1.0000 norm=0.7615                     \n",
      "[iter 9] loss=0.8618 val_loss=1.0058 scale=2.0000 norm=1.4768                     \n",
      "[iter 10] loss=0.8485 val_loss=0.9960 scale=1.0000 norm=0.7478                    \n",
      "[iter 11] loss=0.8499 val_loss=1.0115 scale=2.0000 norm=1.5593                    \n",
      "[iter 12] loss=0.8249 val_loss=0.9635 scale=1.0000 norm=0.7483                    \n",
      "[iter 13] loss=0.7564 val_loss=0.9541 scale=1.0000 norm=0.7034                    \n",
      "[iter 14] loss=0.7734 val_loss=0.9128 scale=1.0000 norm=0.7510                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 15] loss=0.7612 val_loss=0.9046 scale=1.0000 norm=0.7269                    \n",
      "[iter 16] loss=0.7266 val_loss=0.9068 scale=2.0000 norm=1.4349                    \n",
      "[iter 17] loss=0.7558 val_loss=0.9000 scale=1.0000 norm=0.7479                    \n",
      "[iter 18] loss=0.7274 val_loss=0.9026 scale=2.0000 norm=1.4561                    \n",
      "[iter 19] loss=0.7252 val_loss=0.8945 scale=2.0000 norm=1.5148                    \n",
      "[iter 20] loss=0.7222 val_loss=0.8748 scale=1.0000 norm=0.7547                    \n",
      "[iter 21] loss=0.7157 val_loss=0.8597 scale=1.0000 norm=0.7602                    \n",
      "[iter 22] loss=0.6873 val_loss=0.8574 scale=2.0000 norm=1.4982                    \n",
      "[iter 23] loss=0.6499 val_loss=0.8474 scale=1.0000 norm=0.7077                    \n",
      "[iter 24] loss=0.6770 val_loss=0.8356 scale=1.0000 norm=0.7306                    \n",
      "[iter 25] loss=0.6774 val_loss=0.8383 scale=1.0000 norm=0.7274                    \n",
      "[iter 26] loss=0.7160 val_loss=0.8336 scale=1.0000 norm=0.7790                    \n",
      "[iter 27] loss=0.6366 val_loss=0.8375 scale=2.0000 norm=1.4135                    \n",
      "[iter 28] loss=0.6875 val_loss=0.8334 scale=1.0000 norm=0.7554                    \n",
      "[iter 29] loss=0.6656 val_loss=0.8358 scale=1.0000 norm=0.7465                    \n",
      "[iter 30] loss=0.6849 val_loss=0.8309 scale=0.5000 norm=0.3790                    \n",
      "[iter 31] loss=0.6616 val_loss=0.8369 scale=1.0000 norm=0.7379                    \n",
      "[iter 32] loss=0.6700 val_loss=0.8398 scale=1.0000 norm=0.7570                    \n",
      "[iter 33] loss=0.6768 val_loss=0.8249 scale=0.5000 norm=0.3794                    \n",
      "[iter 34] loss=0.6436 val_loss=0.8157 scale=0.5000 norm=0.3695                    \n",
      "[iter 35] loss=0.6775 val_loss=0.7889 scale=2.0000 norm=1.5156                    \n",
      "[iter 36] loss=0.6618 val_loss=0.8010 scale=2.0000 norm=1.4562                    \n",
      "[iter 37] loss=0.6511 val_loss=0.7964 scale=0.2500 norm=0.1850                    \n",
      "[iter 38] loss=0.6277 val_loss=0.8010 scale=2.0000 norm=1.4538                    \n",
      "[iter 39] loss=0.6167 val_loss=0.7958 scale=1.0000 norm=0.7171                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL35 (val_loss=0.7889)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=5,\n",
      "                      max_features='sqrt', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.449181149343382, 'minibatch_frac': 0.6, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5178 val_loss=1.2394 scale=1.0000 norm=1.0617                     \n",
      "[iter 1] loss=1.2161 val_loss=1.0169 scale=2.0000 norm=1.4993                     \n",
      "[iter 2] loss=1.0399 val_loss=0.9212 scale=1.0000 norm=0.7409                     \n",
      "[iter 3] loss=0.8895 val_loss=0.8540 scale=2.0000 norm=1.3848                     \n",
      "[iter 4] loss=0.7754 val_loss=0.8193 scale=1.0000 norm=0.6911                     \n",
      "[iter 5] loss=0.7137 val_loss=0.8323 scale=2.0000 norm=1.3234                     \n",
      "[iter 6] loss=0.5983 val_loss=0.8327 scale=1.0000 norm=0.6698                     \n",
      "[iter 7] loss=0.6448 val_loss=0.8268 scale=1.0000 norm=0.6864                     \n",
      "[iter 8] loss=0.6250 val_loss=0.8131 scale=1.0000 norm=0.6951                     \n",
      "[iter 9] loss=0.5867 val_loss=0.8158 scale=1.0000 norm=0.6704                     \n",
      "[iter 10] loss=0.5762 val_loss=0.8309 scale=1.0000 norm=0.6584                    \n",
      "[iter 11] loss=0.5380 val_loss=0.8312 scale=1.0000 norm=0.6549                    \n",
      "[iter 12] loss=0.5110 val_loss=0.8318 scale=1.0000 norm=0.6473                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL8 (val_loss=0.8131)                                        \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.75, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.10644880801082242, 'minibatch_frac': 0.6, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5178 val_loss=1.3794 scale=1.0000 norm=1.0617                     \n",
      "[iter 1] loss=1.2449 val_loss=1.1909 scale=2.0000 norm=1.6251                     \n",
      "[iter 2] loss=1.1088 val_loss=1.0887 scale=2.0000 norm=1.4353                     \n",
      "[iter 3] loss=0.9939 val_loss=1.0093 scale=2.0000 norm=1.3198                     \n",
      "[iter 4] loss=0.9216 val_loss=0.9471 scale=2.0000 norm=1.2835                     \n",
      "[iter 5] loss=0.8398 val_loss=0.8838 scale=2.0000 norm=1.2270                     \n",
      "[iter 6] loss=0.7611 val_loss=0.8276 scale=2.0000 norm=1.1833                     \n",
      "[iter 7] loss=0.7067 val_loss=0.7807 scale=2.0000 norm=1.1759                     \n",
      "[iter 8] loss=0.6315 val_loss=0.7516 scale=2.0000 norm=1.1615                     \n",
      "[iter 9] loss=0.5728 val_loss=0.7259 scale=2.0000 norm=1.1446                     \n",
      "[iter 10] loss=0.5149 val_loss=0.6865 scale=2.0000 norm=1.1170                    \n",
      "[iter 11] loss=0.4758 val_loss=0.6571 scale=2.0000 norm=1.1285                    \n",
      "[iter 12] loss=0.4205 val_loss=0.6467 scale=2.0000 norm=1.1055                    \n",
      "[iter 13] loss=0.3720 val_loss=0.6427 scale=2.0000 norm=1.0835                    \n",
      "[iter 14] loss=0.3401 val_loss=0.6485 scale=2.0000 norm=1.0933                    \n",
      "[iter 15] loss=0.3256 val_loss=0.6325 scale=2.0000 norm=1.1050                    \n",
      "[iter 16] loss=0.2919 val_loss=0.6350 scale=2.0000 norm=1.0955                    \n",
      "[iter 17] loss=0.2782 val_loss=0.6430 scale=2.0000 norm=1.0869                    \n",
      "[iter 18] loss=0.2530 val_loss=0.6600 scale=2.0000 norm=1.0835                    \n",
      "[iter 19] loss=0.2321 val_loss=0.6682 scale=2.0000 norm=1.0980                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL15 (val_loss=0.6325)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=5,\n",
      "                      max_features='log2', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.022327275348495323, 'minibatch_frac': 0.7, 'n_estimators': 1000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 10}}\n",
      "[iter 0] loss=1.5211 val_loss=1.5047 scale=1.0000 norm=1.0651                     \n",
      "[iter 1] loss=1.3293 val_loss=1.4855 scale=1.0000 norm=0.8908                     \n",
      "[iter 2] loss=1.2967 val_loss=1.4381 scale=1.0000 norm=0.8656                     \n",
      "[iter 3] loss=1.2741 val_loss=1.3961 scale=1.0000 norm=0.8406                     \n",
      "[iter 4] loss=1.2572 val_loss=1.3716 scale=1.0000 norm=0.8376                     \n",
      "[iter 5] loss=1.2181 val_loss=1.3253 scale=1.0000 norm=0.8075                     \n",
      "[iter 6] loss=1.2021 val_loss=1.3048 scale=1.0000 norm=0.7904                     \n",
      "[iter 7] loss=1.1980 val_loss=1.2701 scale=2.0000 norm=1.5934                     \n",
      "[iter 8] loss=1.1740 val_loss=1.2543 scale=1.0000 norm=0.7825                     \n",
      "[iter 9] loss=1.1582 val_loss=1.2283 scale=2.0000 norm=1.5322                     \n",
      "[iter 10] loss=1.1287 val_loss=1.2060 scale=2.0000 norm=1.4979                    \n",
      "[iter 11] loss=1.1178 val_loss=1.1908 scale=2.0000 norm=1.4988                    \n",
      "[iter 12] loss=1.0969 val_loss=1.1802 scale=1.0000 norm=0.7323                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 13] loss=1.0790 val_loss=1.1605 scale=2.0000 norm=1.4504                    \n",
      "[iter 14] loss=1.0771 val_loss=1.1411 scale=2.0000 norm=1.4603                    \n",
      "[iter 15] loss=1.0497 val_loss=1.1230 scale=2.0000 norm=1.4283                    \n",
      "[iter 16] loss=1.0367 val_loss=1.1077 scale=2.0000 norm=1.4228                    \n",
      "[iter 17] loss=1.0208 val_loss=1.0926 scale=2.0000 norm=1.4158                    \n",
      "[iter 18] loss=1.0012 val_loss=1.0776 scale=2.0000 norm=1.3952                    \n",
      "[iter 19] loss=0.9852 val_loss=1.0626 scale=2.0000 norm=1.3773                    \n",
      "[iter 20] loss=0.9730 val_loss=1.0472 scale=2.0000 norm=1.3667                    \n",
      "[iter 21] loss=0.9627 val_loss=1.0320 scale=2.0000 norm=1.3746                    \n",
      "[iter 22] loss=0.9442 val_loss=1.0177 scale=2.0000 norm=1.3534                    \n",
      "[iter 23] loss=0.9217 val_loss=1.0041 scale=2.0000 norm=1.3210                    \n",
      "[iter 24] loss=0.9093 val_loss=0.9920 scale=2.0000 norm=1.3224                    \n",
      "[iter 25] loss=0.8962 val_loss=0.9791 scale=2.0000 norm=1.3223                    \n",
      "[iter 26] loss=0.8943 val_loss=0.9652 scale=2.0000 norm=1.3358                    \n",
      "[iter 27] loss=0.8662 val_loss=0.9534 scale=2.0000 norm=1.2951                    \n",
      "[iter 28] loss=0.8612 val_loss=0.9421 scale=2.0000 norm=1.3075                    \n",
      "[iter 29] loss=0.8443 val_loss=0.9302 scale=2.0000 norm=1.2964                    \n",
      "[iter 30] loss=0.8330 val_loss=0.9185 scale=2.0000 norm=1.2956                    \n",
      "[iter 31] loss=0.8145 val_loss=0.9080 scale=2.0000 norm=1.2782                    \n",
      "[iter 32] loss=0.8070 val_loss=0.8955 scale=2.0000 norm=1.2871                    \n",
      "[iter 33] loss=0.7940 val_loss=0.8856 scale=2.0000 norm=1.2772                    \n",
      "[iter 34] loss=0.7814 val_loss=0.8758 scale=2.0000 norm=1.2655                    \n",
      "[iter 35] loss=0.7675 val_loss=0.8652 scale=2.0000 norm=1.2596                    \n",
      "[iter 36] loss=0.7538 val_loss=0.8541 scale=2.0000 norm=1.2561                    \n",
      "[iter 37] loss=0.7387 val_loss=0.8453 scale=2.0000 norm=1.2481                    \n",
      "[iter 38] loss=0.7176 val_loss=0.8363 scale=2.0000 norm=1.2210                    \n",
      "[iter 39] loss=0.7168 val_loss=0.8275 scale=2.0000 norm=1.2432                    \n",
      "[iter 40] loss=0.7086 val_loss=0.8187 scale=2.0000 norm=1.2468                    \n",
      "[iter 41] loss=0.6890 val_loss=0.8107 scale=2.0000 norm=1.2232                    \n",
      "[iter 42] loss=0.6748 val_loss=0.8017 scale=2.0000 norm=1.2205                    \n",
      "[iter 43] loss=0.6633 val_loss=0.7929 scale=2.0000 norm=1.2146                    \n",
      "[iter 44] loss=0.6571 val_loss=0.7840 scale=2.0000 norm=1.2201                    \n",
      "[iter 45] loss=0.6542 val_loss=0.7767 scale=2.0000 norm=1.2289                    \n",
      "[iter 46] loss=0.6374 val_loss=0.7708 scale=2.0000 norm=1.2221                    \n",
      "[iter 47] loss=0.6282 val_loss=0.7632 scale=2.0000 norm=1.2168                    \n",
      "[iter 48] loss=0.6127 val_loss=0.7557 scale=2.0000 norm=1.1999                    \n",
      "[iter 49] loss=0.6046 val_loss=0.7473 scale=2.0000 norm=1.2092                    \n",
      "[iter 50] loss=0.5986 val_loss=0.7409 scale=2.0000 norm=1.2144                    \n",
      "[iter 51] loss=0.5857 val_loss=0.7328 scale=2.0000 norm=1.2035                    \n",
      "[iter 52] loss=0.5698 val_loss=0.7281 scale=2.0000 norm=1.1829                    \n",
      "[iter 53] loss=0.5631 val_loss=0.7221 scale=2.0000 norm=1.1928                    \n",
      "[iter 54] loss=0.5474 val_loss=0.7191 scale=1.0000 norm=0.5900                    \n",
      "[iter 55] loss=0.5518 val_loss=0.7134 scale=2.0000 norm=1.1962                    \n",
      "[iter 56] loss=0.5437 val_loss=0.7070 scale=2.0000 norm=1.1983                    \n",
      "[iter 57] loss=0.5299 val_loss=0.7034 scale=1.0000 norm=0.5906                    \n",
      "[iter 58] loss=0.5205 val_loss=0.6964 scale=2.0000 norm=1.1788                    \n",
      "[iter 59] loss=0.5232 val_loss=0.6913 scale=2.0000 norm=1.1904                    \n",
      "[iter 60] loss=0.5126 val_loss=0.6840 scale=2.0000 norm=1.1848                    \n",
      "[iter 61] loss=0.5097 val_loss=0.6814 scale=1.0000 norm=0.5958                    \n",
      "[iter 62] loss=0.4971 val_loss=0.6786 scale=1.0000 norm=0.5860                    \n",
      "[iter 63] loss=0.4945 val_loss=0.6764 scale=1.0000 norm=0.5890                    \n",
      "[iter 64] loss=0.4815 val_loss=0.6714 scale=2.0000 norm=1.1633                    \n",
      "[iter 65] loss=0.4828 val_loss=0.6677 scale=1.0000 norm=0.5898                    \n",
      "[iter 66] loss=0.4693 val_loss=0.6635 scale=2.0000 norm=1.1651                    \n",
      "[iter 67] loss=0.4737 val_loss=0.6610 scale=1.0000 norm=0.5905                    \n",
      "[iter 68] loss=0.4639 val_loss=0.6562 scale=2.0000 norm=1.1667                    \n",
      "[iter 69] loss=0.4687 val_loss=0.6540 scale=1.0000 norm=0.5924                    \n",
      "[iter 70] loss=0.4584 val_loss=0.6515 scale=1.0000 norm=0.5859                    \n",
      "[iter 71] loss=0.4522 val_loss=0.6471 scale=2.0000 norm=1.1802                    \n",
      "[iter 72] loss=0.4395 val_loss=0.6404 scale=2.0000 norm=1.1638                    \n",
      "[iter 73] loss=0.4347 val_loss=0.6387 scale=1.0000 norm=0.5833                    \n",
      "[iter 74] loss=0.4272 val_loss=0.6359 scale=2.0000 norm=1.1574                    \n",
      "[iter 75] loss=0.4268 val_loss=0.6321 scale=2.0000 norm=1.1722                    \n",
      "[iter 76] loss=0.4266 val_loss=0.6290 scale=2.0000 norm=1.1827                    \n",
      "[iter 77] loss=0.4200 val_loss=0.6279 scale=1.0000 norm=0.5872                    \n",
      "[iter 78] loss=0.4186 val_loss=0.6256 scale=1.0000 norm=0.5873                    \n",
      "[iter 79] loss=0.4052 val_loss=0.6220 scale=1.0000 norm=0.5833                    \n",
      "[iter 80] loss=0.3976 val_loss=0.6208 scale=1.0000 norm=0.5766                    \n",
      "[iter 81] loss=0.3996 val_loss=0.6179 scale=1.0000 norm=0.5851                    \n",
      "[iter 82] loss=0.3985 val_loss=0.6136 scale=2.0000 norm=1.1719                    \n",
      "[iter 83] loss=0.3859 val_loss=0.6100 scale=2.0000 norm=1.1504                    \n",
      "[iter 84] loss=0.3745 val_loss=0.6061 scale=2.0000 norm=1.1488                    \n",
      "[iter 85] loss=0.3677 val_loss=0.6044 scale=1.0000 norm=0.5696                    \n",
      "[iter 86] loss=0.3838 val_loss=0.6040 scale=1.0000 norm=0.5862                    \n",
      "[iter 87] loss=0.3728 val_loss=0.6030 scale=2.0000 norm=1.1673                    \n",
      "[iter 88] loss=0.3579 val_loss=0.5994 scale=1.0000 norm=0.5776                    \n",
      "[iter 89] loss=0.3659 val_loss=0.5985 scale=1.0000 norm=0.5820                    \n",
      "[iter 90] loss=0.3624 val_loss=0.5975 scale=1.0000 norm=0.5800                    \n",
      "[iter 91] loss=0.3536 val_loss=0.5967 scale=2.0000 norm=1.1469                    \n",
      "[iter 92] loss=0.3621 val_loss=0.5961 scale=1.0000 norm=0.5821                    \n",
      "[iter 93] loss=0.3592 val_loss=0.5940 scale=2.0000 norm=1.1750                    \n",
      "[iter 94] loss=0.3505 val_loss=0.5921 scale=1.0000 norm=0.5837                    \n",
      "[iter 95] loss=0.3486 val_loss=0.5913 scale=2.0000 norm=1.1696                    \n",
      "[iter 96] loss=0.3584 val_loss=0.5905 scale=1.0000 norm=0.5906                    \n",
      "[iter 97] loss=0.3413 val_loss=0.5927 scale=2.0000 norm=1.1643                    \n",
      "[iter 98] loss=0.3217 val_loss=0.5919 scale=1.0000 norm=0.5728                    \n",
      "[iter 99] loss=0.3376 val_loss=0.5910 scale=1.0000 norm=0.5848                    \n",
      "[iter 100] loss=0.3275 val_loss=0.5901 scale=2.0000 norm=1.1552                   \n",
      "[iter 101] loss=0.3250 val_loss=0.5896 scale=1.0000 norm=0.5787                   \n",
      "[iter 102] loss=0.3373 val_loss=0.5886 scale=1.0000 norm=0.5875                   \n",
      "[iter 103] loss=0.3253 val_loss=0.5885 scale=1.0000 norm=0.5801                   \n",
      "[iter 104] loss=0.3191 val_loss=0.5886 scale=1.0000 norm=0.5815                   \n",
      "[iter 105] loss=0.3095 val_loss=0.5910 scale=2.0000 norm=1.1429                   \n",
      "[iter 106] loss=0.3029 val_loss=0.5903 scale=1.0000 norm=0.5741                   \n",
      "[iter 107] loss=0.3168 val_loss=0.5904 scale=1.0000 norm=0.5891                   \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL103 (val_loss=0.5885)                                      \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=1,\n",
      "                      max_features='log2', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      random_state=69, splitter='best'), 'learning_rate': 0.005931167425682865, 'minibatch_frac': 0.8, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5200 val_loss=1.5810 scale=1.0000 norm=1.0640                     \n",
      "[iter 1] loss=1.4973 val_loss=1.5766 scale=1.0000 norm=1.0410                     \n",
      "[iter 2] loss=1.4904 val_loss=1.5725 scale=1.0000 norm=1.0344                     \n",
      "[iter 3] loss=1.4146 val_loss=1.5696 scale=1.0000 norm=0.9587                     \n",
      "[iter 4] loss=1.4712 val_loss=1.5666 scale=1.0000 norm=1.0160                     \n",
      "[iter 5] loss=1.4963 val_loss=1.5625 scale=1.0000 norm=1.0411                     \n",
      "[iter 6] loss=1.4337 val_loss=1.5589 scale=1.0000 norm=0.9787                     \n",
      "[iter 7] loss=1.4845 val_loss=1.5554 scale=1.0000 norm=1.0296                     \n",
      "[iter 8] loss=1.4743 val_loss=1.5525 scale=1.0000 norm=1.0200                     \n",
      "[iter 9] loss=1.4360 val_loss=1.5508 scale=1.0000 norm=0.9820                     \n",
      "[iter 10] loss=1.4512 val_loss=1.5482 scale=1.0000 norm=0.9967                    \n",
      "[iter 11] loss=1.4667 val_loss=1.5454 scale=1.0000 norm=1.0127                    \n",
      "[iter 12] loss=1.4085 val_loss=1.5439 scale=1.0000 norm=0.9548                    \n",
      "[iter 13] loss=1.4508 val_loss=1.5410 scale=1.0000 norm=0.9974                    \n",
      "[iter 14] loss=1.4046 val_loss=1.5387 scale=1.0000 norm=0.9519                    \n",
      "[iter 15] loss=1.4479 val_loss=1.5360 scale=1.0000 norm=0.9954                    \n",
      "[iter 16] loss=1.4076 val_loss=1.5335 scale=1.0000 norm=0.9559                    \n",
      "[iter 17] loss=1.4811 val_loss=1.5309 scale=1.0000 norm=1.0297                    \n",
      "[iter 18] loss=1.4740 val_loss=1.5286 scale=1.0000 norm=1.0228                    \n",
      "[iter 19] loss=1.4567 val_loss=1.5264 scale=1.0000 norm=1.0053                    \n",
      "[iter 20] loss=1.4283 val_loss=1.5246 scale=1.0000 norm=0.9774                    \n",
      "[iter 21] loss=1.4523 val_loss=1.5226 scale=1.0000 norm=1.0016                    \n",
      "[iter 22] loss=1.4231 val_loss=1.5204 scale=1.0000 norm=0.9726                    \n",
      "[iter 23] loss=1.3879 val_loss=1.5185 scale=1.0000 norm=0.9375                    \n",
      "[iter 24] loss=1.4557 val_loss=1.5164 scale=1.0000 norm=1.0063                    \n",
      "[iter 25] loss=1.4513 val_loss=1.5143 scale=1.0000 norm=1.0019                    \n",
      "[iter 26] loss=1.4516 val_loss=1.5125 scale=1.0000 norm=1.0025                    \n",
      "[iter 27] loss=1.4203 val_loss=1.5112 scale=1.0000 norm=0.9716                    \n",
      "[iter 28] loss=1.4345 val_loss=1.5096 scale=1.0000 norm=0.9863                    \n",
      "[iter 29] loss=1.4315 val_loss=1.5082 scale=1.0000 norm=0.9832                    \n",
      "[iter 30] loss=1.4395 val_loss=1.5067 scale=1.0000 norm=0.9913                    \n",
      "[iter 31] loss=1.4156 val_loss=1.5052 scale=1.0000 norm=0.9680                    \n",
      "[iter 32] loss=1.4493 val_loss=1.5037 scale=1.0000 norm=1.0019                    \n",
      "[iter 33] loss=1.4381 val_loss=1.5023 scale=1.0000 norm=0.9923                    \n",
      "[iter 34] loss=1.3856 val_loss=1.5010 scale=1.0000 norm=0.9395                    \n",
      "[iter 35] loss=1.4351 val_loss=1.4997 scale=1.0000 norm=0.9887                    \n",
      "[iter 36] loss=1.4470 val_loss=1.4985 scale=1.0000 norm=1.0008                    \n",
      "[iter 37] loss=1.4365 val_loss=1.4971 scale=1.0000 norm=0.9908                    \n",
      "[iter 38] loss=1.3773 val_loss=1.4958 scale=1.0000 norm=0.9312                    \n",
      "[iter 39] loss=1.4320 val_loss=1.4945 scale=1.0000 norm=0.9876                    \n",
      "[iter 40] loss=1.4341 val_loss=1.4936 scale=1.0000 norm=0.9894                    \n",
      "[iter 41] loss=1.3982 val_loss=1.4927 scale=1.0000 norm=0.9532                    \n",
      "[iter 42] loss=1.3761 val_loss=1.4915 scale=1.0000 norm=0.9324                    \n",
      "[iter 43] loss=1.4146 val_loss=1.4906 scale=1.0000 norm=0.9710                    \n",
      "[iter 44] loss=1.4197 val_loss=1.4895 scale=1.0000 norm=0.9754                    \n",
      "[iter 45] loss=1.4439 val_loss=1.4884 scale=1.0000 norm=1.0013                    \n",
      "[iter 46] loss=1.4240 val_loss=1.4872 scale=1.0000 norm=0.9810                    \n",
      "[iter 47] loss=1.4335 val_loss=1.4861 scale=1.0000 norm=0.9916                    \n",
      "[iter 48] loss=1.3533 val_loss=1.4849 scale=2.0000 norm=1.8221                    \n",
      "[iter 49] loss=1.4201 val_loss=1.4839 scale=1.0000 norm=0.9792                    \n",
      "[iter 50] loss=1.4273 val_loss=1.4830 scale=1.0000 norm=0.9862                    \n",
      "[iter 51] loss=1.4297 val_loss=1.4820 scale=1.0000 norm=0.9889                    \n",
      "[iter 52] loss=1.4015 val_loss=1.4811 scale=2.0000 norm=1.9203                    \n",
      "[iter 53] loss=1.4243 val_loss=1.4802 scale=1.0000 norm=0.9843                    \n",
      "[iter 54] loss=1.3999 val_loss=1.4792 scale=2.0000 norm=1.9176                    \n",
      "[iter 55] loss=1.4355 val_loss=1.4784 scale=1.0000 norm=0.9962                    \n",
      "[iter 56] loss=1.4391 val_loss=1.4775 scale=1.0000 norm=1.0004                    \n",
      "[iter 57] loss=1.4323 val_loss=1.4768 scale=1.0000 norm=0.9938                    \n",
      "[iter 58] loss=1.3805 val_loss=1.4759 scale=1.0000 norm=0.9422                    \n",
      "[iter 59] loss=1.4241 val_loss=1.4753 scale=1.0000 norm=0.9850                    \n",
      "[iter 60] loss=1.4069 val_loss=1.4746 scale=1.0000 norm=0.9699                    \n",
      "[iter 61] loss=1.4137 val_loss=1.4739 scale=1.0000 norm=0.9762                    \n",
      "[iter 62] loss=1.3952 val_loss=1.4733 scale=1.0000 norm=0.9577                    \n",
      "[iter 63] loss=1.4035 val_loss=1.4727 scale=1.0000 norm=0.9658                    \n",
      "[iter 64] loss=1.4086 val_loss=1.4725 scale=2.0000 norm=1.9412                    \n",
      "[iter 65] loss=1.4136 val_loss=1.4718 scale=1.0000 norm=0.9782                    \n",
      "[iter 66] loss=1.3856 val_loss=1.4711 scale=1.0000 norm=0.9501                    \n",
      "[iter 67] loss=1.4007 val_loss=1.4703 scale=1.0000 norm=0.9660                    \n",
      "[iter 68] loss=1.3487 val_loss=1.4689 scale=2.0000 norm=1.8279                    \n",
      "[iter 69] loss=1.3657 val_loss=1.4683 scale=1.0000 norm=0.9318                    \n",
      "[iter 70] loss=1.3997 val_loss=1.4682 scale=2.0000 norm=1.9290                    \n",
      "[iter 71] loss=1.4186 val_loss=1.4676 scale=1.0000 norm=0.9854                    \n",
      "[iter 72] loss=1.4044 val_loss=1.4672 scale=1.0000 norm=0.9711                    \n",
      "[iter 73] loss=1.4136 val_loss=1.4667 scale=1.0000 norm=0.9804                    \n",
      "[iter 74] loss=1.4231 val_loss=1.4660 scale=1.0000 norm=0.9918                    \n",
      "[iter 75] loss=1.4066 val_loss=1.4655 scale=1.0000 norm=0.9737                    \n",
      "[iter 76] loss=1.4311 val_loss=1.4648 scale=1.0000 norm=1.0000                    \n",
      "[iter 77] loss=1.3696 val_loss=1.4638 scale=2.0000 norm=1.8767                    \n",
      "[iter 78] loss=1.4077 val_loss=1.4633 scale=1.0000 norm=0.9777                    \n",
      "[iter 79] loss=1.4291 val_loss=1.4628 scale=1.0000 norm=0.9993                    \n",
      "[iter 80] loss=1.4033 val_loss=1.4623 scale=1.0000 norm=0.9724                    \n",
      "[iter 81] loss=1.3944 val_loss=1.4618 scale=1.0000 norm=0.9648                    \n",
      "[iter 82] loss=1.3542 val_loss=1.4611 scale=2.0000 norm=1.8475                    \n",
      "[iter 83] loss=1.3518 val_loss=1.4606 scale=2.0000 norm=1.8449                    \n",
      "[iter 84] loss=1.3978 val_loss=1.4601 scale=1.0000 norm=0.9697                    \n",
      "[iter 85] loss=1.3549 val_loss=1.4599 scale=2.0000 norm=1.8524                    \n",
      "[iter 86] loss=1.3728 val_loss=1.4591 scale=2.0000 norm=1.8930                    \n",
      "[iter 87] loss=1.3867 val_loss=1.4587 scale=1.0000 norm=0.9606                    \n",
      "[iter 88] loss=1.3870 val_loss=1.4582 scale=1.0000 norm=0.9607                    \n",
      "[iter 89] loss=1.4032 val_loss=1.4582 scale=2.0000 norm=1.9511                    \n",
      "[iter 90] loss=1.3969 val_loss=1.4578 scale=1.0000 norm=0.9698                    \n",
      "[iter 91] loss=1.4036 val_loss=1.4577 scale=2.0000 norm=1.9554                    \n",
      "[iter 92] loss=1.4065 val_loss=1.4574 scale=1.0000 norm=0.9824                    \n",
      "[iter 93] loss=1.4211 val_loss=1.4573 scale=2.0000 norm=1.9907                    \n",
      "[iter 94] loss=1.4014 val_loss=1.4569 scale=1.0000 norm=0.9761                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 95] loss=1.4078 val_loss=1.4569 scale=2.0000 norm=1.9662                    \n",
      "[iter 96] loss=1.4079 val_loss=1.4569 scale=2.0000 norm=1.9675                    \n",
      "[iter 97] loss=1.3534 val_loss=1.4562 scale=2.0000 norm=1.8596                    \n",
      "[iter 98] loss=1.3492 val_loss=1.4552 scale=2.0000 norm=1.8530                    \n",
      "[iter 99] loss=1.3299 val_loss=1.4552 scale=2.0000 norm=1.8141                    \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=5,\n",
      "                      max_features=0.75, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.03426746434823279, 'minibatch_frac': 0.6, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5178 val_loss=1.4656 scale=1.0000 norm=1.0617                     \n",
      "[iter 1] loss=1.3045 val_loss=1.3265 scale=2.0000 norm=1.7384                     \n",
      "[iter 2] loss=1.2190 val_loss=1.2620 scale=2.0000 norm=1.6122                     \n",
      "[iter 3] loss=1.1572 val_loss=1.2154 scale=2.0000 norm=1.5152                     \n",
      "[iter 4] loss=1.1271 val_loss=1.1796 scale=2.0000 norm=1.4961                     \n",
      "[iter 5] loss=1.0832 val_loss=1.1439 scale=2.0000 norm=1.4365                     \n",
      "[iter 6] loss=1.0482 val_loss=1.1145 scale=2.0000 norm=1.3808                     \n",
      "[iter 7] loss=1.0331 val_loss=1.0859 scale=2.0000 norm=1.3899                     \n",
      "[iter 8] loss=1.0029 val_loss=1.0608 scale=2.0000 norm=1.3702                     \n",
      "[iter 9] loss=0.9748 val_loss=1.0355 scale=2.0000 norm=1.3395                     \n",
      "[iter 10] loss=0.9447 val_loss=1.0074 scale=2.0000 norm=1.3180                    \n",
      "[iter 11] loss=0.9254 val_loss=0.9812 scale=2.0000 norm=1.3247                    \n",
      "[iter 12] loss=0.8934 val_loss=0.9569 scale=2.0000 norm=1.2778                    \n",
      "[iter 13] loss=0.8592 val_loss=0.9356 scale=2.0000 norm=1.2549                    \n",
      "[iter 14] loss=0.8443 val_loss=0.9145 scale=2.0000 norm=1.2565                    \n",
      "[iter 15] loss=0.8172 val_loss=0.8913 scale=2.0000 norm=1.2375                    \n",
      "[iter 16] loss=0.7962 val_loss=0.8719 scale=2.0000 norm=1.2364                    \n",
      "[iter 17] loss=0.7815 val_loss=0.8511 scale=2.0000 norm=1.2384                    \n",
      "[iter 18] loss=0.7529 val_loss=0.8326 scale=2.0000 norm=1.2226                    \n",
      "[iter 19] loss=0.7291 val_loss=0.8151 scale=2.0000 norm=1.2115                    \n",
      "[iter 20] loss=0.7058 val_loss=0.7991 scale=2.0000 norm=1.1957                    \n",
      "[iter 21] loss=0.6954 val_loss=0.7803 scale=2.0000 norm=1.2146                    \n",
      "[iter 22] loss=0.6691 val_loss=0.7638 scale=2.0000 norm=1.2010                    \n",
      "[iter 23] loss=0.6363 val_loss=0.7480 scale=2.0000 norm=1.1615                    \n",
      "[iter 24] loss=0.6348 val_loss=0.7348 scale=2.0000 norm=1.1818                    \n",
      "[iter 25] loss=0.6129 val_loss=0.7209 scale=2.0000 norm=1.1776                    \n",
      "[iter 26] loss=0.6088 val_loss=0.7086 scale=2.0000 norm=1.1943                    \n",
      "[iter 27] loss=0.5704 val_loss=0.6963 scale=2.0000 norm=1.1487                    \n",
      "[iter 28] loss=0.5661 val_loss=0.6842 scale=2.0000 norm=1.1681                    \n",
      "[iter 29] loss=0.5470 val_loss=0.6728 scale=2.0000 norm=1.1655                    \n",
      "[iter 30] loss=0.5348 val_loss=0.6630 scale=2.0000 norm=1.1606                    \n",
      "[iter 31] loss=0.5211 val_loss=0.6534 scale=2.0000 norm=1.1594                    \n",
      "[iter 32] loss=0.4964 val_loss=0.6440 scale=2.0000 norm=1.1480                    \n",
      "[iter 33] loss=0.4907 val_loss=0.6308 scale=2.0000 norm=1.1554                    \n",
      "[iter 34] loss=0.4752 val_loss=0.6225 scale=2.0000 norm=1.1553                    \n",
      "[iter 35] loss=0.4637 val_loss=0.6133 scale=2.0000 norm=1.1549                    \n",
      "[iter 36] loss=0.4481 val_loss=0.6069 scale=2.0000 norm=1.1435                    \n",
      "[iter 37] loss=0.4325 val_loss=0.6022 scale=2.0000 norm=1.1389                    \n",
      "[iter 38] loss=0.4215 val_loss=0.5981 scale=2.0000 norm=1.1296                    \n",
      "[iter 39] loss=0.4093 val_loss=0.5915 scale=2.0000 norm=1.1343                    \n",
      "[iter 40] loss=0.4030 val_loss=0.5875 scale=2.0000 norm=1.1577                    \n",
      "[iter 41] loss=0.3795 val_loss=0.5828 scale=2.0000 norm=1.1249                    \n",
      "[iter 42] loss=0.3607 val_loss=0.5775 scale=2.0000 norm=1.1163                    \n",
      "[iter 43] loss=0.3539 val_loss=0.5755 scale=2.0000 norm=1.1168                    \n",
      "[iter 44] loss=0.3627 val_loss=0.5703 scale=2.0000 norm=1.1429                    \n",
      "[iter 45] loss=0.3569 val_loss=0.5684 scale=2.0000 norm=1.1435                    \n",
      "[iter 46] loss=0.3409 val_loss=0.5660 scale=1.0000 norm=0.5752                    \n",
      "[iter 47] loss=0.3468 val_loss=0.5619 scale=2.0000 norm=1.1402                    \n",
      "[iter 48] loss=0.3215 val_loss=0.5612 scale=1.0000 norm=0.5643                    \n",
      "[iter 49] loss=0.3225 val_loss=0.5599 scale=2.0000 norm=1.1272                    \n",
      "[iter 50] loss=0.3079 val_loss=0.5587 scale=1.0000 norm=0.5625                    \n",
      "[iter 51] loss=0.3109 val_loss=0.5581 scale=2.0000 norm=1.1305                    \n",
      "[iter 52] loss=0.2980 val_loss=0.5589 scale=2.0000 norm=1.1093                    \n",
      "[iter 53] loss=0.2919 val_loss=0.5561 scale=2.0000 norm=1.1349                    \n",
      "[iter 54] loss=0.2792 val_loss=0.5554 scale=1.0000 norm=0.5591                    \n",
      "[iter 55] loss=0.2780 val_loss=0.5542 scale=1.0000 norm=0.5634                    \n",
      "[iter 56] loss=0.2772 val_loss=0.5539 scale=2.0000 norm=1.1335                    \n",
      "[iter 57] loss=0.2761 val_loss=0.5546 scale=1.0000 norm=0.5638                    \n",
      "[iter 58] loss=0.2746 val_loss=0.5567 scale=2.0000 norm=1.1353                    \n",
      "[iter 59] loss=0.2664 val_loss=0.5569 scale=2.0000 norm=1.1425                    \n",
      "[iter 60] loss=0.2654 val_loss=0.5554 scale=2.0000 norm=1.1301                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL56 (val_loss=0.5539)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=5,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.08017520606671898, 'minibatch_frac': 0.6, 'n_estimators': 2000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5178 val_loss=1.4010 scale=1.0000 norm=1.0617                     \n",
      "[iter 1] loss=1.2944 val_loss=1.2150 scale=2.0000 norm=1.7143                     \n",
      "[iter 2] loss=1.1613 val_loss=1.1511 scale=2.0000 norm=1.5181                     \n",
      "[iter 3] loss=1.0742 val_loss=1.0872 scale=2.0000 norm=1.4064                     \n",
      "[iter 4] loss=1.0168 val_loss=1.0276 scale=2.0000 norm=1.3674                     \n",
      "[iter 5] loss=0.9464 val_loss=0.9755 scale=2.0000 norm=1.3107                     \n",
      "[iter 6] loss=0.8822 val_loss=0.9277 scale=2.0000 norm=1.2591                     \n",
      "[iter 7] loss=0.8424 val_loss=0.8845 scale=2.0000 norm=1.2507                     \n",
      "[iter 8] loss=0.7905 val_loss=0.8450 scale=2.0000 norm=1.2392                     \n",
      "[iter 9] loss=0.7357 val_loss=0.8093 scale=2.0000 norm=1.2139                     \n",
      "[iter 10] loss=0.6855 val_loss=0.7764 scale=2.0000 norm=1.1911                    \n",
      "[iter 11] loss=0.6500 val_loss=0.7443 scale=2.0000 norm=1.2090                    \n",
      "[iter 12] loss=0.5985 val_loss=0.7162 scale=2.0000 norm=1.1655                    \n",
      "[iter 13] loss=0.5493 val_loss=0.6914 scale=2.0000 norm=1.1446                    \n",
      "[iter 14] loss=0.5268 val_loss=0.6716 scale=2.0000 norm=1.1653                    \n",
      "[iter 15] loss=0.4996 val_loss=0.6496 scale=2.0000 norm=1.1617                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 16] loss=0.4613 val_loss=0.6200 scale=2.0000 norm=1.1512                    \n",
      "[iter 17] loss=0.4424 val_loss=0.6075 scale=2.0000 norm=1.1496                    \n",
      "[iter 18] loss=0.4172 val_loss=0.6009 scale=2.0000 norm=1.1455                    \n",
      "[iter 19] loss=0.3921 val_loss=0.5961 scale=2.0000 norm=1.1515                    \n",
      "[iter 20] loss=0.3654 val_loss=0.5892 scale=2.0000 norm=1.1489                    \n",
      "[iter 21] loss=0.3673 val_loss=0.5774 scale=2.0000 norm=1.1736                    \n",
      "[iter 22] loss=0.3425 val_loss=0.5767 scale=1.0000 norm=0.5885                    \n",
      "[iter 23] loss=0.3133 val_loss=0.5801 scale=2.0000 norm=1.1407                    \n",
      "[iter 24] loss=0.3265 val_loss=0.5798 scale=2.0000 norm=1.1680                    \n",
      "[iter 25] loss=0.3037 val_loss=0.5760 scale=1.0000 norm=0.5767                    \n",
      "[iter 26] loss=0.3217 val_loss=0.5778 scale=1.0000 norm=0.5999                    \n",
      "[iter 27] loss=0.2846 val_loss=0.5789 scale=1.0000 norm=0.5729                    \n",
      "[iter 28] loss=0.2924 val_loss=0.5820 scale=1.0000 norm=0.5840                    \n",
      "[iter 29] loss=0.2901 val_loss=0.5837 scale=1.0000 norm=0.5832                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL25 (val_loss=0.5760)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=3,\n",
      "                      max_features='sqrt', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.4026456020733489, 'minibatch_frac': 0.7, 'n_estimators': 1000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 5}}\n",
      "[iter 0] loss=1.5211 val_loss=1.3863 scale=1.0000 norm=1.0651                     \n",
      "[iter 1] loss=1.3160 val_loss=1.2020 scale=2.0000 norm=1.6833                     \n",
      "[iter 2] loss=1.1430 val_loss=1.0605 scale=2.0000 norm=1.5449                     \n",
      "[iter 3] loss=1.0220 val_loss=1.0075 scale=1.0000 norm=0.7710                     \n",
      "[iter 4] loss=1.0075 val_loss=0.9912 scale=1.0000 norm=0.7615                     \n",
      "[iter 5] loss=0.9800 val_loss=0.9786 scale=1.0000 norm=0.7473                     \n",
      "[iter 6] loss=0.9318 val_loss=0.9613 scale=2.0000 norm=1.4822                     \n",
      "[iter 7] loss=0.9258 val_loss=0.9050 scale=1.0000 norm=0.7506                     \n",
      "[iter 8] loss=0.8916 val_loss=0.8605 scale=1.0000 norm=0.7371                     \n",
      "[iter 9] loss=0.8377 val_loss=0.8489 scale=1.0000 norm=0.7364                     \n",
      "[iter 10] loss=0.8221 val_loss=0.8067 scale=1.0000 norm=0.7198                    \n",
      "[iter 11] loss=0.7710 val_loss=0.7934 scale=1.0000 norm=0.7231                    \n",
      "[iter 12] loss=0.7799 val_loss=0.7659 scale=1.0000 norm=0.7100                    \n",
      "[iter 13] loss=0.7078 val_loss=0.7346 scale=1.0000 norm=0.6918                    \n",
      "[iter 14] loss=0.6932 val_loss=0.7202 scale=1.0000 norm=0.6961                    \n",
      "[iter 15] loss=0.6688 val_loss=0.7189 scale=1.0000 norm=0.6981                    \n",
      "[iter 16] loss=0.6389 val_loss=0.7156 scale=1.0000 norm=0.7034                    \n",
      "[iter 17] loss=0.6633 val_loss=0.7135 scale=1.0000 norm=0.7093                    \n",
      "[iter 18] loss=0.6602 val_loss=0.7411 scale=1.0000 norm=0.7131                    \n",
      "[iter 19] loss=0.6234 val_loss=0.7337 scale=0.5000 norm=0.3529                    \n",
      "[iter 20] loss=0.6481 val_loss=0.7267 scale=1.0000 norm=0.7064                    \n",
      "[iter 21] loss=0.6415 val_loss=0.7321 scale=1.0000 norm=0.7187                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL17 (val_loss=0.7135)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features='sqrt', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.1275647796964525, 'minibatch_frac': 0.8, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5200 val_loss=1.2919 scale=1.0000 norm=1.0640                     \n",
      "[iter 1] loss=1.2124 val_loss=1.1616 scale=2.0000 norm=1.5869                     \n",
      "[iter 2] loss=1.1039 val_loss=1.0643 scale=2.0000 norm=1.4731                     \n",
      "[iter 3] loss=0.9912 val_loss=0.9858 scale=2.0000 norm=1.3578                     \n",
      "[iter 4] loss=0.9138 val_loss=0.9258 scale=2.0000 norm=1.3165                     \n",
      "[iter 5] loss=0.8300 val_loss=0.8615 scale=2.0000 norm=1.2733                     \n",
      "[iter 6] loss=0.7370 val_loss=0.8082 scale=2.0000 norm=1.2194                     \n",
      "[iter 7] loss=0.6672 val_loss=0.7659 scale=2.0000 norm=1.1899                     \n",
      "[iter 8] loss=0.6153 val_loss=0.7534 scale=1.0000 norm=0.6010                     \n",
      "[iter 9] loss=0.5744 val_loss=0.7301 scale=2.0000 norm=1.1771                     \n",
      "[iter 10] loss=0.5043 val_loss=0.7108 scale=2.0000 norm=1.1462                    \n",
      "[iter 11] loss=0.4645 val_loss=0.7023 scale=2.0000 norm=1.1545                    \n",
      "[iter 12] loss=0.4055 val_loss=0.6927 scale=2.0000 norm=1.1308                    \n",
      "[iter 13] loss=0.3647 val_loss=0.6816 scale=2.0000 norm=1.1180                    \n",
      "[iter 14] loss=0.3263 val_loss=0.6774 scale=1.0000 norm=0.5590                    \n",
      "[iter 15] loss=0.3231 val_loss=0.6680 scale=1.0000 norm=0.5647                    \n",
      "[iter 16] loss=0.2958 val_loss=0.6710 scale=1.0000 norm=0.5596                    \n",
      "[iter 17] loss=0.2947 val_loss=0.6716 scale=1.0000 norm=0.5631                    \n",
      "[iter 18] loss=0.2805 val_loss=0.6739 scale=1.0000 norm=0.5622                    \n",
      "[iter 19] loss=0.2646 val_loss=0.6684 scale=1.0000 norm=0.5607                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL15 (val_loss=0.6680)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=3,\n",
      "                      max_features=0.75, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.1735251010902597, 'minibatch_frac': 0.6, 'n_estimators': 2000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 10}}\n",
      "[iter 0] loss=1.5178 val_loss=1.3303 scale=1.0000 norm=1.0617                     \n",
      "[iter 1] loss=1.2506 val_loss=1.1184 scale=2.0000 norm=1.6245                     \n",
      "[iter 2] loss=1.0998 val_loss=1.0164 scale=2.0000 norm=1.4321                     \n",
      "[iter 3] loss=0.9529 val_loss=0.9271 scale=2.0000 norm=1.3260                     \n",
      "[iter 4] loss=0.8656 val_loss=0.8445 scale=2.0000 norm=1.3062                     \n",
      "[iter 5] loss=0.7710 val_loss=0.7849 scale=2.0000 norm=1.2734                     \n",
      "[iter 6] loss=0.6885 val_loss=0.7224 scale=2.0000 norm=1.2644                     \n",
      "[iter 7] loss=0.6645 val_loss=0.6838 scale=2.0000 norm=1.2765                     \n",
      "[iter 8] loss=0.5876 val_loss=0.6551 scale=1.0000 norm=0.6485                     \n",
      "[iter 9] loss=0.5702 val_loss=0.6261 scale=2.0000 norm=1.2902                     \n",
      "[iter 10] loss=0.5394 val_loss=0.5996 scale=2.0000 norm=1.3004                    \n",
      "[iter 11] loss=0.4970 val_loss=0.5964 scale=1.0000 norm=0.6503                    \n",
      "[iter 12] loss=0.4884 val_loss=0.5839 scale=2.0000 norm=1.2964                    \n",
      "[iter 13] loss=0.4447 val_loss=0.5851 scale=1.0000 norm=0.6291                    \n",
      "[iter 14] loss=0.4319 val_loss=0.5780 scale=1.0000 norm=0.6486                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 15] loss=0.4317 val_loss=0.5754 scale=1.0000 norm=0.6488                    \n",
      "[iter 16] loss=0.4267 val_loss=0.5698 scale=2.0000 norm=1.3020                    \n",
      "[iter 17] loss=0.4289 val_loss=0.5660 scale=1.0000 norm=0.6557                    \n",
      "[iter 18] loss=0.4247 val_loss=0.5657 scale=1.0000 norm=0.6543                    \n",
      "[iter 19] loss=0.4157 val_loss=0.5676 scale=1.0000 norm=0.6592                    \n",
      "[iter 20] loss=0.3927 val_loss=0.5686 scale=1.0000 norm=0.6512                    \n",
      "[iter 21] loss=0.4126 val_loss=0.5538 scale=1.0000 norm=0.6674                    \n",
      "[iter 22] loss=0.3908 val_loss=0.5577 scale=2.0000 norm=1.3273                    \n",
      "[iter 23] loss=0.3703 val_loss=0.5620 scale=1.0000 norm=0.6479                    \n",
      "[iter 24] loss=0.4005 val_loss=0.5621 scale=1.0000 norm=0.6676                    \n",
      "[iter 25] loss=0.3971 val_loss=0.5688 scale=2.0000 norm=1.3094                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL21 (val_loss=0.5538)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=1,\n",
      "                      max_features='sqrt', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.04884539424768673, 'minibatch_frac': 0.6, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5178 val_loss=1.5484 scale=1.0000 norm=1.0617                     \n",
      "[iter 1] loss=1.4802 val_loss=1.5261 scale=1.0000 norm=1.0250                     \n",
      "[iter 2] loss=1.4888 val_loss=1.5082 scale=1.0000 norm=1.0348                     \n",
      "[iter 3] loss=1.3912 val_loss=1.4992 scale=1.0000 norm=0.9386                     \n",
      "[iter 4] loss=1.4660 val_loss=1.4928 scale=1.0000 norm=1.0175                     \n",
      "[iter 5] loss=1.4370 val_loss=1.4831 scale=1.0000 norm=0.9898                     \n",
      "[iter 6] loss=1.3512 val_loss=1.4743 scale=2.0000 norm=1.8125                     \n",
      "[iter 7] loss=1.4462 val_loss=1.4692 scale=1.0000 norm=1.0105                     \n",
      "[iter 8] loss=1.4308 val_loss=1.4638 scale=1.0000 norm=0.9962                     \n",
      "[iter 9] loss=1.4252 val_loss=1.4653 scale=1.0000 norm=0.9917                     \n",
      "[iter 10] loss=1.4228 val_loss=1.4629 scale=2.0000 norm=1.9767                    \n",
      "[iter 11] loss=1.4412 val_loss=1.4613 scale=2.0000 norm=2.0240                    \n",
      "[iter 12] loss=1.3975 val_loss=1.4654 scale=2.0000 norm=1.9343                    \n",
      "[iter 13] loss=1.3902 val_loss=1.4621 scale=1.0000 norm=0.9633                    \n",
      "[iter 14] loss=1.3325 val_loss=1.4546 scale=2.0000 norm=1.8201                    \n",
      "[iter 15] loss=1.3601 val_loss=1.4520 scale=1.0000 norm=0.9447                    \n",
      "[iter 16] loss=1.3313 val_loss=1.4489 scale=2.0000 norm=1.8422                    \n",
      "[iter 17] loss=1.4160 val_loss=1.4506 scale=1.0000 norm=1.0091                    \n",
      "[iter 18] loss=1.4121 val_loss=1.4516 scale=2.0000 norm=2.0120                    \n",
      "[iter 19] loss=1.3858 val_loss=1.4497 scale=1.0000 norm=0.9809                    \n",
      "[iter 20] loss=1.3548 val_loss=1.4530 scale=2.0000 norm=1.8990                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL16 (val_loss=1.4489)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=3,\n",
      "                      max_features='log2', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.1353285774422666, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.4869 scale=1.0000 norm=1.0277                     \n",
      "[iter 1] loss=1.3316 val_loss=1.4071 scale=1.0000 norm=0.8917                     \n",
      "[iter 2] loss=1.2530 val_loss=1.3627 scale=1.0000 norm=0.8334                     \n",
      "[iter 3] loss=1.2002 val_loss=1.3266 scale=1.0000 norm=0.8039                     \n",
      "[iter 4] loss=1.1624 val_loss=1.2362 scale=2.0000 norm=1.5733                     \n",
      "[iter 5] loss=1.0911 val_loss=1.2038 scale=2.0000 norm=1.5202                     \n",
      "[iter 6] loss=1.0478 val_loss=1.1732 scale=2.0000 norm=1.5178                     \n",
      "[iter 7] loss=1.0049 val_loss=1.0922 scale=2.0000 norm=1.5053                     \n",
      "[iter 8] loss=0.9534 val_loss=1.0668 scale=2.0000 norm=1.4623                     \n",
      "[iter 9] loss=0.9202 val_loss=1.0493 scale=1.0000 norm=0.7302                     \n",
      "[iter 10] loss=0.8914 val_loss=1.0290 scale=1.0000 norm=0.7200                    \n",
      "[iter 11] loss=0.8594 val_loss=1.0226 scale=1.0000 norm=0.7077                    \n",
      "[iter 12] loss=0.8434 val_loss=1.0118 scale=1.0000 norm=0.7067                    \n",
      "[iter 13] loss=0.8224 val_loss=0.9887 scale=1.0000 norm=0.7024                    \n",
      "[iter 14] loss=0.7891 val_loss=0.9669 scale=2.0000 norm=1.3836                    \n",
      "[iter 15] loss=0.7556 val_loss=0.9579 scale=1.0000 norm=0.6879                    \n",
      "[iter 16] loss=0.7372 val_loss=0.9606 scale=2.0000 norm=1.3730                    \n",
      "[iter 17] loss=0.7194 val_loss=0.9612 scale=1.0000 norm=0.6923                    \n",
      "[iter 18] loss=0.7078 val_loss=0.9516 scale=1.0000 norm=0.6918                    \n",
      "[iter 19] loss=0.6955 val_loss=0.9546 scale=2.0000 norm=1.3809                    \n",
      "[iter 20] loss=0.6803 val_loss=0.9580 scale=1.0000 norm=0.6929                    \n",
      "[iter 21] loss=0.6737 val_loss=0.9496 scale=1.0000 norm=0.6943                    \n",
      "[iter 22] loss=0.6656 val_loss=0.9522 scale=1.0000 norm=0.6944                    \n",
      "[iter 23] loss=0.6580 val_loss=0.9487 scale=1.0000 norm=0.6943                    \n",
      "[iter 24] loss=0.6520 val_loss=0.9428 scale=1.0000 norm=0.6950                    \n",
      "[iter 25] loss=0.6371 val_loss=0.9431 scale=1.0000 norm=0.6920                    \n",
      "[iter 26] loss=0.6300 val_loss=0.9461 scale=1.0000 norm=0.6920                    \n",
      "[iter 27] loss=0.6244 val_loss=0.9524 scale=1.0000 norm=0.6927                    \n",
      "[iter 28] loss=0.6200 val_loss=0.9551 scale=1.0000 norm=0.6939                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL24 (val_loss=0.9428)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.09993496818410182, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.3188 scale=1.0000 norm=1.0277                     \n",
      "[iter 1] loss=1.2214 val_loss=1.1893 scale=2.0000 norm=1.6022                     \n",
      "[iter 2] loss=1.0965 val_loss=1.0979 scale=2.0000 norm=1.4385                     \n",
      "[iter 3] loss=1.0070 val_loss=1.0260 scale=2.0000 norm=1.3492                     \n",
      "[iter 4] loss=0.9265 val_loss=0.9465 scale=2.0000 norm=1.2872                     \n",
      "[iter 5] loss=0.8505 val_loss=0.8715 scale=2.0000 norm=1.2409                     \n",
      "[iter 6] loss=0.7788 val_loss=0.8136 scale=2.0000 norm=1.2062                     \n",
      "[iter 7] loss=0.7112 val_loss=0.7618 scale=2.0000 norm=1.1790                     \n",
      "[iter 8] loss=0.6486 val_loss=0.7196 scale=2.0000 norm=1.1588                     \n",
      "[iter 9] loss=0.5904 val_loss=0.6828 scale=2.0000 norm=1.1416                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 10] loss=0.5355 val_loss=0.6523 scale=2.0000 norm=1.1263                    \n",
      "[iter 11] loss=0.4839 val_loss=0.6231 scale=2.0000 norm=1.1136                    \n",
      "[iter 12] loss=0.4378 val_loss=0.6033 scale=2.0000 norm=1.1050                    \n",
      "[iter 13] loss=0.3957 val_loss=0.5849 scale=2.0000 norm=1.0974                    \n",
      "[iter 14] loss=0.3575 val_loss=0.5721 scale=2.0000 norm=1.0918                    \n",
      "[iter 15] loss=0.3215 val_loss=0.5610 scale=2.0000 norm=1.0843                    \n",
      "[iter 16] loss=0.2872 val_loss=0.5624 scale=2.0000 norm=1.0772                    \n",
      "[iter 17] loss=0.2600 val_loss=0.5630 scale=2.0000 norm=1.0765                    \n",
      "[iter 18] loss=0.2329 val_loss=0.5668 scale=1.0000 norm=0.5369                    \n",
      "[iter 19] loss=0.2195 val_loss=0.5796 scale=2.0000 norm=1.0712                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL15 (val_loss=0.5610)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.08131144203158858, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.3473 scale=1.0000 norm=1.0277                     \n",
      "[iter 1] loss=1.2346 val_loss=1.2159 scale=2.0000 norm=1.6248                     \n",
      "[iter 2] loss=1.1210 val_loss=1.1225 scale=2.0000 norm=1.4708                     \n",
      "[iter 3] loss=1.0442 val_loss=1.0582 scale=2.0000 norm=1.3874                     \n",
      "[iter 4] loss=0.9750 val_loss=0.9898 scale=2.0000 norm=1.3266                     \n",
      "[iter 5] loss=0.9096 val_loss=0.9309 scale=2.0000 norm=1.2788                     \n",
      "[iter 6] loss=0.8492 val_loss=0.8762 scale=2.0000 norm=1.2433                     \n",
      "[iter 7] loss=0.7906 val_loss=0.8263 scale=2.0000 norm=1.2135                     \n",
      "[iter 8] loss=0.7348 val_loss=0.7753 scale=2.0000 norm=1.1901                     \n",
      "[iter 9] loss=0.6816 val_loss=0.7368 scale=2.0000 norm=1.1715                     \n",
      "[iter 10] loss=0.6308 val_loss=0.7034 scale=2.0000 norm=1.1542                    \n",
      "[iter 11] loss=0.5821 val_loss=0.6673 scale=2.0000 norm=1.1390                    \n",
      "[iter 12] loss=0.5375 val_loss=0.6326 scale=2.0000 norm=1.1281                    \n",
      "[iter 13] loss=0.4954 val_loss=0.6106 scale=2.0000 norm=1.1177                    \n",
      "[iter 14] loss=0.4552 val_loss=0.5887 scale=2.0000 norm=1.1078                    \n",
      "[iter 15] loss=0.4166 val_loss=0.5697 scale=2.0000 norm=1.0970                    \n",
      "[iter 16] loss=0.3809 val_loss=0.5520 scale=2.0000 norm=1.0885                    \n",
      "[iter 17] loss=0.3499 val_loss=0.5408 scale=2.0000 norm=1.0844                    \n",
      "[iter 18] loss=0.3184 val_loss=0.5378 scale=2.0000 norm=1.0781                    \n",
      "[iter 19] loss=0.2900 val_loss=0.5309 scale=2.0000 norm=1.0737                    \n",
      "[iter 20] loss=0.2649 val_loss=0.5281 scale=2.0000 norm=1.0709                    \n",
      "[iter 21] loss=0.2424 val_loss=0.5269 scale=2.0000 norm=1.0708                    \n",
      "[iter 22] loss=0.2202 val_loss=0.5366 scale=2.0000 norm=1.0678                    \n",
      "[iter 23] loss=0.2006 val_loss=0.5460 scale=2.0000 norm=1.0672                    \n",
      "[iter 24] loss=0.1804 val_loss=0.5552 scale=2.0000 norm=1.0634                    \n",
      "[iter 25] loss=0.1645 val_loss=0.5553 scale=1.0000 norm=0.5324                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL21 (val_loss=0.5269)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=3,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.1504667175227341, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.3550 scale=1.0000 norm=1.0277                     \n",
      "[iter 1] loss=1.2898 val_loss=1.1807 scale=2.0000 norm=1.7020                     \n",
      "[iter 2] loss=1.1159 val_loss=1.0683 scale=2.0000 norm=1.4604                     \n",
      "[iter 3] loss=1.0062 val_loss=0.9786 scale=2.0000 norm=1.3708                     \n",
      "[iter 4] loss=0.9116 val_loss=0.9040 scale=2.0000 norm=1.3204                     \n",
      "[iter 5] loss=0.8307 val_loss=0.8346 scale=2.0000 norm=1.2924                     \n",
      "[iter 6] loss=0.7511 val_loss=0.7852 scale=2.0000 norm=1.2695                     \n",
      "[iter 7] loss=0.6925 val_loss=0.7181 scale=2.0000 norm=1.2691                     \n",
      "[iter 8] loss=0.6307 val_loss=0.6830 scale=2.0000 norm=1.2574                     \n",
      "[iter 9] loss=0.5832 val_loss=0.6540 scale=2.0000 norm=1.2592                     \n",
      "[iter 10] loss=0.5418 val_loss=0.6414 scale=2.0000 norm=1.2610                    \n",
      "[iter 11] loss=0.5066 val_loss=0.6326 scale=1.0000 norm=0.6321                    \n",
      "[iter 12] loss=0.4918 val_loss=0.6165 scale=2.0000 norm=1.2668                    \n",
      "[iter 13] loss=0.4689 val_loss=0.6156 scale=1.0000 norm=0.6375                    \n",
      "[iter 14] loss=0.4579 val_loss=0.6135 scale=1.0000 norm=0.6383                    \n",
      "[iter 15] loss=0.4465 val_loss=0.6112 scale=1.0000 norm=0.6384                    \n",
      "[iter 16] loss=0.4350 val_loss=0.6113 scale=1.0000 norm=0.6377                    \n",
      "[iter 17] loss=0.4278 val_loss=0.6187 scale=2.0000 norm=1.2804                    \n",
      "[iter 18] loss=0.4164 val_loss=0.6212 scale=1.0000 norm=0.6456                    \n",
      "[iter 19] loss=0.4099 val_loss=0.6182 scale=1.0000 norm=0.6465                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL15 (val_loss=0.6112)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=1,\n",
      "                      max_features=0.75, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.2630617020395969, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.5037 scale=1.0000 norm=1.0277                     \n",
      "[iter 1] loss=1.3894 val_loss=1.3715 scale=1.0000 norm=0.9314                     \n",
      "[iter 2] loss=1.2322 val_loss=1.1604 scale=2.0000 norm=1.6215                     \n",
      "[iter 3] loss=1.0717 val_loss=1.0798 scale=2.0000 norm=1.4615                     \n",
      "[iter 4] loss=0.9667 val_loss=1.0294 scale=1.0000 norm=0.7137                     \n",
      "[iter 5] loss=0.9068 val_loss=0.9905 scale=1.0000 norm=0.7049                     \n",
      "[iter 6] loss=0.8575 val_loss=0.9058 scale=2.0000 norm=1.4011                     \n",
      "[iter 7] loss=0.7995 val_loss=0.8759 scale=1.0000 norm=0.7134                     \n",
      "[iter 8] loss=0.7688 val_loss=0.8550 scale=2.0000 norm=1.4263                     \n",
      "[iter 9] loss=0.7334 val_loss=0.8329 scale=1.0000 norm=0.7243                     \n",
      "[iter 10] loss=0.7095 val_loss=0.8193 scale=1.0000 norm=0.7217                    \n",
      "[iter 11] loss=0.6869 val_loss=0.7953 scale=1.0000 norm=0.7180                    \n",
      "[iter 12] loss=0.6662 val_loss=0.7908 scale=1.0000 norm=0.7176                    \n",
      "[iter 13] loss=0.6503 val_loss=0.7569 scale=1.0000 norm=0.7167                    \n",
      "[iter 14] loss=0.6357 val_loss=0.7576 scale=1.0000 norm=0.7179                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 15] loss=0.6247 val_loss=0.7479 scale=2.0000 norm=1.4355                    \n",
      "[iter 16] loss=0.6144 val_loss=0.7394 scale=1.0000 norm=0.7285                    \n",
      "[iter 17] loss=0.6034 val_loss=0.7359 scale=0.5000 norm=0.3633                    \n",
      "[iter 18] loss=0.6003 val_loss=0.7213 scale=1.0000 norm=0.7262                    \n",
      "[iter 19] loss=0.5903 val_loss=0.7137 scale=1.0000 norm=0.7256                    \n",
      "[iter 20] loss=0.5785 val_loss=0.7148 scale=0.5000 norm=0.3617                    \n",
      "[iter 21] loss=0.5754 val_loss=0.7131 scale=0.5000 norm=0.3615                    \n",
      "[iter 22] loss=0.5735 val_loss=0.7133 scale=2.0000 norm=1.4461                    \n",
      "[iter 23] loss=0.5691 val_loss=0.7208 scale=2.0000 norm=1.4554                    \n",
      "[iter 24] loss=0.5652 val_loss=0.7171 scale=1.0000 norm=0.7296                    \n",
      "[iter 25] loss=0.5618 val_loss=0.7064 scale=1.0000 norm=0.7285                    \n",
      "[iter 26] loss=0.5597 val_loss=0.6994 scale=1.0000 norm=0.7281                    \n",
      "[iter 27] loss=0.5540 val_loss=0.6973 scale=0.5000 norm=0.3632                    \n",
      "[iter 28] loss=0.5523 val_loss=0.6960 scale=0.5000 norm=0.3625                    \n",
      "[iter 29] loss=0.5514 val_loss=0.6882 scale=2.0000 norm=1.4485                    \n",
      "[iter 30] loss=0.5467 val_loss=0.6852 scale=0.5000 norm=0.3628                    \n",
      "[iter 31] loss=0.5452 val_loss=0.6838 scale=2.0000 norm=1.4497                    \n",
      "[iter 32] loss=0.5428 val_loss=0.6720 scale=1.0000 norm=0.7262                    \n",
      "[iter 33] loss=0.5381 val_loss=0.6718 scale=0.1250 norm=0.0905                    \n",
      "[iter 34] loss=0.5380 val_loss=0.6685 scale=1.0000 norm=0.7242                    \n",
      "[iter 35] loss=0.5350 val_loss=0.6651 scale=2.0000 norm=1.4473                    \n",
      "[iter 36] loss=0.5331 val_loss=0.6572 scale=4.0000 norm=2.9007                    \n",
      "[iter 37] loss=0.5285 val_loss=0.6488 scale=4.0000 norm=2.9018                    \n",
      "[iter 38] loss=0.5239 val_loss=0.6481 scale=1.0000 norm=0.7254                    \n",
      "[iter 39] loss=0.5230 val_loss=0.6449 scale=1.0000 norm=0.7248                    \n",
      "[iter 40] loss=0.5182 val_loss=0.6469 scale=2.0000 norm=1.4443                    \n",
      "[iter 41] loss=0.5148 val_loss=0.6469 scale=0.0010 norm=0.0007                    \n",
      "[iter 42] loss=0.5148 val_loss=0.6469 scale=0.0010 norm=0.0007                    \n",
      "[iter 43] loss=0.5148 val_loss=0.6469 scale=0.0010 norm=0.0007                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL39 (val_loss=0.6449)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.19944442887670794, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.2243 scale=1.0000 norm=1.0277                     \n",
      "[iter 1] loss=1.1769 val_loss=1.0541 scale=2.0000 norm=1.5119                     \n",
      "[iter 2] loss=0.9927 val_loss=0.9026 scale=2.0000 norm=1.3089                     \n",
      "[iter 3] loss=0.8410 val_loss=0.8010 scale=2.0000 norm=1.2179                     \n",
      "[iter 4] loss=0.7061 val_loss=0.7179 scale=2.0000 norm=1.1665                     \n",
      "[iter 5] loss=0.5867 val_loss=0.6384 scale=2.0000 norm=1.1346                     \n",
      "[iter 6] loss=0.4809 val_loss=0.5977 scale=2.0000 norm=1.1092                     \n",
      "[iter 7] loss=0.3960 val_loss=0.5927 scale=2.0000 norm=1.0949                     \n",
      "[iter 8] loss=0.3293 val_loss=0.6002 scale=2.0000 norm=1.0926                     \n",
      "[iter 9] loss=0.2733 val_loss=0.6279 scale=2.0000 norm=1.0872                     \n",
      "[iter 10] loss=0.2305 val_loss=0.6619 scale=2.0000 norm=1.0898                    \n",
      "[iter 11] loss=0.1988 val_loss=0.7214 scale=2.0000 norm=1.0976                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL7 (val_loss=0.5927)                                        \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features='log2', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.16894928671051157, 'minibatch_frac': 1.0, 'n_estimators': 1000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.2844 scale=1.0000 norm=1.0277                     \n",
      "[iter 1] loss=1.2218 val_loss=1.1765 scale=1.0000 norm=0.7983                     \n",
      "[iter 2] loss=1.1081 val_loss=1.0470 scale=2.0000 norm=1.4471                     \n",
      "[iter 3] loss=0.9747 val_loss=0.9462 scale=2.0000 norm=1.3350                     \n",
      "[iter 4] loss=0.8547 val_loss=0.8688 scale=2.0000 norm=1.2580                     \n",
      "[iter 5] loss=0.7525 val_loss=0.7966 scale=2.0000 norm=1.2185                     \n",
      "[iter 6] loss=0.6532 val_loss=0.7421 scale=2.0000 norm=1.1834                     \n",
      "[iter 7] loss=0.5701 val_loss=0.7125 scale=2.0000 norm=1.1633                     \n",
      "[iter 8] loss=0.4955 val_loss=0.6821 scale=2.0000 norm=1.1494                     \n",
      "[iter 9] loss=0.4334 val_loss=0.6678 scale=2.0000 norm=1.1431                     \n",
      "[iter 10] loss=0.3798 val_loss=0.6695 scale=2.0000 norm=1.1351                    \n",
      "[iter 11] loss=0.3313 val_loss=0.6680 scale=1.0000 norm=0.5634                    \n",
      "[iter 12] loss=0.3102 val_loss=0.6969 scale=2.0000 norm=1.1238                    \n",
      "[iter 13] loss=0.2768 val_loss=0.6920 scale=1.0000 norm=0.5638                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL9 (val_loss=0.6678)                                        \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.75, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.11313076398419838, 'minibatch_frac': 0.7, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5211 val_loss=1.3230 scale=1.0000 norm=1.0651                     \n",
      "[iter 1] loss=1.2141 val_loss=1.1604 scale=2.0000 norm=1.5912                     \n",
      "[iter 2] loss=1.0883 val_loss=1.0657 scale=2.0000 norm=1.4155                     \n",
      "[iter 3] loss=0.9789 val_loss=0.9861 scale=2.0000 norm=1.3118                     \n",
      "[iter 4] loss=0.9085 val_loss=0.9160 scale=2.0000 norm=1.2713                     \n",
      "[iter 5] loss=0.8143 val_loss=0.8556 scale=2.0000 norm=1.2163                     \n",
      "[iter 6] loss=0.7305 val_loss=0.8018 scale=2.0000 norm=1.1734                     \n",
      "[iter 7] loss=0.6635 val_loss=0.7571 scale=2.0000 norm=1.1586                     \n",
      "[iter 8] loss=0.5997 val_loss=0.7128 scale=2.0000 norm=1.1503                     \n",
      "[iter 9] loss=0.5336 val_loss=0.6791 scale=2.0000 norm=1.1287                     \n",
      "[iter 10] loss=0.4658 val_loss=0.6583 scale=2.0000 norm=1.1006                    \n",
      "[iter 11] loss=0.4330 val_loss=0.6374 scale=2.0000 norm=1.1105                    \n",
      "[iter 12] loss=0.3663 val_loss=0.6302 scale=2.0000 norm=1.0797                    \n",
      "[iter 13] loss=0.3302 val_loss=0.6323 scale=2.0000 norm=1.0738                    \n",
      "[iter 14] loss=0.2939 val_loss=0.6350 scale=2.0000 norm=1.0738                    \n",
      "[iter 15] loss=0.2755 val_loss=0.6377 scale=1.0000 norm=0.5427                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 16] loss=0.2456 val_loss=0.6589 scale=2.0000 norm=1.0705                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL12 (val_loss=0.6302)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.06351813178391136, 'minibatch_frac': 0.8, 'n_estimators': 2000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5200 val_loss=1.3597 scale=1.0000 norm=1.0640                     \n",
      "[iter 1] loss=1.2391 val_loss=1.2425 scale=2.0000 norm=1.6297                     \n",
      "[iter 2] loss=1.1429 val_loss=1.1701 scale=2.0000 norm=1.5003                     \n",
      "[iter 3] loss=1.0751 val_loss=1.1133 scale=2.0000 norm=1.4147                     \n",
      "[iter 4] loss=1.0262 val_loss=1.0552 scale=2.0000 norm=1.3732                     \n",
      "[iter 5] loss=0.9704 val_loss=1.0031 scale=2.0000 norm=1.3303                     \n",
      "[iter 6] loss=0.9118 val_loss=0.9610 scale=2.0000 norm=1.2749                     \n",
      "[iter 7] loss=0.8653 val_loss=0.9218 scale=2.0000 norm=1.2490                     \n",
      "[iter 8] loss=0.8286 val_loss=0.8877 scale=2.0000 norm=1.2407                     \n",
      "[iter 9] loss=0.7818 val_loss=0.8525 scale=2.0000 norm=1.2121                     \n",
      "[iter 10] loss=0.7324 val_loss=0.8193 scale=2.0000 norm=1.1871                    \n",
      "[iter 11] loss=0.6939 val_loss=0.7887 scale=2.0000 norm=1.1814                    \n",
      "[iter 12] loss=0.6475 val_loss=0.7633 scale=2.0000 norm=1.1507                    \n",
      "[iter 13] loss=0.6061 val_loss=0.7376 scale=2.0000 norm=1.1415                    \n",
      "[iter 14] loss=0.5727 val_loss=0.7150 scale=2.0000 norm=1.1353                    \n",
      "[iter 15] loss=0.5427 val_loss=0.6918 scale=2.0000 norm=1.1341                    \n",
      "[iter 16] loss=0.5038 val_loss=0.6751 scale=2.0000 norm=1.1187                    \n",
      "[iter 17] loss=0.4826 val_loss=0.6637 scale=2.0000 norm=1.1222                    \n",
      "[iter 18] loss=0.4499 val_loss=0.6487 scale=2.0000 norm=1.1103                    \n",
      "[iter 19] loss=0.4222 val_loss=0.6392 scale=2.0000 norm=1.1023                    \n",
      "[iter 20] loss=0.3928 val_loss=0.6243 scale=2.0000 norm=1.0965                    \n",
      "[iter 21] loss=0.3702 val_loss=0.6198 scale=2.0000 norm=1.0911                    \n",
      "[iter 22] loss=0.3442 val_loss=0.6143 scale=2.0000 norm=1.0902                    \n",
      "[iter 23] loss=0.3152 val_loss=0.6177 scale=2.0000 norm=1.0759                    \n",
      "[iter 24] loss=0.3021 val_loss=0.6207 scale=2.0000 norm=1.0796                    \n",
      "[iter 25] loss=0.2789 val_loss=0.6209 scale=2.0000 norm=1.0802                    \n",
      "[iter 26] loss=0.2639 val_loss=0.6214 scale=1.0000 norm=0.5397                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL22 (val_loss=0.6143)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=1,\n",
      "                      max_features='log2', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.13615510756553728, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 5}}\n",
      "[iter 0] loss=1.4840 val_loss=1.5058 scale=1.0000 norm=1.0277                     \n",
      "[iter 1] loss=1.4293 val_loss=1.4768 scale=1.0000 norm=0.9782                     \n",
      "[iter 2] loss=1.4092 val_loss=1.4626 scale=1.0000 norm=0.9654                     \n",
      "[iter 3] loss=1.3979 val_loss=1.4538 scale=1.0000 norm=0.9616                     \n",
      "[iter 4] loss=1.3906 val_loss=1.4450 scale=2.0000 norm=1.9230                     \n",
      "[iter 5] loss=1.3806 val_loss=1.4470 scale=2.0000 norm=1.9300                     \n",
      "[iter 6] loss=1.3712 val_loss=1.4537 scale=2.0000 norm=1.9304                     \n",
      "[iter 7] loss=1.3634 val_loss=1.4400 scale=2.0000 norm=1.9292                     \n",
      "[iter 8] loss=1.3562 val_loss=1.4495 scale=2.0000 norm=1.9342                     \n",
      "[iter 9] loss=1.3511 val_loss=1.4439 scale=1.0000 norm=0.9681                     \n",
      "[iter 10] loss=1.3487 val_loss=1.4432 scale=1.0000 norm=0.9692                    \n",
      "[iter 11] loss=1.3463 val_loss=1.4451 scale=1.0000 norm=0.9703                    \n",
      "[iter 12] loss=1.3445 val_loss=1.4397 scale=1.0000 norm=0.9704                    \n",
      "[iter 13] loss=1.3423 val_loss=1.4445 scale=1.0000 norm=0.9712                    \n",
      "[iter 14] loss=1.3406 val_loss=1.4425 scale=2.0000 norm=1.9418                    \n",
      "[iter 15] loss=1.3367 val_loss=1.4389 scale=2.0000 norm=1.9419                    \n",
      "[iter 16] loss=1.3339 val_loss=1.4339 scale=1.0000 norm=0.9713                    \n",
      "[iter 17] loss=1.3321 val_loss=1.4382 scale=1.0000 norm=0.9716                    \n",
      "[iter 18] loss=1.3304 val_loss=1.4387 scale=1.0000 norm=0.9708                    \n",
      "[iter 19] loss=1.3289 val_loss=1.4379 scale=2.0000 norm=1.9423                    \n",
      "[iter 20] loss=1.3259 val_loss=1.4359 scale=1.0000 norm=0.9706                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL16 (val_loss=1.4339)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=1,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.23010309560836498, 'minibatch_frac': 0.6, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5178 val_loss=1.5093 scale=1.0000 norm=1.0617                     \n",
      "[iter 1] loss=1.4122 val_loss=1.4301 scale=1.0000 norm=0.9371                     \n",
      "[iter 2] loss=1.3537 val_loss=1.3005 scale=1.0000 norm=0.8981                     \n",
      "[iter 3] loss=1.2088 val_loss=1.2070 scale=1.0000 norm=0.7879                     \n",
      "[iter 4] loss=1.1286 val_loss=1.1290 scale=2.0000 norm=1.4952                     \n",
      "[iter 5] loss=1.0330 val_loss=1.0872 scale=2.0000 norm=1.4501                     \n",
      "[iter 6] loss=0.9759 val_loss=1.0556 scale=1.0000 norm=0.7234                     \n",
      "[iter 7] loss=0.9570 val_loss=1.0394 scale=2.0000 norm=1.4520                     \n",
      "[iter 8] loss=0.9187 val_loss=0.9811 scale=1.0000 norm=0.7601                     \n",
      "[iter 9] loss=0.8850 val_loss=0.9688 scale=1.0000 norm=0.7418                     \n",
      "[iter 10] loss=0.8563 val_loss=0.9559 scale=1.0000 norm=0.7273                    \n",
      "[iter 11] loss=0.8671 val_loss=0.9335 scale=1.0000 norm=0.7685                    \n",
      "[iter 12] loss=0.8190 val_loss=0.9075 scale=2.0000 norm=1.4518                    \n",
      "[iter 13] loss=0.7668 val_loss=0.9160 scale=2.0000 norm=1.4292                    \n",
      "[iter 14] loss=0.7804 val_loss=0.8734 scale=1.0000 norm=0.7692                    \n",
      "[iter 15] loss=0.7752 val_loss=0.8657 scale=1.0000 norm=0.7466                    \n",
      "[iter 16] loss=0.7408 val_loss=0.8643 scale=1.0000 norm=0.7311                    \n",
      "[iter 17] loss=0.7581 val_loss=0.8773 scale=2.0000 norm=1.4824                    \n",
      "[iter 18] loss=0.7371 val_loss=0.8674 scale=1.0000 norm=0.7337                    \n",
      "[iter 19] loss=0.7373 val_loss=0.8653 scale=1.0000 norm=0.7501                    \n",
      "[iter 20] loss=0.7309 val_loss=0.8599 scale=1.0000 norm=0.7500                    \n",
      "[iter 21] loss=0.7308 val_loss=0.8462 scale=1.0000 norm=0.7693                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 22] loss=0.7010 val_loss=0.8137 scale=1.0000 norm=0.7571                    \n",
      "[iter 23] loss=0.6625 val_loss=0.8253 scale=2.0000 norm=1.4287                    \n",
      "[iter 24] loss=0.6801 val_loss=0.8252 scale=1.0000 norm=0.7340                    \n",
      "[iter 25] loss=0.7035 val_loss=0.8215 scale=0.5000 norm=0.3708                    \n",
      "[iter 26] loss=0.7375 val_loss=0.8169 scale=0.5000 norm=0.3934                    \n",
      "[iter 27] loss=0.6587 val_loss=0.8132 scale=1.0000 norm=0.7145                    \n",
      "[iter 28] loss=0.7051 val_loss=0.7990 scale=2.0000 norm=1.5098                    \n",
      "[iter 29] loss=0.6797 val_loss=0.7998 scale=1.0000 norm=0.7422                    \n",
      "[iter 30] loss=0.6921 val_loss=0.8018 scale=1.0000 norm=0.7565                    \n",
      "[iter 31] loss=0.6804 val_loss=0.8046 scale=2.0000 norm=1.4892                    \n",
      "[iter 32] loss=0.6768 val_loss=0.8056 scale=0.5000 norm=0.3760                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL28 (val_loss=0.7990)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features='sqrt', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.15992706184197375, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.2924 scale=1.0000 norm=1.0277                     \n",
      "[iter 1] loss=1.2251 val_loss=1.1087 scale=2.0000 norm=1.6034                     \n",
      "[iter 2] loss=1.0549 val_loss=0.9987 scale=2.0000 norm=1.3895                     \n",
      "[iter 3] loss=0.9324 val_loss=0.9012 scale=2.0000 norm=1.2969                     \n",
      "[iter 4] loss=0.8193 val_loss=0.8252 scale=2.0000 norm=1.2358                     \n",
      "[iter 5] loss=0.7196 val_loss=0.7634 scale=2.0000 norm=1.1967                     \n",
      "[iter 6] loss=0.6288 val_loss=0.7172 scale=2.0000 norm=1.1684                     \n",
      "[iter 7] loss=0.5473 val_loss=0.6947 scale=2.0000 norm=1.1471                     \n",
      "[iter 8] loss=0.4776 val_loss=0.6720 scale=2.0000 norm=1.1342                     \n",
      "[iter 9] loss=0.4188 val_loss=0.6736 scale=2.0000 norm=1.1293                     \n",
      "[iter 10] loss=0.3689 val_loss=0.6958 scale=2.0000 norm=1.1266                    \n",
      "[iter 11] loss=0.3294 val_loss=0.7099 scale=2.0000 norm=1.1293                    \n",
      "[iter 12] loss=0.2932 val_loss=0.7559 scale=2.0000 norm=1.1266                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL8 (val_loss=0.6720)                                        \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=5,\n",
      "                      max_features='sqrt', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.09500339797364094, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 10}}\n",
      "[iter 0] loss=1.4840 val_loss=1.4316 scale=1.0000 norm=1.0277                     \n",
      "[iter 1] loss=1.2579 val_loss=1.2884 scale=1.0000 norm=0.8326                     \n",
      "[iter 2] loss=1.1817 val_loss=1.1853 scale=2.0000 norm=1.5538                     \n",
      "[iter 3] loss=1.0992 val_loss=1.1104 scale=2.0000 norm=1.4626                     \n",
      "[iter 4] loss=1.0326 val_loss=1.0628 scale=2.0000 norm=1.4114                     \n",
      "[iter 5] loss=0.9764 val_loss=1.0016 scale=2.0000 norm=1.3824                     \n",
      "[iter 6] loss=0.9139 val_loss=0.9436 scale=2.0000 norm=1.3405                     \n",
      "[iter 7] loss=0.8543 val_loss=0.8920 scale=2.0000 norm=1.3040                     \n",
      "[iter 8] loss=0.7964 val_loss=0.8459 scale=2.0000 norm=1.2746                     \n",
      "[iter 9] loss=0.7434 val_loss=0.8060 scale=2.0000 norm=1.2538                     \n",
      "[iter 10] loss=0.6916 val_loss=0.7682 scale=2.0000 norm=1.2340                    \n",
      "[iter 11] loss=0.6432 val_loss=0.7431 scale=2.0000 norm=1.2185                    \n",
      "[iter 12] loss=0.6033 val_loss=0.7141 scale=2.0000 norm=1.2125                    \n",
      "[iter 13] loss=0.5605 val_loss=0.6929 scale=2.0000 norm=1.2022                    \n",
      "[iter 14] loss=0.5250 val_loss=0.6747 scale=2.0000 norm=1.1983                    \n",
      "[iter 15] loss=0.4931 val_loss=0.6584 scale=2.0000 norm=1.1952                    \n",
      "[iter 16] loss=0.4571 val_loss=0.6521 scale=1.0000 norm=0.5911                    \n",
      "[iter 17] loss=0.4427 val_loss=0.6524 scale=2.0000 norm=1.1796                    \n",
      "[iter 18] loss=0.4177 val_loss=0.6442 scale=2.0000 norm=1.1794                    \n",
      "[iter 19] loss=0.3930 val_loss=0.6431 scale=1.0000 norm=0.5884                    \n",
      "[iter 20] loss=0.3817 val_loss=0.6412 scale=1.0000 norm=0.5883                    \n",
      "[iter 21] loss=0.3724 val_loss=0.6377 scale=1.0000 norm=0.5888                    \n",
      "[iter 22] loss=0.3620 val_loss=0.6384 scale=1.0000 norm=0.5889                    \n",
      "[iter 23] loss=0.3544 val_loss=0.6380 scale=1.0000 norm=0.5900                    \n",
      "[iter 24] loss=0.3466 val_loss=0.6376 scale=1.0000 norm=0.5913                    \n",
      "[iter 25] loss=0.3387 val_loss=0.6294 scale=2.0000 norm=1.1843                    \n",
      "[iter 26] loss=0.3234 val_loss=0.6293 scale=2.0000 norm=1.1852                    \n",
      "[iter 27] loss=0.3113 val_loss=0.6308 scale=1.0000 norm=0.5940                    \n",
      "[iter 28] loss=0.3055 val_loss=0.6313 scale=1.0000 norm=0.5946                    \n",
      "[iter 29] loss=0.2995 val_loss=0.6357 scale=2.0000 norm=1.1910                    \n",
      "[iter 30] loss=0.2897 val_loss=0.6381 scale=1.0000 norm=0.5978                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL26 (val_loss=0.6293)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=3,\n",
      "                      max_features='sqrt', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.0378751233538932, 'minibatch_frac': 0.7, 'n_estimators': 1000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5211 val_loss=1.5369 scale=1.0000 norm=1.0651                     \n",
      "[iter 1] loss=1.3749 val_loss=1.5047 scale=1.0000 norm=0.9286                     \n",
      "[iter 2] loss=1.3418 val_loss=1.4779 scale=1.0000 norm=0.9035                     \n",
      "[iter 3] loss=1.2944 val_loss=1.4683 scale=1.0000 norm=0.8593                     \n",
      "[iter 4] loss=1.2874 val_loss=1.4433 scale=1.0000 norm=0.8635                     \n",
      "[iter 5] loss=1.2472 val_loss=1.4227 scale=1.0000 norm=0.8331                     \n",
      "[iter 6] loss=1.2258 val_loss=1.3918 scale=2.0000 norm=1.6260                     \n",
      "[iter 7] loss=1.1972 val_loss=1.3666 scale=2.0000 norm=1.6078                     \n",
      "[iter 8] loss=1.1861 val_loss=1.3390 scale=2.0000 norm=1.6053                     \n",
      "[iter 9] loss=1.1674 val_loss=1.3160 scale=2.0000 norm=1.5825                     \n",
      "[iter 10] loss=1.1321 val_loss=1.3060 scale=2.0000 norm=1.5441                    \n",
      "[iter 11] loss=1.1385 val_loss=1.2932 scale=1.0000 norm=0.7886                    \n",
      "[iter 12] loss=1.1114 val_loss=1.2834 scale=1.0000 norm=0.7659                    \n",
      "[iter 13] loss=1.1006 val_loss=1.2725 scale=2.0000 norm=1.5230                    \n",
      "[iter 14] loss=1.1048 val_loss=1.2589 scale=1.0000 norm=0.7769                    \n",
      "[iter 15] loss=1.0840 val_loss=1.2484 scale=1.0000 norm=0.7605                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 16] loss=1.0738 val_loss=1.2357 scale=1.0000 norm=0.7601                    \n",
      "[iter 17] loss=1.0681 val_loss=1.2263 scale=1.0000 norm=0.7616                    \n",
      "[iter 18] loss=1.0508 val_loss=1.2135 scale=2.0000 norm=1.5010                    \n",
      "[iter 19] loss=1.0270 val_loss=1.2056 scale=2.0000 norm=1.4743                    \n",
      "[iter 20] loss=1.0234 val_loss=1.1727 scale=2.0000 norm=1.4798                    \n",
      "[iter 21] loss=1.0134 val_loss=1.1651 scale=2.0000 norm=1.4950                    \n",
      "[iter 22] loss=1.0009 val_loss=1.1585 scale=1.0000 norm=0.7399                    \n",
      "[iter 23] loss=0.9791 val_loss=1.1361 scale=2.0000 norm=1.4475                    \n",
      "[iter 24] loss=0.9637 val_loss=1.1264 scale=1.0000 norm=0.7207                    \n",
      "[iter 25] loss=0.9562 val_loss=1.1198 scale=1.0000 norm=0.7212                    \n",
      "[iter 26] loss=0.9696 val_loss=1.1120 scale=2.0000 norm=1.4735                    \n",
      "[iter 27] loss=0.9317 val_loss=1.0881 scale=2.0000 norm=1.4165                    \n",
      "[iter 28] loss=0.9372 val_loss=1.0759 scale=1.0000 norm=0.7248                    \n",
      "[iter 29] loss=0.9237 val_loss=1.0692 scale=2.0000 norm=1.4360                    \n",
      "[iter 30] loss=0.9145 val_loss=1.0668 scale=1.0000 norm=0.7187                    \n",
      "[iter 31] loss=0.9036 val_loss=1.0551 scale=1.0000 norm=0.7110                    \n",
      "[iter 32] loss=0.9018 val_loss=1.0494 scale=1.0000 norm=0.7192                    \n",
      "[iter 33] loss=0.8914 val_loss=1.0396 scale=2.0000 norm=1.4268                    \n",
      "[iter 34] loss=0.8860 val_loss=1.0293 scale=1.0000 norm=0.7128                    \n",
      "[iter 35] loss=0.8763 val_loss=1.0262 scale=2.0000 norm=1.4140                    \n",
      "[iter 36] loss=0.8679 val_loss=1.0193 scale=2.0000 norm=1.4208                    \n",
      "[iter 37] loss=0.8537 val_loss=1.0122 scale=2.0000 norm=1.4103                    \n",
      "[iter 38] loss=0.8305 val_loss=1.0021 scale=1.0000 norm=0.6913                    \n",
      "[iter 39] loss=0.8372 val_loss=0.9899 scale=1.0000 norm=0.7028                    \n",
      "[iter 40] loss=0.8358 val_loss=0.9846 scale=2.0000 norm=1.4148                    \n",
      "[iter 41] loss=0.8130 val_loss=0.9834 scale=2.0000 norm=1.3827                    \n",
      "[iter 42] loss=0.8078 val_loss=0.9794 scale=2.0000 norm=1.3902                    \n",
      "[iter 43] loss=0.8016 val_loss=0.9729 scale=2.0000 norm=1.3859                    \n",
      "[iter 44] loss=0.7996 val_loss=0.9696 scale=1.0000 norm=0.6973                    \n",
      "[iter 45] loss=0.8019 val_loss=0.9609 scale=2.0000 norm=1.4137                    \n",
      "[iter 46] loss=0.7849 val_loss=0.9591 scale=1.0000 norm=0.7007                    \n",
      "[iter 47] loss=0.7781 val_loss=0.9546 scale=1.0000 norm=0.6968                    \n",
      "[iter 48] loss=0.7626 val_loss=0.9502 scale=2.0000 norm=1.3653                    \n",
      "[iter 49] loss=0.7668 val_loss=0.9469 scale=1.0000 norm=0.6985                    \n",
      "[iter 50] loss=0.7616 val_loss=0.9447 scale=1.0000 norm=0.6989                    \n",
      "[iter 51] loss=0.7605 val_loss=0.9436 scale=1.0000 norm=0.6984                    \n",
      "[iter 52] loss=0.7494 val_loss=0.9397 scale=1.0000 norm=0.6847                    \n",
      "[iter 53] loss=0.7452 val_loss=0.9397 scale=2.0000 norm=1.3794                    \n",
      "[iter 54] loss=0.7288 val_loss=0.9389 scale=1.0000 norm=0.6818                    \n",
      "[iter 55] loss=0.7403 val_loss=0.9301 scale=1.0000 norm=0.6970                    \n",
      "[iter 56] loss=0.7338 val_loss=0.9253 scale=1.0000 norm=0.6947                    \n",
      "[iter 57] loss=0.7285 val_loss=0.9247 scale=1.0000 norm=0.6920                    \n",
      "[iter 58] loss=0.7174 val_loss=0.9213 scale=1.0000 norm=0.6896                    \n",
      "[iter 59] loss=0.7268 val_loss=0.9199 scale=1.0000 norm=0.6955                    \n",
      "[iter 60] loss=0.7200 val_loss=0.9181 scale=2.0000 norm=1.3798                    \n",
      "[iter 61] loss=0.7292 val_loss=0.9143 scale=1.0000 norm=0.7043                    \n",
      "[iter 62] loss=0.7111 val_loss=0.9115 scale=1.0000 norm=0.6855                    \n",
      "[iter 63] loss=0.7128 val_loss=0.9112 scale=1.0000 norm=0.6910                    \n",
      "[iter 64] loss=0.7052 val_loss=0.9098 scale=2.0000 norm=1.3746                    \n",
      "[iter 65] loss=0.7058 val_loss=0.9063 scale=1.0000 norm=0.6967                    \n",
      "[iter 66] loss=0.6863 val_loss=0.9067 scale=2.0000 norm=1.3690                    \n",
      "[iter 67] loss=0.6973 val_loss=0.9035 scale=1.0000 norm=0.6923                    \n",
      "[iter 68] loss=0.6817 val_loss=0.9059 scale=2.0000 norm=1.3626                    \n",
      "[iter 69] loss=0.6928 val_loss=0.9048 scale=1.0000 norm=0.6974                    \n",
      "[iter 70] loss=0.6842 val_loss=0.9016 scale=1.0000 norm=0.6887                    \n",
      "[iter 71] loss=0.6823 val_loss=0.8985 scale=1.0000 norm=0.6942                    \n",
      "[iter 72] loss=0.6711 val_loss=0.8967 scale=1.0000 norm=0.6841                    \n",
      "[iter 73] loss=0.6777 val_loss=0.8878 scale=1.0000 norm=0.6918                    \n",
      "[iter 74] loss=0.6645 val_loss=0.8884 scale=1.0000 norm=0.6845                    \n",
      "[iter 75] loss=0.6720 val_loss=0.8862 scale=1.0000 norm=0.6924                    \n",
      "[iter 76] loss=0.6737 val_loss=0.8847 scale=1.0000 norm=0.7016                    \n",
      "[iter 77] loss=0.6684 val_loss=0.8833 scale=1.0000 norm=0.6970                    \n",
      "[iter 78] loss=0.6719 val_loss=0.8823 scale=2.0000 norm=1.3964                    \n",
      "[iter 79] loss=0.6577 val_loss=0.8808 scale=1.0000 norm=0.6919                    \n",
      "[iter 80] loss=0.6414 val_loss=0.8751 scale=1.0000 norm=0.6751                    \n",
      "[iter 81] loss=0.6529 val_loss=0.8728 scale=2.0000 norm=1.3820                    \n",
      "[iter 82] loss=0.6390 val_loss=0.8728 scale=1.0000 norm=0.6831                    \n",
      "[iter 83] loss=0.6342 val_loss=0.8679 scale=1.0000 norm=0.6748                    \n",
      "[iter 84] loss=0.6238 val_loss=0.8642 scale=1.0000 norm=0.6765                    \n",
      "[iter 85] loss=0.6218 val_loss=0.8611 scale=1.0000 norm=0.6713                    \n",
      "[iter 86] loss=0.6332 val_loss=0.8613 scale=1.0000 norm=0.6858                    \n",
      "[iter 87] loss=0.6310 val_loss=0.8558 scale=1.0000 norm=0.6886                    \n",
      "[iter 88] loss=0.6129 val_loss=0.8558 scale=1.0000 norm=0.6802                    \n",
      "[iter 89] loss=0.6269 val_loss=0.8560 scale=2.0000 norm=1.3767                    \n",
      "[iter 90] loss=0.6223 val_loss=0.8552 scale=1.0000 norm=0.6846                    \n",
      "[iter 91] loss=0.6028 val_loss=0.8554 scale=1.0000 norm=0.6663                    \n",
      "[iter 92] loss=0.6190 val_loss=0.8534 scale=1.0000 norm=0.6833                    \n",
      "[iter 93] loss=0.6289 val_loss=0.8525 scale=1.0000 norm=0.7002                    \n",
      "[iter 94] loss=0.6109 val_loss=0.8517 scale=1.0000 norm=0.6855                    \n",
      "[iter 95] loss=0.6137 val_loss=0.8521 scale=1.0000 norm=0.6867                    \n",
      "[iter 96] loss=0.6326 val_loss=0.8516 scale=1.0000 norm=0.7031                    \n",
      "[iter 97] loss=0.6095 val_loss=0.8514 scale=1.0000 norm=0.6871                    \n",
      "[iter 98] loss=0.5965 val_loss=0.8502 scale=1.0000 norm=0.6792                    \n",
      "[iter 99] loss=0.6084 val_loss=0.8488 scale=1.0000 norm=0.6884                    \n",
      "[iter 100] loss=0.5941 val_loss=0.8478 scale=1.0000 norm=0.6781                   \n",
      "[iter 101] loss=0.5916 val_loss=0.8431 scale=1.0000 norm=0.6701                   \n",
      "[iter 102] loss=0.6194 val_loss=0.8410 scale=1.0000 norm=0.7007                   \n",
      "[iter 103] loss=0.5913 val_loss=0.8385 scale=1.0000 norm=0.6781                   \n",
      "[iter 104] loss=0.5836 val_loss=0.8365 scale=1.0000 norm=0.6761                   \n",
      "[iter 105] loss=0.5761 val_loss=0.8362 scale=1.0000 norm=0.6733                   \n",
      "[iter 106] loss=0.5686 val_loss=0.8377 scale=1.0000 norm=0.6706                   \n",
      "[iter 107] loss=0.5863 val_loss=0.8331 scale=1.0000 norm=0.6862                   \n",
      "[iter 108] loss=0.5854 val_loss=0.8335 scale=2.0000 norm=1.3682                   \n",
      "[iter 109] loss=0.5662 val_loss=0.8340 scale=1.0000 norm=0.6719                   \n",
      "[iter 110] loss=0.5750 val_loss=0.8323 scale=1.0000 norm=0.6833                   \n",
      "[iter 111] loss=0.5812 val_loss=0.8322 scale=1.0000 norm=0.6863                   \n",
      "[iter 112] loss=0.5806 val_loss=0.8308 scale=1.0000 norm=0.6818                   \n",
      "[iter 113] loss=0.5717 val_loss=0.8303 scale=1.0000 norm=0.6836                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 114] loss=0.5728 val_loss=0.8302 scale=1.0000 norm=0.6846                   \n",
      "[iter 115] loss=0.5772 val_loss=0.8297 scale=1.0000 norm=0.6897                   \n",
      "[iter 116] loss=0.5714 val_loss=0.8295 scale=1.0000 norm=0.6881                   \n",
      "[iter 117] loss=0.5650 val_loss=0.8304 scale=1.0000 norm=0.6811                   \n",
      "[iter 118] loss=0.5757 val_loss=0.8302 scale=2.0000 norm=1.3778                   \n",
      "[iter 119] loss=0.5634 val_loss=0.8290 scale=1.0000 norm=0.6829                   \n",
      "[iter 120] loss=0.5744 val_loss=0.8289 scale=1.0000 norm=0.6950                   \n",
      "[iter 121] loss=0.5582 val_loss=0.8299 scale=2.0000 norm=1.3682                   \n",
      "[iter 122] loss=0.5713 val_loss=0.8308 scale=1.0000 norm=0.6942                   \n",
      "[iter 123] loss=0.5645 val_loss=0.8299 scale=1.0000 norm=0.6880                   \n",
      "[iter 124] loss=0.5620 val_loss=0.8248 scale=1.0000 norm=0.6832                   \n",
      "[iter 125] loss=0.5460 val_loss=0.8255 scale=1.0000 norm=0.6812                   \n",
      "[iter 126] loss=0.5321 val_loss=0.8238 scale=1.0000 norm=0.6593                   \n",
      "[iter 127] loss=0.5745 val_loss=0.8189 scale=1.0000 norm=0.6960                   \n",
      "[iter 128] loss=0.5595 val_loss=0.8176 scale=1.0000 norm=0.6906                   \n",
      "[iter 129] loss=0.5329 val_loss=0.8157 scale=1.0000 norm=0.6721                   \n",
      "[iter 130] loss=0.5462 val_loss=0.8139 scale=1.0000 norm=0.6788                   \n",
      "[iter 131] loss=0.5431 val_loss=0.8134 scale=1.0000 norm=0.6806                   \n",
      "[iter 132] loss=0.5445 val_loss=0.8140 scale=1.0000 norm=0.6853                   \n",
      "[iter 133] loss=0.5492 val_loss=0.8103 scale=1.0000 norm=0.6812                   \n",
      "[iter 134] loss=0.5375 val_loss=0.8093 scale=1.0000 norm=0.6797                   \n",
      "[iter 135] loss=0.5422 val_loss=0.8056 scale=1.0000 norm=0.6852                   \n",
      "[iter 136] loss=0.5345 val_loss=0.8043 scale=1.0000 norm=0.6840                   \n",
      "[iter 137] loss=0.5316 val_loss=0.8039 scale=1.0000 norm=0.6778                   \n",
      "[iter 138] loss=0.5408 val_loss=0.8025 scale=1.0000 norm=0.6849                   \n",
      "[iter 139] loss=0.5481 val_loss=0.8023 scale=1.0000 norm=0.6870                   \n",
      "[iter 140] loss=0.5392 val_loss=0.8000 scale=1.0000 norm=0.6822                   \n",
      "[iter 141] loss=0.5206 val_loss=0.7776 scale=2.0000 norm=1.3444                   \n",
      "[iter 142] loss=0.5310 val_loss=0.7723 scale=1.0000 norm=0.6881                   \n",
      "[iter 143] loss=0.5235 val_loss=0.7723 scale=1.0000 norm=0.6751                   \n",
      "[iter 144] loss=0.5392 val_loss=0.7685 scale=1.0000 norm=0.6896                   \n",
      "[iter 145] loss=0.5161 val_loss=0.7624 scale=1.0000 norm=0.6738                   \n",
      "[iter 146] loss=0.5306 val_loss=0.7617 scale=1.0000 norm=0.6901                   \n",
      "[iter 147] loss=0.5042 val_loss=0.7620 scale=2.0000 norm=1.3304                   \n",
      "[iter 148] loss=0.5000 val_loss=0.7624 scale=1.0000 norm=0.6655                   \n",
      "[iter 149] loss=0.5086 val_loss=0.7608 scale=1.0000 norm=0.6706                   \n",
      "[iter 150] loss=0.5177 val_loss=0.7589 scale=1.0000 norm=0.6763                   \n",
      "[iter 151] loss=0.5148 val_loss=0.7494 scale=1.0000 norm=0.6860                   \n",
      "[iter 152] loss=0.5143 val_loss=0.7490 scale=1.0000 norm=0.6851                   \n",
      "[iter 153] loss=0.5092 val_loss=0.7498 scale=1.0000 norm=0.6765                   \n",
      "[iter 154] loss=0.5068 val_loss=0.7470 scale=1.0000 norm=0.6787                   \n",
      "[iter 155] loss=0.5076 val_loss=0.7482 scale=1.0000 norm=0.6858                   \n",
      "[iter 156] loss=0.5144 val_loss=0.7483 scale=1.0000 norm=0.6848                   \n",
      "[iter 157] loss=0.5140 val_loss=0.7479 scale=1.0000 norm=0.6857                   \n",
      "[iter 158] loss=0.4986 val_loss=0.7477 scale=1.0000 norm=0.6778                   \n",
      "[iter 159] loss=0.5003 val_loss=0.7463 scale=1.0000 norm=0.6770                   \n",
      "[iter 160] loss=0.5121 val_loss=0.7467 scale=1.0000 norm=0.6900                   \n",
      "[iter 161] loss=0.5067 val_loss=0.7457 scale=1.0000 norm=0.6824                   \n",
      "[iter 162] loss=0.5006 val_loss=0.7453 scale=1.0000 norm=0.6784                   \n",
      "[iter 163] loss=0.4980 val_loss=0.7455 scale=1.0000 norm=0.6772                   \n",
      "[iter 164] loss=0.5094 val_loss=0.7459 scale=1.0000 norm=0.6848                   \n",
      "[iter 165] loss=0.4984 val_loss=0.7406 scale=1.0000 norm=0.6765                   \n",
      "[iter 166] loss=0.5132 val_loss=0.7410 scale=1.0000 norm=0.6906                   \n",
      "[iter 167] loss=0.5003 val_loss=0.7396 scale=1.0000 norm=0.6842                   \n",
      "[iter 168] loss=0.4868 val_loss=0.7391 scale=1.0000 norm=0.6685                   \n",
      "[iter 169] loss=0.4871 val_loss=0.7387 scale=1.0000 norm=0.6746                   \n",
      "[iter 170] loss=0.4953 val_loss=0.7386 scale=1.0000 norm=0.6751                   \n",
      "[iter 171] loss=0.4805 val_loss=0.7385 scale=1.0000 norm=0.6692                   \n",
      "[iter 172] loss=0.5029 val_loss=0.7394 scale=1.0000 norm=0.6859                   \n",
      "[iter 173] loss=0.4852 val_loss=0.7380 scale=1.0000 norm=0.6814                   \n",
      "[iter 174] loss=0.4818 val_loss=0.7389 scale=1.0000 norm=0.6765                   \n",
      "[iter 175] loss=0.5020 val_loss=0.7392 scale=1.0000 norm=0.6913                   \n",
      "[iter 176] loss=0.4763 val_loss=0.7368 scale=1.0000 norm=0.6756                   \n",
      "[iter 177] loss=0.4862 val_loss=0.7373 scale=1.0000 norm=0.6783                   \n",
      "[iter 178] loss=0.4838 val_loss=0.7332 scale=1.0000 norm=0.6774                   \n",
      "[iter 179] loss=0.4842 val_loss=0.7324 scale=1.0000 norm=0.6772                   \n",
      "[iter 180] loss=0.4534 val_loss=0.7334 scale=1.0000 norm=0.6566                   \n",
      "[iter 181] loss=0.4850 val_loss=0.7339 scale=1.0000 norm=0.6815                   \n",
      "[iter 182] loss=0.4907 val_loss=0.7339 scale=1.0000 norm=0.6855                   \n",
      "[iter 183] loss=0.4779 val_loss=0.7344 scale=1.0000 norm=0.6785                   \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL179 (val_loss=0.7324)                                      \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.18171174839971127, 'minibatch_frac': 0.8, 'n_estimators': 2000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5200 val_loss=1.2039 scale=1.0000 norm=1.0640                     \n",
      "[iter 1] loss=1.1753 val_loss=1.0546 scale=2.0000 norm=1.5072                     \n",
      "[iter 2] loss=1.0096 val_loss=0.9424 scale=2.0000 norm=1.3241                     \n",
      "[iter 3] loss=0.8600 val_loss=0.8389 scale=2.0000 norm=1.2301                     \n",
      "[iter 4] loss=0.7505 val_loss=0.7631 scale=2.0000 norm=1.1873                     \n",
      "[iter 5] loss=0.6375 val_loss=0.7004 scale=2.0000 norm=1.1564                     \n",
      "[iter 6] loss=0.5270 val_loss=0.6579 scale=2.0000 norm=1.1173                     \n",
      "[iter 7] loss=0.4507 val_loss=0.6420 scale=2.0000 norm=1.1031                     \n",
      "[iter 8] loss=0.3855 val_loss=0.6435 scale=2.0000 norm=1.1059                     \n",
      "[iter 9] loss=0.3257 val_loss=0.6420 scale=2.0000 norm=1.1068                     \n",
      "[iter 10] loss=0.2712 val_loss=0.6604 scale=2.0000 norm=1.0913                    \n",
      "[iter 11] loss=0.2425 val_loss=0.7106 scale=2.0000 norm=1.1080                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL7 (val_loss=0.6420)                                        \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=5,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.20128717058643453, 'minibatch_frac': 0.6, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 5}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=1.5178 val_loss=1.2924 scale=1.0000 norm=1.0617                     \n",
      "[iter 1] loss=1.2690 val_loss=1.0971 scale=2.0000 norm=1.6299                     \n",
      "[iter 2] loss=1.0596 val_loss=0.9576 scale=2.0000 norm=1.3560                     \n",
      "[iter 3] loss=0.9160 val_loss=0.8524 scale=2.0000 norm=1.2529                     \n",
      "[iter 4] loss=0.7978 val_loss=0.7636 scale=2.0000 norm=1.2246                     \n",
      "[iter 5] loss=0.6865 val_loss=0.6851 scale=2.0000 norm=1.1847                     \n",
      "[iter 6] loss=0.5781 val_loss=0.6417 scale=2.0000 norm=1.1784                     \n",
      "[iter 7] loss=0.5520 val_loss=0.5986 scale=2.0000 norm=1.1941                     \n",
      "[iter 8] loss=0.4880 val_loss=0.5832 scale=1.0000 norm=0.6025                     \n",
      "[iter 9] loss=0.4458 val_loss=0.5810 scale=2.0000 norm=1.1963                     \n",
      "[iter 10] loss=0.4160 val_loss=0.5736 scale=1.0000 norm=0.5955                    \n",
      "[iter 11] loss=0.4032 val_loss=0.5795 scale=1.0000 norm=0.5996                    \n",
      "[iter 12] loss=0.3733 val_loss=0.5819 scale=1.0000 norm=0.5852                    \n",
      "[iter 13] loss=0.3366 val_loss=0.5830 scale=1.0000 norm=0.5704                    \n",
      "[iter 14] loss=0.3443 val_loss=0.5529 scale=1.0000 norm=0.5956                    \n",
      "[iter 15] loss=0.3538 val_loss=0.5620 scale=1.0000 norm=0.5994                    \n",
      "[iter 16] loss=0.3257 val_loss=0.5830 scale=2.0000 norm=1.1911                    \n",
      "[iter 17] loss=0.3256 val_loss=0.5950 scale=1.0000 norm=0.5976                    \n",
      "[iter 18] loss=0.3182 val_loss=0.6076 scale=1.0000 norm=0.5933                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL14 (val_loss=0.5529)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=5,\n",
      "                      max_features='log2', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.3300379337455973, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.2590 scale=1.0000 norm=1.0277                     \n",
      "[iter 1] loss=1.1809 val_loss=1.0691 scale=2.0000 norm=1.4999                     \n",
      "[iter 2] loss=0.9891 val_loss=0.8757 scale=2.0000 norm=1.3646                     \n",
      "[iter 3] loss=0.8164 val_loss=0.7437 scale=2.0000 norm=1.2864                     \n",
      "[iter 4] loss=0.6848 val_loss=0.6958 scale=1.0000 norm=0.6304                     \n",
      "[iter 5] loss=0.6214 val_loss=0.6463 scale=2.0000 norm=1.2368                     \n",
      "[iter 6] loss=0.5327 val_loss=0.6389 scale=1.0000 norm=0.6173                     \n",
      "[iter 7] loss=0.5026 val_loss=0.6335 scale=1.0000 norm=0.6176                     \n",
      "[iter 8] loss=0.4741 val_loss=0.6220 scale=1.0000 norm=0.6160                     \n",
      "[iter 9] loss=0.4516 val_loss=0.6256 scale=1.0000 norm=0.6188                     \n",
      "[iter 10] loss=0.4291 val_loss=0.6357 scale=1.0000 norm=0.6155                    \n",
      "[iter 11] loss=0.4128 val_loss=0.6589 scale=2.0000 norm=1.2324                    \n",
      "[iter 12] loss=0.3884 val_loss=0.6650 scale=1.0000 norm=0.6215                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL8 (val_loss=0.6220)                                        \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=3,\n",
      "                      max_features='log2', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.024683459274774208, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.5577 scale=1.0000 norm=1.0277                     \n",
      "[iter 1] loss=1.3879 val_loss=1.5267 scale=1.0000 norm=0.9398                     \n",
      "[iter 2] loss=1.3523 val_loss=1.5005 scale=1.0000 norm=0.9090                     \n",
      "[iter 3] loss=1.3346 val_loss=1.4793 scale=1.0000 norm=0.8954                     \n",
      "[iter 4] loss=1.3208 val_loss=1.4603 scale=1.0000 norm=0.8852                     \n",
      "[iter 5] loss=1.3088 val_loss=1.4457 scale=1.0000 norm=0.8766                     \n",
      "[iter 6] loss=1.2957 val_loss=1.4303 scale=1.0000 norm=0.8670                     \n",
      "[iter 7] loss=1.2795 val_loss=1.4181 scale=1.0000 norm=0.8552                     \n",
      "[iter 8] loss=1.2706 val_loss=1.4060 scale=1.0000 norm=0.8496                     \n",
      "[iter 9] loss=1.2611 val_loss=1.3955 scale=1.0000 norm=0.8437                     \n",
      "[iter 10] loss=1.2533 val_loss=1.3851 scale=1.0000 norm=0.8391                    \n",
      "[iter 11] loss=1.2458 val_loss=1.3792 scale=1.0000 norm=0.8349                    \n",
      "[iter 12] loss=1.2390 val_loss=1.3718 scale=1.0000 norm=0.8314                    \n",
      "[iter 13] loss=1.2330 val_loss=1.3639 scale=1.0000 norm=0.8286                    \n",
      "[iter 14] loss=1.2254 val_loss=1.3575 scale=1.0000 norm=0.8245                    \n",
      "[iter 15] loss=1.2198 val_loss=1.3507 scale=1.0000 norm=0.8221                    \n",
      "[iter 16] loss=1.2128 val_loss=1.3452 scale=1.0000 norm=0.8185                    \n",
      "[iter 17] loss=1.2068 val_loss=1.3367 scale=2.0000 norm=1.6316                    \n",
      "[iter 18] loss=1.1984 val_loss=1.3333 scale=1.0000 norm=0.8139                    \n",
      "[iter 19] loss=1.1938 val_loss=1.3265 scale=1.0000 norm=0.8123                    \n",
      "[iter 20] loss=1.1882 val_loss=1.3066 scale=1.0000 norm=0.8099                    \n",
      "[iter 21] loss=1.1752 val_loss=1.3010 scale=2.0000 norm=1.6016                    \n",
      "[iter 22] loss=1.1653 val_loss=1.2949 scale=1.0000 norm=0.7979                    \n",
      "[iter 23] loss=1.1553 val_loss=1.2840 scale=2.0000 norm=1.5851                    \n",
      "[iter 24] loss=1.1404 val_loss=1.2798 scale=1.0000 norm=0.7854                    \n",
      "[iter 25] loss=1.1345 val_loss=1.2699 scale=2.0000 norm=1.5661                    \n",
      "[iter 26] loss=1.1245 val_loss=1.2632 scale=1.0000 norm=0.7800                    \n",
      "[iter 27] loss=1.1187 val_loss=1.2574 scale=1.0000 norm=0.7777                    \n",
      "[iter 28] loss=1.1131 val_loss=1.2453 scale=2.0000 norm=1.5516                    \n",
      "[iter 29] loss=1.1016 val_loss=1.2381 scale=2.0000 norm=1.5421                    \n",
      "[iter 30] loss=1.0944 val_loss=1.2272 scale=2.0000 norm=1.5413                    \n",
      "[iter 31] loss=1.0832 val_loss=1.2204 scale=2.0000 norm=1.5332                    \n",
      "[iter 32] loss=1.0757 val_loss=1.2146 scale=1.0000 norm=0.7654                    \n",
      "[iter 33] loss=1.0690 val_loss=1.2073 scale=2.0000 norm=1.5259                    \n",
      "[iter 34] loss=1.0601 val_loss=1.2001 scale=2.0000 norm=1.5210                    \n",
      "[iter 35] loss=1.0521 val_loss=1.1937 scale=2.0000 norm=1.5179                    \n",
      "[iter 36] loss=1.0445 val_loss=1.1830 scale=2.0000 norm=1.5154                    \n",
      "[iter 37] loss=1.0347 val_loss=1.1787 scale=2.0000 norm=1.5094                    \n",
      "[iter 38] loss=1.0278 val_loss=1.1741 scale=2.0000 norm=1.5078                    \n",
      "[iter 39] loss=1.0213 val_loss=1.1692 scale=1.0000 norm=0.7533                    \n",
      "[iter 40] loss=1.0159 val_loss=1.1643 scale=2.0000 norm=1.5035                    \n",
      "[iter 41] loss=1.0094 val_loss=1.1595 scale=1.0000 norm=0.7511                    \n",
      "[iter 42] loss=1.0031 val_loss=1.1557 scale=2.0000 norm=1.4974                    \n",
      "[iter 43] loss=0.9970 val_loss=1.1511 scale=2.0000 norm=1.4968                    \n",
      "[iter 44] loss=0.9910 val_loss=1.1462 scale=2.0000 norm=1.4963                    \n",
      "[iter 45] loss=0.9850 val_loss=1.1311 scale=2.0000 norm=1.4960                    \n",
      "[iter 46] loss=0.9760 val_loss=1.1203 scale=2.0000 norm=1.4885                    \n",
      "[iter 47] loss=0.9631 val_loss=1.1150 scale=1.0000 norm=0.7385                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 48] loss=0.9580 val_loss=1.1067 scale=2.0000 norm=1.4730                    \n",
      "[iter 49] loss=0.9479 val_loss=1.0984 scale=1.0000 norm=0.7333                    \n",
      "[iter 50] loss=0.9413 val_loss=1.0934 scale=2.0000 norm=1.4600                    \n",
      "[iter 51] loss=0.9324 val_loss=1.0896 scale=2.0000 norm=1.4554                    \n",
      "[iter 52] loss=0.9262 val_loss=1.0825 scale=2.0000 norm=1.4550                    \n",
      "[iter 53] loss=0.9181 val_loss=1.0763 scale=1.0000 norm=0.7256                    \n",
      "[iter 54] loss=0.9124 val_loss=1.0715 scale=2.0000 norm=1.4463                    \n",
      "[iter 55] loss=0.9051 val_loss=1.0627 scale=1.0000 norm=0.7217                    \n",
      "[iter 56] loss=0.8983 val_loss=1.0580 scale=2.0000 norm=1.4375                    \n",
      "[iter 57] loss=0.8912 val_loss=1.0539 scale=2.0000 norm=1.4351                    \n",
      "[iter 58] loss=0.8847 val_loss=1.0490 scale=1.0000 norm=0.7167                    \n",
      "[iter 59] loss=0.8799 val_loss=1.0442 scale=1.0000 norm=0.7149                    \n",
      "[iter 60] loss=0.8750 val_loss=1.0422 scale=2.0000 norm=1.4265                    \n",
      "[iter 61] loss=0.8685 val_loss=1.0410 scale=2.0000 norm=1.4248                    \n",
      "[iter 62] loss=0.8623 val_loss=1.0393 scale=1.0000 norm=0.7117                    \n",
      "[iter 63] loss=0.8589 val_loss=1.0315 scale=2.0000 norm=1.4218                    \n",
      "[iter 64] loss=0.8526 val_loss=1.0239 scale=1.0000 norm=0.7098                    \n",
      "[iter 65] loss=0.8464 val_loss=1.0213 scale=1.0000 norm=0.7074                    \n",
      "[iter 66] loss=0.8420 val_loss=1.0179 scale=2.0000 norm=1.4121                    \n",
      "[iter 67] loss=0.8368 val_loss=1.0120 scale=2.0000 norm=1.4121                    \n",
      "[iter 68] loss=0.8308 val_loss=1.0097 scale=2.0000 norm=1.4105                    \n",
      "[iter 69] loss=0.8256 val_loss=1.0079 scale=1.0000 norm=0.7050                    \n",
      "[iter 70] loss=0.8225 val_loss=1.0009 scale=1.0000 norm=0.7047                    \n",
      "[iter 71] loss=0.8172 val_loss=0.9992 scale=1.0000 norm=0.7027                    \n",
      "[iter 72] loss=0.8143 val_loss=0.9924 scale=1.0000 norm=0.7024                    \n",
      "[iter 73] loss=0.8089 val_loss=0.9920 scale=2.0000 norm=1.4011                    \n",
      "[iter 74] loss=0.8039 val_loss=0.9903 scale=1.0000 norm=0.7002                    \n",
      "[iter 75] loss=0.8011 val_loss=0.9892 scale=2.0000 norm=1.4000                    \n",
      "[iter 76] loss=0.7970 val_loss=0.9877 scale=1.0000 norm=0.7002                    \n",
      "[iter 77] loss=0.7943 val_loss=0.9857 scale=2.0000 norm=1.3999                    \n",
      "[iter 78] loss=0.7901 val_loss=0.9841 scale=1.0000 norm=0.7001                    \n",
      "[iter 79] loss=0.7876 val_loss=0.9831 scale=1.0000 norm=0.7000                    \n",
      "[iter 80] loss=0.7855 val_loss=0.9822 scale=1.0000 norm=0.7000                    \n",
      "[iter 81] loss=0.7834 val_loss=0.9816 scale=1.0000 norm=0.7001                    \n",
      "[iter 82] loss=0.7813 val_loss=0.9803 scale=1.0000 norm=0.7001                    \n",
      "[iter 83] loss=0.7790 val_loss=0.9790 scale=2.0000 norm=1.4000                    \n",
      "[iter 84] loss=0.7754 val_loss=0.9785 scale=2.0000 norm=1.4007                    \n",
      "[iter 85] loss=0.7718 val_loss=0.9777 scale=1.0000 norm=0.7006                    \n",
      "[iter 86] loss=0.7696 val_loss=0.9768 scale=1.0000 norm=0.7006                    \n",
      "[iter 87] loss=0.7679 val_loss=0.9741 scale=1.0000 norm=0.7007                    \n",
      "[iter 88] loss=0.7651 val_loss=0.9720 scale=2.0000 norm=1.4003                    \n",
      "[iter 89] loss=0.7610 val_loss=0.9711 scale=2.0000 norm=1.3995                    \n",
      "[iter 90] loss=0.7581 val_loss=0.9707 scale=1.0000 norm=0.7001                    \n",
      "[iter 91] loss=0.7559 val_loss=0.9705 scale=2.0000 norm=1.4001                    \n",
      "[iter 92] loss=0.7528 val_loss=0.9687 scale=1.0000 norm=0.7005                    \n",
      "[iter 93] loss=0.7508 val_loss=0.9612 scale=1.0000 norm=0.7004                    \n",
      "[iter 94] loss=0.7489 val_loss=0.9578 scale=1.0000 norm=0.7005                    \n",
      "[iter 95] loss=0.7452 val_loss=0.9563 scale=1.0000 norm=0.6996                    \n",
      "[iter 96] loss=0.7435 val_loss=0.9548 scale=2.0000 norm=1.3995                    \n",
      "[iter 97] loss=0.7410 val_loss=0.9541 scale=1.0000 norm=0.7003                    \n",
      "[iter 98] loss=0.7389 val_loss=0.9530 scale=1.0000 norm=0.7002                    \n",
      "[iter 99] loss=0.7366 val_loss=0.9447 scale=1.0000 norm=0.6999                    \n",
      "[iter 100] loss=0.7318 val_loss=0.9403 scale=1.0000 norm=0.6983                   \n",
      "[iter 101] loss=0.7279 val_loss=0.9399 scale=1.0000 norm=0.6971                   \n",
      "[iter 102] loss=0.7262 val_loss=0.9379 scale=1.0000 norm=0.6971                   \n",
      "[iter 103] loss=0.7244 val_loss=0.9379 scale=1.0000 norm=0.6970                   \n",
      "[iter 104] loss=0.7228 val_loss=0.9334 scale=1.0000 norm=0.6970                   \n",
      "[iter 105] loss=0.7190 val_loss=0.9324 scale=1.0000 norm=0.6958                   \n",
      "[iter 106] loss=0.7171 val_loss=0.9259 scale=1.0000 norm=0.6957                   \n",
      "[iter 107] loss=0.7131 val_loss=0.9260 scale=2.0000 norm=1.3890                   \n",
      "[iter 108] loss=0.7110 val_loss=0.9242 scale=1.0000 norm=0.6952                   \n",
      "[iter 109] loss=0.7093 val_loss=0.9253 scale=2.0000 norm=1.3901                   \n",
      "[iter 110] loss=0.7059 val_loss=0.9245 scale=1.0000 norm=0.6947                   \n",
      "[iter 111] loss=0.7043 val_loss=0.9199 scale=1.0000 norm=0.6947                   \n",
      "[iter 112] loss=0.7006 val_loss=0.9179 scale=1.0000 norm=0.6936                   \n",
      "[iter 113] loss=0.6976 val_loss=0.9145 scale=1.0000 norm=0.6929                   \n",
      "[iter 114] loss=0.6944 val_loss=0.9144 scale=2.0000 norm=1.3844                   \n",
      "[iter 115] loss=0.6921 val_loss=0.9149 scale=1.0000 norm=0.6926                   \n",
      "[iter 116] loss=0.6903 val_loss=0.9142 scale=2.0000 norm=1.3848                   \n",
      "[iter 117] loss=0.6878 val_loss=0.9146 scale=1.0000 norm=0.6926                   \n",
      "[iter 118] loss=0.6859 val_loss=0.9106 scale=1.0000 norm=0.6924                   \n",
      "[iter 119] loss=0.6827 val_loss=0.9074 scale=1.0000 norm=0.6916                   \n",
      "[iter 120] loss=0.6794 val_loss=0.9079 scale=1.0000 norm=0.6908                   \n",
      "[iter 121] loss=0.6783 val_loss=0.9064 scale=1.0000 norm=0.6910                   \n",
      "[iter 122] loss=0.6750 val_loss=0.9033 scale=1.0000 norm=0.6901                   \n",
      "[iter 123] loss=0.6719 val_loss=0.9030 scale=1.0000 norm=0.6893                   \n",
      "[iter 124] loss=0.6704 val_loss=0.9033 scale=1.0000 norm=0.6895                   \n",
      "[iter 125] loss=0.6690 val_loss=0.8844 scale=2.0000 norm=1.3792                   \n",
      "[iter 126] loss=0.6667 val_loss=0.8776 scale=1.0000 norm=0.6901                   \n",
      "[iter 127] loss=0.6655 val_loss=0.8790 scale=2.0000 norm=1.3807                   \n",
      "[iter 128] loss=0.6640 val_loss=0.8789 scale=1.0000 norm=0.6912                   \n",
      "[iter 129] loss=0.6626 val_loss=0.8786 scale=1.0000 norm=0.6911                   \n",
      "[iter 130] loss=0.6614 val_loss=0.8790 scale=2.0000 norm=1.3824                   \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL126 (val_loss=0.8776)                                      \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.75, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.242192341706159, 'minibatch_frac': 0.6, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 10}}\n",
      "[iter 0] loss=1.5178 val_loss=1.2489 scale=1.0000 norm=1.0617                     \n",
      "[iter 1] loss=1.2006 val_loss=1.0739 scale=2.0000 norm=1.5108                     \n",
      "[iter 2] loss=1.0393 val_loss=0.9386 scale=2.0000 norm=1.3091                     \n",
      "[iter 3] loss=0.8344 val_loss=0.8437 scale=2.0000 norm=1.2049                     \n",
      "[iter 4] loss=0.6938 val_loss=0.7760 scale=2.0000 norm=1.1836                     \n",
      "[iter 5] loss=0.5821 val_loss=0.7339 scale=2.0000 norm=1.1511                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 6] loss=0.4771 val_loss=0.6950 scale=2.0000 norm=1.1359                     \n",
      "[iter 7] loss=0.4616 val_loss=0.6839 scale=1.0000 norm=0.5831                     \n",
      "[iter 8] loss=0.3921 val_loss=0.7055 scale=2.0000 norm=1.1592                     \n",
      "[iter 9] loss=0.3547 val_loss=0.7177 scale=1.0000 norm=0.5959                     \n",
      "[iter 10] loss=0.3410 val_loss=0.7246 scale=1.0000 norm=0.5808                    \n",
      "[iter 11] loss=0.3360 val_loss=0.7417 scale=1.0000 norm=0.5876                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL7 (val_loss=0.6839)                                        \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=3,\n",
      "                      max_features=0.75, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.14407022838043573, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.3352 scale=1.0000 norm=1.0277                     \n",
      "[iter 1] loss=1.2320 val_loss=1.1353 scale=2.0000 norm=1.6224                     \n",
      "[iter 2] loss=1.0750 val_loss=1.0383 scale=2.0000 norm=1.4208                     \n",
      "[iter 3] loss=0.9743 val_loss=0.9536 scale=2.0000 norm=1.3477                     \n",
      "[iter 4] loss=0.8809 val_loss=0.8716 scale=2.0000 norm=1.3000                     \n",
      "[iter 5] loss=0.7985 val_loss=0.7987 scale=2.0000 norm=1.2752                     \n",
      "[iter 6] loss=0.7236 val_loss=0.7380 scale=2.0000 norm=1.2557                     \n",
      "[iter 7] loss=0.6591 val_loss=0.7061 scale=2.0000 norm=1.2473                     \n",
      "[iter 8] loss=0.6027 val_loss=0.6787 scale=2.0000 norm=1.2421                     \n",
      "[iter 9] loss=0.5564 val_loss=0.6555 scale=2.0000 norm=1.2436                     \n",
      "[iter 10] loss=0.5157 val_loss=0.6437 scale=2.0000 norm=1.2475                    \n",
      "[iter 11] loss=0.4835 val_loss=0.6185 scale=2.0000 norm=1.2555                    \n",
      "[iter 12] loss=0.4580 val_loss=0.6099 scale=1.0000 norm=0.6319                    \n",
      "[iter 13] loss=0.4461 val_loss=0.5990 scale=2.0000 norm=1.2668                    \n",
      "[iter 14] loss=0.4243 val_loss=0.5923 scale=1.0000 norm=0.6365                    \n",
      "[iter 15] loss=0.4153 val_loss=0.5887 scale=1.0000 norm=0.6381                    \n",
      "[iter 16] loss=0.4071 val_loss=0.5839 scale=1.0000 norm=0.6396                    \n",
      "[iter 17] loss=0.3980 val_loss=0.5768 scale=1.0000 norm=0.6394                    \n",
      "[iter 18] loss=0.3899 val_loss=0.5713 scale=1.0000 norm=0.6402                    \n",
      "[iter 19] loss=0.3844 val_loss=0.5703 scale=2.0000 norm=1.2852                    \n",
      "[iter 20] loss=0.3729 val_loss=0.5700 scale=1.0000 norm=0.6454                    \n",
      "[iter 21] loss=0.3672 val_loss=0.5700 scale=1.0000 norm=0.6453                    \n",
      "[iter 22] loss=0.3629 val_loss=0.5687 scale=1.0000 norm=0.6469                    \n",
      "[iter 23] loss=0.3594 val_loss=0.5613 scale=1.0000 norm=0.6483                    \n",
      "[iter 24] loss=0.3546 val_loss=0.5620 scale=1.0000 norm=0.6487                    \n",
      "[iter 25] loss=0.3519 val_loss=0.5592 scale=2.0000 norm=1.3004                    \n",
      "[iter 26] loss=0.3463 val_loss=0.5650 scale=1.0000 norm=0.6524                    \n",
      "[iter 27] loss=0.3416 val_loss=0.5683 scale=1.0000 norm=0.6509                    \n",
      "[iter 28] loss=0.3362 val_loss=0.5646 scale=1.0000 norm=0.6487                    \n",
      "[iter 29] loss=0.3284 val_loss=0.5644 scale=1.0000 norm=0.6448                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL25 (val_loss=0.5592)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=3,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.11577207723409703, 'minibatch_frac': 0.7, 'n_estimators': 1000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 5}}\n",
      "[iter 0] loss=1.5211 val_loss=1.3723 scale=1.0000 norm=1.0651                     \n",
      "[iter 1] loss=1.3147 val_loss=1.2151 scale=2.0000 norm=1.7411                     \n",
      "[iter 2] loss=1.1648 val_loss=1.1007 scale=2.0000 norm=1.5280                     \n",
      "[iter 3] loss=1.0508 val_loss=1.0286 scale=2.0000 norm=1.3930                     \n",
      "[iter 4] loss=0.9882 val_loss=0.9605 scale=2.0000 norm=1.3643                     \n",
      "[iter 5] loss=0.9015 val_loss=0.9029 scale=2.0000 norm=1.3155                     \n",
      "[iter 6] loss=0.8450 val_loss=0.8458 scale=2.0000 norm=1.3008                     \n",
      "[iter 7] loss=0.7806 val_loss=0.7961 scale=2.0000 norm=1.2678                     \n",
      "[iter 8] loss=0.7343 val_loss=0.7569 scale=2.0000 norm=1.2794                     \n",
      "[iter 9] loss=0.6804 val_loss=0.7187 scale=2.0000 norm=1.2592                     \n",
      "[iter 10] loss=0.6191 val_loss=0.6916 scale=2.0000 norm=1.2352                    \n",
      "[iter 11] loss=0.5958 val_loss=0.6747 scale=2.0000 norm=1.2774                    \n",
      "[iter 12] loss=0.5480 val_loss=0.6598 scale=2.0000 norm=1.2353                    \n",
      "[iter 13] loss=0.5138 val_loss=0.6498 scale=2.0000 norm=1.2385                    \n",
      "[iter 14] loss=0.4912 val_loss=0.6415 scale=2.0000 norm=1.2528                    \n",
      "[iter 15] loss=0.4689 val_loss=0.6315 scale=1.0000 norm=0.6279                    \n",
      "[iter 16] loss=0.4582 val_loss=0.6277 scale=2.0000 norm=1.2724                    \n",
      "[iter 17] loss=0.4553 val_loss=0.6197 scale=1.0000 norm=0.6387                    \n",
      "[iter 18] loss=0.4427 val_loss=0.6180 scale=1.0000 norm=0.6359                    \n",
      "[iter 19] loss=0.4359 val_loss=0.6036 scale=1.0000 norm=0.6394                    \n",
      "[iter 20] loss=0.4229 val_loss=0.6056 scale=1.0000 norm=0.6360                    \n",
      "[iter 21] loss=0.4304 val_loss=0.6046 scale=1.0000 norm=0.6476                    \n",
      "[iter 22] loss=0.4062 val_loss=0.6051 scale=1.0000 norm=0.6390                    \n",
      "[iter 23] loss=0.3814 val_loss=0.6019 scale=1.0000 norm=0.6245                    \n",
      "[iter 24] loss=0.3931 val_loss=0.6068 scale=1.0000 norm=0.6395                    \n",
      "[iter 25] loss=0.3907 val_loss=0.6005 scale=1.0000 norm=0.6397                    \n",
      "[iter 26] loss=0.3997 val_loss=0.6018 scale=1.0000 norm=0.6516                    \n",
      "[iter 27] loss=0.3809 val_loss=0.5995 scale=1.0000 norm=0.6376                    \n",
      "[iter 28] loss=0.3748 val_loss=0.6014 scale=1.0000 norm=0.6389                    \n",
      "[iter 29] loss=0.3650 val_loss=0.6071 scale=1.0000 norm=0.6336                    \n",
      "[iter 30] loss=0.3815 val_loss=0.6104 scale=1.0000 norm=0.6523                    \n",
      "[iter 31] loss=0.3714 val_loss=0.6072 scale=1.0000 norm=0.6487                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL27 (val_loss=0.5995)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=5,\n",
      "                      max_features=0.75, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.054398052092466724, 'minibatch_frac': 0.6, 'n_estimators': 2000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5178 val_loss=1.4253 scale=1.0000 norm=1.0617                     \n",
      "[iter 1] loss=1.2795 val_loss=1.2795 scale=2.0000 norm=1.6953                     \n",
      "[iter 2] loss=1.1861 val_loss=1.2075 scale=2.0000 norm=1.5597                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 3] loss=1.1094 val_loss=1.1380 scale=2.0000 norm=1.4532                     \n",
      "[iter 4] loss=1.0658 val_loss=1.0915 scale=2.0000 norm=1.4199                     \n",
      "[iter 5] loss=1.0120 val_loss=1.0517 scale=2.0000 norm=1.3614                     \n",
      "[iter 6] loss=0.9665 val_loss=1.0117 scale=2.0000 norm=1.3090                     \n",
      "[iter 7] loss=0.9352 val_loss=0.9749 scale=2.0000 norm=1.3042                     \n",
      "[iter 8] loss=0.8936 val_loss=0.9408 scale=2.0000 norm=1.2919                     \n",
      "[iter 9] loss=0.8535 val_loss=0.9105 scale=2.0000 norm=1.2597                     \n",
      "[iter 10] loss=0.8159 val_loss=0.8743 scale=2.0000 norm=1.2401                    \n",
      "[iter 11] loss=0.7820 val_loss=0.8437 scale=2.0000 norm=1.2447                    \n",
      "[iter 12] loss=0.7410 val_loss=0.8147 scale=2.0000 norm=1.2057                    \n",
      "[iter 13] loss=0.6991 val_loss=0.7903 scale=2.0000 norm=1.1827                    \n",
      "[iter 14] loss=0.6750 val_loss=0.7645 scale=2.0000 norm=1.1930                    \n",
      "[iter 15] loss=0.6442 val_loss=0.7397 scale=2.0000 norm=1.1809                    \n",
      "[iter 16] loss=0.6153 val_loss=0.7179 scale=2.0000 norm=1.1747                    \n",
      "[iter 17] loss=0.5950 val_loss=0.6939 scale=2.0000 norm=1.1729                    \n",
      "[iter 18] loss=0.5641 val_loss=0.6738 scale=2.0000 norm=1.1635                    \n",
      "[iter 19] loss=0.5357 val_loss=0.6565 scale=2.0000 norm=1.1533                    \n",
      "[iter 20] loss=0.5061 val_loss=0.6432 scale=2.0000 norm=1.1450                    \n",
      "[iter 21] loss=0.4956 val_loss=0.6197 scale=2.0000 norm=1.1644                    \n",
      "[iter 22] loss=0.4632 val_loss=0.6116 scale=2.0000 norm=1.1556                    \n",
      "[iter 23] loss=0.4295 val_loss=0.6030 scale=2.0000 norm=1.1199                    \n",
      "[iter 24] loss=0.4307 val_loss=0.5947 scale=2.0000 norm=1.1445                    \n",
      "[iter 25] loss=0.4071 val_loss=0.5772 scale=2.0000 norm=1.1384                    \n",
      "[iter 26] loss=0.4075 val_loss=0.5746 scale=1.0000 norm=0.5793                    \n",
      "[iter 27] loss=0.3764 val_loss=0.5644 scale=2.0000 norm=1.1183                    \n",
      "[iter 28] loss=0.3682 val_loss=0.5584 scale=2.0000 norm=1.1343                    \n",
      "[iter 29] loss=0.3540 val_loss=0.5564 scale=2.0000 norm=1.1361                    \n",
      "[iter 30] loss=0.3490 val_loss=0.5514 scale=2.0000 norm=1.1414                    \n",
      "[iter 31] loss=0.3398 val_loss=0.5484 scale=2.0000 norm=1.1493                    \n",
      "[iter 32] loss=0.3118 val_loss=0.5424 scale=2.0000 norm=1.1266                    \n",
      "[iter 33] loss=0.3075 val_loss=0.5394 scale=1.0000 norm=0.5696                    \n",
      "[iter 34] loss=0.2967 val_loss=0.5419 scale=2.0000 norm=1.1356                    \n",
      "[iter 35] loss=0.2927 val_loss=0.5395 scale=1.0000 norm=0.5732                    \n",
      "[iter 36] loss=0.2908 val_loss=0.5390 scale=1.0000 norm=0.5687                    \n",
      "[iter 37] loss=0.2804 val_loss=0.5316 scale=2.0000 norm=1.1356                    \n",
      "[iter 38] loss=0.2740 val_loss=0.5324 scale=2.0000 norm=1.1341                    \n",
      "[iter 39] loss=0.2596 val_loss=0.5306 scale=2.0000 norm=1.1294                    \n",
      "[iter 40] loss=0.2580 val_loss=0.5308 scale=1.0000 norm=0.5786                    \n",
      "[iter 41] loss=0.2445 val_loss=0.5314 scale=2.0000 norm=1.1336                    \n",
      "[iter 42] loss=0.2237 val_loss=0.5348 scale=2.0000 norm=1.1168                    \n",
      "[iter 43] loss=0.2220 val_loss=0.5416 scale=2.0000 norm=1.1195                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL39 (val_loss=0.5306)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=1,\n",
      "                      max_features='sqrt', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.08397881821523298, 'minibatch_frac': 1.0, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.5315 scale=1.0000 norm=1.0277                     \n",
      "[iter 1] loss=1.4465 val_loss=1.5027 scale=1.0000 norm=0.9935                     \n",
      "[iter 2] loss=1.4267 val_loss=1.4847 scale=1.0000 norm=0.9777                     \n",
      "[iter 3] loss=1.4142 val_loss=1.4727 scale=1.0000 norm=0.9697                     \n",
      "[iter 4] loss=1.4055 val_loss=1.4645 scale=1.0000 norm=0.9655                     \n",
      "[iter 5] loss=1.3990 val_loss=1.4583 scale=1.0000 norm=0.9635                     \n",
      "[iter 6] loss=1.3940 val_loss=1.4510 scale=2.0000 norm=1.9258                     \n",
      "[iter 7] loss=1.3863 val_loss=1.4511 scale=2.0000 norm=1.9271                     \n",
      "[iter 8] loss=1.3796 val_loss=1.4515 scale=2.0000 norm=1.9262                     \n",
      "[iter 9] loss=1.3738 val_loss=1.4488 scale=1.0000 norm=0.9629                     \n",
      "[iter 10] loss=1.3716 val_loss=1.4411 scale=2.0000 norm=1.9281                    \n",
      "[iter 11] loss=1.3677 val_loss=1.4436 scale=1.0000 norm=0.9672                    \n",
      "[iter 12] loss=1.3653 val_loss=1.4461 scale=1.0000 norm=0.9668                    \n",
      "[iter 13] loss=1.3631 val_loss=1.4424 scale=1.0000 norm=0.9666                    \n",
      "[iter 14] loss=1.3613 val_loss=1.4445 scale=1.0000 norm=0.9676                    \n",
      "[iter 15] loss=1.3593 val_loss=1.4376 scale=2.0000 norm=1.9347                    \n",
      "[iter 16] loss=1.3562 val_loss=1.4401 scale=1.0000 norm=0.9699                    \n",
      "[iter 17] loss=1.3543 val_loss=1.4418 scale=1.0000 norm=0.9694                    \n",
      "[iter 18] loss=1.3529 val_loss=1.4426 scale=1.0000 norm=0.9696                    \n",
      "[iter 19] loss=1.3514 val_loss=1.4421 scale=1.0000 norm=0.9695                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL15 (val_loss=1.4376)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=1,\n",
      "                      max_features=0.75, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.29467768445960013, 'minibatch_frac': 0.8, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 10}}\n",
      "[iter 0] loss=1.5200 val_loss=1.4875 scale=1.0000 norm=1.0640                     \n",
      "[iter 1] loss=1.3930 val_loss=1.3977 scale=1.0000 norm=0.9277                     \n",
      "[iter 2] loss=1.2595 val_loss=1.2308 scale=2.0000 norm=1.6451                     \n",
      "[iter 3] loss=1.1102 val_loss=1.1387 scale=1.0000 norm=0.7635                     \n",
      "[iter 4] loss=1.0249 val_loss=1.0479 scale=2.0000 norm=1.4428                     \n",
      "[iter 5] loss=0.8942 val_loss=0.9972 scale=2.0000 norm=1.4201                     \n",
      "[iter 6] loss=0.8275 val_loss=0.9674 scale=1.0000 norm=0.7112                     \n",
      "[iter 7] loss=0.7948 val_loss=0.9398 scale=1.0000 norm=0.7132                     \n",
      "[iter 8] loss=0.7890 val_loss=0.8372 scale=2.0000 norm=1.4775                     \n",
      "[iter 9] loss=0.7712 val_loss=0.8148 scale=1.0000 norm=0.7518                     \n",
      "[iter 10] loss=0.7075 val_loss=0.8061 scale=2.0000 norm=1.4468                    \n",
      "[iter 11] loss=0.7117 val_loss=0.7945 scale=1.0000 norm=0.7533                    \n",
      "[iter 12] loss=0.6727 val_loss=0.7875 scale=1.0000 norm=0.7232                    \n",
      "[iter 13] loss=0.6347 val_loss=0.7795 scale=1.0000 norm=0.7005                    \n",
      "[iter 14] loss=0.6410 val_loss=0.7758 scale=1.0000 norm=0.7332                    \n",
      "[iter 15] loss=0.6404 val_loss=0.7410 scale=2.0000 norm=1.4647                    \n",
      "[iter 16] loss=0.6031 val_loss=0.7241 scale=1.0000 norm=0.7266                    \n",
      "[iter 17] loss=0.6268 val_loss=0.7225 scale=1.0000 norm=0.7458                    \n",
      "[iter 18] loss=0.6088 val_loss=0.7067 scale=1.0000 norm=0.7286                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 19] loss=0.5942 val_loss=0.6993 scale=1.0000 norm=0.7305                    \n",
      "[iter 20] loss=0.5757 val_loss=0.6944 scale=2.0000 norm=1.4348                    \n",
      "[iter 21] loss=0.5805 val_loss=0.6829 scale=2.0000 norm=1.4693                    \n",
      "[iter 22] loss=0.5624 val_loss=0.6734 scale=1.0000 norm=0.7319                    \n",
      "[iter 23] loss=0.5438 val_loss=0.6767 scale=1.0000 norm=0.7123                    \n",
      "[iter 24] loss=0.5674 val_loss=0.6730 scale=0.5000 norm=0.3680                    \n",
      "[iter 25] loss=0.5594 val_loss=0.6687 scale=1.0000 norm=0.7336                    \n",
      "[iter 26] loss=0.5688 val_loss=0.6643 scale=2.0000 norm=1.4819                    \n",
      "[iter 27] loss=0.5406 val_loss=0.6583 scale=1.0000 norm=0.7132                    \n",
      "[iter 28] loss=0.5515 val_loss=0.6534 scale=1.0000 norm=0.7278                    \n",
      "[iter 29] loss=0.5446 val_loss=0.6520 scale=2.0000 norm=1.4395                    \n",
      "[iter 30] loss=0.5446 val_loss=0.6459 scale=1.0000 norm=0.7346                    \n",
      "[iter 31] loss=0.5536 val_loss=0.6511 scale=1.0000 norm=0.7378                    \n",
      "[iter 32] loss=0.5483 val_loss=0.6511 scale=1.0000 norm=0.7378                    \n",
      "[iter 33] loss=0.5423 val_loss=0.6485 scale=1.0000 norm=0.7277                    \n",
      "[iter 34] loss=0.5389 val_loss=0.6514 scale=1.0000 norm=0.7326                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL30 (val_loss=0.6459)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.26648259003556213, 'minibatch_frac': 1.0, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.1888 scale=1.0000 norm=1.0277                     \n",
      "[iter 1] loss=1.1580 val_loss=0.9934 scale=2.0000 norm=1.4641                     \n",
      "[iter 2] loss=0.9355 val_loss=0.8004 scale=2.0000 norm=1.2543                     \n",
      "[iter 3] loss=0.7504 val_loss=0.6729 scale=2.0000 norm=1.1768                     \n",
      "[iter 4] loss=0.5930 val_loss=0.5975 scale=2.0000 norm=1.1341                     \n",
      "[iter 5] loss=0.4650 val_loss=0.5698 scale=2.0000 norm=1.1116                     \n",
      "[iter 6] loss=0.3653 val_loss=0.5808 scale=2.0000 norm=1.1031                     \n",
      "[iter 7] loss=0.2966 val_loss=0.6650 scale=2.0000 norm=1.1111                     \n",
      "[iter 8] loss=0.2450 val_loss=0.7502 scale=2.0000 norm=1.1196                     \n",
      "[iter 9] loss=0.2105 val_loss=0.8010 scale=1.0000 norm=0.5661                     \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL5 (val_loss=0.5698)                                        \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features='log2', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.10275291886717783, 'minibatch_frac': 0.6, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5178 val_loss=1.3647 scale=1.0000 norm=1.0617                     \n",
      "[iter 1] loss=1.2654 val_loss=1.2824 scale=1.0000 norm=0.8322                     \n",
      "[iter 2] loss=1.1786 val_loss=1.1839 scale=2.0000 norm=1.5541                     \n",
      "[iter 3] loss=1.0796 val_loss=1.1035 scale=2.0000 norm=1.4469                     \n",
      "[iter 4] loss=1.0238 val_loss=1.0633 scale=1.0000 norm=0.7067                     \n",
      "[iter 5] loss=0.9734 val_loss=1.0075 scale=2.0000 norm=1.3548                     \n",
      "[iter 6] loss=0.8955 val_loss=0.9508 scale=2.0000 norm=1.2979                     \n",
      "[iter 7] loss=0.8501 val_loss=0.9059 scale=2.0000 norm=1.2857                     \n",
      "[iter 8] loss=0.7888 val_loss=0.8296 scale=2.0000 norm=1.2668                     \n",
      "[iter 9] loss=0.7417 val_loss=0.8044 scale=1.0000 norm=0.6259                     \n",
      "[iter 10] loss=0.7025 val_loss=0.7576 scale=2.0000 norm=1.2166                    \n",
      "[iter 11] loss=0.6525 val_loss=0.7214 scale=2.0000 norm=1.2228                    \n",
      "[iter 12] loss=0.5905 val_loss=0.6802 scale=2.0000 norm=1.1788                    \n",
      "[iter 13] loss=0.5195 val_loss=0.6634 scale=2.0000 norm=1.1338                    \n",
      "[iter 14] loss=0.4902 val_loss=0.6446 scale=1.0000 norm=0.5799                    \n",
      "[iter 15] loss=0.4702 val_loss=0.6336 scale=1.0000 norm=0.5743                    \n",
      "[iter 16] loss=0.4409 val_loss=0.6163 scale=2.0000 norm=1.1302                    \n",
      "[iter 17] loss=0.4176 val_loss=0.6115 scale=1.0000 norm=0.5651                    \n",
      "[iter 18] loss=0.3936 val_loss=0.6025 scale=1.0000 norm=0.5603                    \n",
      "[iter 19] loss=0.3737 val_loss=0.5919 scale=2.0000 norm=1.1225                    \n",
      "[iter 20] loss=0.3386 val_loss=0.5890 scale=2.0000 norm=1.1199                    \n",
      "[iter 21] loss=0.3328 val_loss=0.5859 scale=1.0000 norm=0.5695                    \n",
      "[iter 22] loss=0.3066 val_loss=0.5894 scale=2.0000 norm=1.1283                    \n",
      "[iter 23] loss=0.2728 val_loss=0.5948 scale=1.0000 norm=0.5522                    \n",
      "[iter 24] loss=0.2847 val_loss=0.6024 scale=1.0000 norm=0.5656                    \n",
      "[iter 25] loss=0.2658 val_loss=0.6107 scale=1.0000 norm=0.5568                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL21 (val_loss=0.5859)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.75, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.2154810064981616, 'minibatch_frac': 1.0, 'n_estimators': 1000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 5}}\n",
      "[iter 0] loss=1.4840 val_loss=1.2109 scale=1.0000 norm=1.0277                     \n",
      "[iter 1] loss=1.1556 val_loss=1.0706 scale=2.0000 norm=1.4846                     \n",
      "[iter 2] loss=0.9703 val_loss=0.9423 scale=2.0000 norm=1.2911                     \n",
      "[iter 3] loss=0.8075 val_loss=0.8362 scale=2.0000 norm=1.2018                     \n",
      "[iter 4] loss=0.6665 val_loss=0.7356 scale=2.0000 norm=1.1578                     \n",
      "[iter 5] loss=0.5445 val_loss=0.6763 scale=2.0000 norm=1.1298                     \n",
      "[iter 6] loss=0.4395 val_loss=0.6563 scale=2.0000 norm=1.1035                     \n",
      "[iter 7] loss=0.3552 val_loss=0.6509 scale=2.0000 norm=1.0927                     \n",
      "[iter 8] loss=0.2883 val_loss=0.6800 scale=2.0000 norm=1.0864                     \n",
      "[iter 9] loss=0.2295 val_loss=0.7131 scale=2.0000 norm=1.0749                     \n",
      "[iter 10] loss=0.1939 val_loss=0.8023 scale=2.0000 norm=1.0903                    \n",
      "[iter 11] loss=0.1656 val_loss=0.9119 scale=2.0000 norm=1.0988                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL7 (val_loss=0.6509)                                        \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.4944262808087304, 'minibatch_frac': 0.6, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=1.5178 val_loss=1.1392 scale=1.0000 norm=1.0617                     \n",
      "[iter 1] loss=1.1735 val_loss=0.8858 scale=2.0000 norm=1.3703                     \n",
      "[iter 2] loss=0.8930 val_loss=0.9169 scale=2.0000 norm=1.3104                     \n",
      "[iter 3] loss=0.6341 val_loss=1.0613 scale=2.0000 norm=1.2456                     \n",
      "[iter 4] loss=0.6119 val_loss=1.0223 scale=1.0000 norm=0.7059                     \n",
      "[iter 5] loss=0.6173 val_loss=1.0375 scale=1.0000 norm=0.6633                     \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL1 (val_loss=0.8858)                                        \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=1,\n",
      "                      max_features=0.5, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.006585718176280422, 'minibatch_frac': 0.7, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5211 val_loss=1.5836 scale=1.0000 norm=1.0651                     \n",
      "[iter 1] loss=1.4664 val_loss=1.5515 scale=1.0000 norm=1.0108                     \n",
      "[iter 2] loss=1.4379 val_loss=1.5289 scale=1.0000 norm=0.9845                     \n",
      "[iter 3] loss=1.3839 val_loss=1.5152 scale=1.0000 norm=0.9307                     \n",
      "[iter 4] loss=1.4072 val_loss=1.5025 scale=1.0000 norm=0.9582                     \n",
      "[iter 5] loss=1.3933 val_loss=1.4904 scale=1.0000 norm=0.9451                     \n",
      "[iter 6] loss=1.3498 val_loss=1.4804 scale=1.0000 norm=0.9029                     \n",
      "[iter 7] loss=1.3643 val_loss=1.4695 scale=1.0000 norm=0.9208                     \n",
      "[iter 8] loss=1.3591 val_loss=1.4616 scale=1.0000 norm=0.9174                     \n",
      "[iter 9] loss=1.3245 val_loss=1.4460 scale=2.0000 norm=1.7678                     \n",
      "[iter 10] loss=1.3109 val_loss=1.4398 scale=1.0000 norm=0.8722                    \n",
      "[iter 11] loss=1.3380 val_loss=1.4327 scale=1.0000 norm=0.9005                    \n",
      "[iter 12] loss=1.2910 val_loss=1.4189 scale=2.0000 norm=1.7100                    \n",
      "[iter 13] loss=1.2890 val_loss=1.4139 scale=1.0000 norm=0.8566                    \n",
      "[iter 14] loss=1.3054 val_loss=1.4017 scale=2.0000 norm=1.7462                    \n",
      "[iter 15] loss=1.2885 val_loss=1.3906 scale=2.0000 norm=1.7178                    \n",
      "[iter 16] loss=1.2839 val_loss=1.3797 scale=2.0000 norm=1.7128                    \n",
      "[iter 17] loss=1.2875 val_loss=1.3729 scale=2.0000 norm=1.7298                    \n",
      "[iter 18] loss=1.2738 val_loss=1.3634 scale=2.0000 norm=1.7076                    \n",
      "[iter 19] loss=1.2530 val_loss=1.3558 scale=2.0000 norm=1.6671                    \n",
      "[iter 20] loss=1.2307 val_loss=1.3488 scale=2.0000 norm=1.6274                    \n",
      "[iter 21] loss=1.2521 val_loss=1.3445 scale=2.0000 norm=1.6799                    \n",
      "[iter 22] loss=1.2514 val_loss=1.3362 scale=2.0000 norm=1.6763                    \n",
      "[iter 23] loss=1.2103 val_loss=1.3290 scale=2.0000 norm=1.5983                    \n",
      "[iter 24] loss=1.2220 val_loss=1.3224 scale=2.0000 norm=1.6300                    \n",
      "[iter 25] loss=1.2261 val_loss=1.3164 scale=2.0000 norm=1.6455                    \n",
      "[iter 26] loss=1.2428 val_loss=1.3092 scale=2.0000 norm=1.6826                    \n",
      "[iter 27] loss=1.1936 val_loss=1.3044 scale=2.0000 norm=1.5847                    \n",
      "[iter 28] loss=1.2101 val_loss=1.2981 scale=2.0000 norm=1.6293                    \n",
      "[iter 29] loss=1.2044 val_loss=1.2924 scale=2.0000 norm=1.6215                    \n",
      "[iter 30] loss=1.2088 val_loss=1.2894 scale=1.0000 norm=0.8155                    \n",
      "[iter 31] loss=1.1932 val_loss=1.2853 scale=2.0000 norm=1.6001                    \n",
      "[iter 32] loss=1.2049 val_loss=1.2807 scale=2.0000 norm=1.6287                    \n",
      "[iter 33] loss=1.1924 val_loss=1.2779 scale=1.0000 norm=0.8060                    \n",
      "[iter 34] loss=1.1913 val_loss=1.2752 scale=1.0000 norm=0.8035                    \n",
      "[iter 35] loss=1.1869 val_loss=1.2727 scale=1.0000 norm=0.7982                    \n",
      "[iter 36] loss=1.1833 val_loss=1.2681 scale=2.0000 norm=1.5993                    \n",
      "[iter 37] loss=1.1941 val_loss=1.2639 scale=2.0000 norm=1.6235                    \n",
      "[iter 38] loss=1.1567 val_loss=1.2624 scale=1.0000 norm=0.7725                    \n",
      "[iter 39] loss=1.1762 val_loss=1.2581 scale=2.0000 norm=1.5957                    \n",
      "[iter 40] loss=1.1789 val_loss=1.2534 scale=2.0000 norm=1.6040                    \n",
      "[iter 41] loss=1.1590 val_loss=1.2490 scale=2.0000 norm=1.5601                    \n",
      "[iter 42] loss=1.1692 val_loss=1.2466 scale=1.0000 norm=0.7923                    \n",
      "[iter 43] loss=1.1643 val_loss=1.2442 scale=1.0000 norm=0.7902                    \n",
      "[iter 44] loss=1.1595 val_loss=1.2407 scale=2.0000 norm=1.5744                    \n",
      "[iter 45] loss=1.1687 val_loss=1.2373 scale=2.0000 norm=1.6045                    \n",
      "[iter 46] loss=1.1589 val_loss=1.2333 scale=2.0000 norm=1.5786                    \n",
      "[iter 47] loss=1.1542 val_loss=1.2299 scale=2.0000 norm=1.5821                    \n",
      "[iter 48] loss=1.1422 val_loss=1.2279 scale=1.0000 norm=0.7730                    \n",
      "[iter 49] loss=1.1490 val_loss=1.2247 scale=2.0000 norm=1.5755                    \n",
      "[iter 50] loss=1.1501 val_loss=1.2216 scale=2.0000 norm=1.5827                    \n",
      "[iter 51] loss=1.1393 val_loss=1.2182 scale=2.0000 norm=1.5620                    \n",
      "[iter 52] loss=1.1208 val_loss=1.2155 scale=2.0000 norm=1.5231                    \n",
      "[iter 53] loss=1.1427 val_loss=1.2117 scale=2.0000 norm=1.5782                    \n",
      "[iter 54] loss=1.1244 val_loss=1.2081 scale=2.0000 norm=1.5418                    \n",
      "[iter 55] loss=1.1348 val_loss=1.2045 scale=2.0000 norm=1.5670                    \n",
      "[iter 56] loss=1.1269 val_loss=1.2011 scale=2.0000 norm=1.5581                    \n",
      "[iter 57] loss=1.1290 val_loss=1.1984 scale=2.0000 norm=1.5661                    \n",
      "[iter 58] loss=1.1202 val_loss=1.1955 scale=2.0000 norm=1.5491                    \n",
      "[iter 59] loss=1.1210 val_loss=1.1935 scale=1.0000 norm=0.7748                    \n",
      "[iter 60] loss=1.1238 val_loss=1.1911 scale=2.0000 norm=1.5629                    \n",
      "[iter 61] loss=1.1137 val_loss=1.1891 scale=2.0000 norm=1.5477                    \n",
      "[iter 62] loss=1.1016 val_loss=1.1874 scale=1.0000 norm=0.7611                    \n",
      "[iter 63] loss=1.1082 val_loss=1.1851 scale=2.0000 norm=1.5391                    \n",
      "[iter 64] loss=1.0895 val_loss=1.1827 scale=2.0000 norm=1.5029                    \n",
      "[iter 65] loss=1.1096 val_loss=1.1809 scale=1.0000 norm=0.7757                    \n",
      "[iter 66] loss=1.0920 val_loss=1.1792 scale=1.0000 norm=0.7584                    \n",
      "[iter 67] loss=1.1051 val_loss=1.1773 scale=1.0000 norm=0.7733                    \n",
      "[iter 68] loss=1.0838 val_loss=1.1757 scale=1.0000 norm=0.7469                    \n",
      "[iter 69] loss=1.1100 val_loss=1.1740 scale=1.0000 norm=0.7772                    \n",
      "[iter 70] loss=1.0940 val_loss=1.1709 scale=2.0000 norm=1.5277                    \n",
      "[iter 71] loss=1.0940 val_loss=1.1688 scale=2.0000 norm=1.5371                    \n",
      "[iter 72] loss=1.0789 val_loss=1.1672 scale=1.0000 norm=0.7486                    \n",
      "[iter 73] loss=1.0868 val_loss=1.1656 scale=1.0000 norm=0.7613                    \n",
      "[iter 74] loss=1.0780 val_loss=1.1629 scale=2.0000 norm=1.5115                    \n",
      "[iter 75] loss=1.0923 val_loss=1.1612 scale=1.0000 norm=0.7695                    \n",
      "[iter 76] loss=1.1002 val_loss=1.1591 scale=2.0000 norm=1.5626                    \n",
      "[iter 77] loss=1.0819 val_loss=1.1576 scale=1.0000 norm=0.7624                    \n",
      "[iter 78] loss=1.0892 val_loss=1.1561 scale=1.0000 norm=0.7703                    \n",
      "[iter 79] loss=1.0885 val_loss=1.1531 scale=2.0000 norm=1.5457                    \n",
      "[iter 80] loss=1.0632 val_loss=1.1515 scale=1.0000 norm=0.7463                    \n",
      "[iter 81] loss=1.0745 val_loss=1.1488 scale=2.0000 norm=1.5188                    \n",
      "[iter 82] loss=1.0674 val_loss=1.1472 scale=1.0000 norm=0.7544                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 83] loss=1.0658 val_loss=1.1458 scale=1.0000 norm=0.7506                    \n",
      "[iter 84] loss=1.0504 val_loss=1.1440 scale=2.0000 norm=1.4831                    \n",
      "[iter 85] loss=1.0399 val_loss=1.1426 scale=1.0000 norm=0.7294                    \n",
      "[iter 86] loss=1.0594 val_loss=1.1411 scale=1.0000 norm=0.7487                    \n",
      "[iter 87] loss=1.0616 val_loss=1.1394 scale=2.0000 norm=1.5135                    \n",
      "[iter 88] loss=1.0423 val_loss=1.1373 scale=2.0000 norm=1.4813                    \n",
      "[iter 89] loss=1.0576 val_loss=1.1359 scale=1.0000 norm=0.7522                    \n",
      "[iter 90] loss=1.0490 val_loss=1.1340 scale=2.0000 norm=1.4973                    \n",
      "[iter 91] loss=1.0360 val_loss=1.1314 scale=2.0000 norm=1.4740                    \n",
      "[iter 92] loss=1.0449 val_loss=1.1297 scale=1.0000 norm=0.7420                    \n",
      "[iter 93] loss=1.0568 val_loss=1.1280 scale=2.0000 norm=1.5224                    \n",
      "[iter 94] loss=1.0456 val_loss=1.1267 scale=1.0000 norm=0.7500                    \n",
      "[iter 95] loss=1.0442 val_loss=1.1240 scale=2.0000 norm=1.4983                    \n",
      "[iter 96] loss=1.0548 val_loss=1.1228 scale=1.0000 norm=0.7611                    \n",
      "[iter 97] loss=1.0468 val_loss=1.1214 scale=1.0000 norm=0.7513                    \n",
      "[iter 98] loss=1.0395 val_loss=1.1192 scale=2.0000 norm=1.4953                    \n",
      "[iter 99] loss=1.0331 val_loss=1.1179 scale=1.0000 norm=0.7419                    \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=1,\n",
      "                      max_features='log2', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.07397465795472527, 'minibatch_frac': 0.8, 'n_estimators': 2000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 10}}\n",
      "[iter 0] loss=1.5200 val_loss=1.5322 scale=1.0000 norm=1.0640                     \n",
      "[iter 1] loss=1.4635 val_loss=1.5055 scale=1.0000 norm=1.0088                     \n",
      "[iter 2] loss=1.4402 val_loss=1.4900 scale=1.0000 norm=0.9881                     \n",
      "[iter 3] loss=1.3709 val_loss=1.4729 scale=2.0000 norm=1.8452                     \n",
      "[iter 4] loss=1.4173 val_loss=1.4684 scale=1.0000 norm=0.9804                     \n",
      "[iter 5] loss=1.4149 val_loss=1.4603 scale=1.0000 norm=0.9819                     \n",
      "[iter 6] loss=1.3538 val_loss=1.4524 scale=2.0000 norm=1.8480                     \n",
      "[iter 7] loss=1.4021 val_loss=1.4526 scale=2.0000 norm=1.9636                     \n",
      "[iter 8] loss=1.3993 val_loss=1.4554 scale=1.0000 norm=0.9829                     \n",
      "[iter 9] loss=1.3838 val_loss=1.4598 scale=2.0000 norm=1.9347                     \n",
      "[iter 10] loss=1.3719 val_loss=1.4596 scale=1.0000 norm=0.9594                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL6 (val_loss=1.4524)                                        \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=5,\n",
      "                      max_features='sqrt', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.1243098525543364, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.3988 scale=1.0000 norm=1.0277                     \n",
      "[iter 1] loss=1.2360 val_loss=1.1788 scale=2.0000 norm=1.6279                     \n",
      "[iter 2] loss=1.1042 val_loss=1.0824 scale=2.0000 norm=1.4578                     \n",
      "[iter 3] loss=1.0208 val_loss=1.0012 scale=2.0000 norm=1.3965                     \n",
      "[iter 4] loss=0.9331 val_loss=0.9352 scale=2.0000 norm=1.3320                     \n",
      "[iter 5] loss=0.8549 val_loss=0.8795 scale=2.0000 norm=1.2911                     \n",
      "[iter 6] loss=0.7844 val_loss=0.8214 scale=2.0000 norm=1.2623                     \n",
      "[iter 7] loss=0.7142 val_loss=0.7722 scale=2.0000 norm=1.2373                     \n",
      "[iter 8] loss=0.6491 val_loss=0.7398 scale=2.0000 norm=1.2162                     \n",
      "[iter 9] loss=0.5878 val_loss=0.7076 scale=2.0000 norm=1.1955                     \n",
      "[iter 10] loss=0.5364 val_loss=0.6914 scale=2.0000 norm=1.1885                    \n",
      "[iter 11] loss=0.4937 val_loss=0.6677 scale=2.0000 norm=1.1848                    \n",
      "[iter 12] loss=0.4537 val_loss=0.6475 scale=2.0000 norm=1.1805                    \n",
      "[iter 13] loss=0.4203 val_loss=0.6247 scale=2.0000 norm=1.1813                    \n",
      "[iter 14] loss=0.3913 val_loss=0.6182 scale=1.0000 norm=0.5910                    \n",
      "[iter 15] loss=0.3780 val_loss=0.6253 scale=2.0000 norm=1.1820                    \n",
      "[iter 16] loss=0.3524 val_loss=0.6293 scale=2.0000 norm=1.1807                    \n",
      "[iter 17] loss=0.3326 val_loss=0.6289 scale=1.0000 norm=0.5925                    \n",
      "[iter 18] loss=0.3232 val_loss=0.6382 scale=2.0000 norm=1.1855                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL14 (val_loss=0.6182)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features='sqrt', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.17940044545140618, 'minibatch_frac': 0.6, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5178 val_loss=1.2900 scale=1.0000 norm=1.0617                     \n",
      "[iter 1] loss=1.2388 val_loss=1.1356 scale=2.0000 norm=1.6024                     \n",
      "[iter 2] loss=1.0910 val_loss=1.0620 scale=1.0000 norm=0.7163                     \n",
      "[iter 3] loss=0.9930 val_loss=0.9526 scale=2.0000 norm=1.3395                     \n",
      "[iter 4] loss=0.9012 val_loss=0.8646 scale=2.0000 norm=1.2980                     \n",
      "[iter 5] loss=0.7895 val_loss=0.7922 scale=2.0000 norm=1.2329                     \n",
      "[iter 6] loss=0.6675 val_loss=0.7383 scale=2.0000 norm=1.1950                     \n",
      "[iter 7] loss=0.6096 val_loss=0.7057 scale=2.0000 norm=1.1843                     \n",
      "[iter 8] loss=0.5215 val_loss=0.6938 scale=2.0000 norm=1.1749                     \n",
      "[iter 9] loss=0.4675 val_loss=0.6883 scale=1.0000 norm=0.5918                     \n",
      "[iter 10] loss=0.4377 val_loss=0.6925 scale=2.0000 norm=1.1465                    \n",
      "[iter 11] loss=0.4027 val_loss=0.6898 scale=1.0000 norm=0.5877                    \n",
      "[iter 12] loss=0.3709 val_loss=0.7062 scale=2.0000 norm=1.1622                    \n",
      "[iter 13] loss=0.3182 val_loss=0.7276 scale=1.0000 norm=0.5630                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL9 (val_loss=0.6883)                                        \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=5,\n",
      "                      max_features='log2', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.01565610961094303, 'minibatch_frac': 1.0, 'n_estimators': 500, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': True, 'n_neighbors': 5}}\n",
      "[iter 0] loss=1.4840 val_loss=1.5540 scale=1.0000 norm=1.0277                     \n",
      "[iter 1] loss=1.3495 val_loss=1.5228 scale=1.0000 norm=0.9048                     \n",
      "[iter 2] loss=1.3117 val_loss=1.4982 scale=1.0000 norm=0.8723                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 3] loss=1.2912 val_loss=1.4658 scale=1.0000 norm=0.8561                     \n",
      "[iter 4] loss=1.2737 val_loss=1.4398 scale=1.0000 norm=0.8428                     \n",
      "[iter 5] loss=1.2586 val_loss=1.4184 scale=1.0000 norm=0.8319                     \n",
      "[iter 6] loss=1.2451 val_loss=1.3908 scale=1.0000 norm=0.8223                     \n",
      "[iter 7] loss=1.2319 val_loss=1.3756 scale=1.0000 norm=0.8132                     \n",
      "[iter 8] loss=1.2215 val_loss=1.3539 scale=1.0000 norm=0.8064                     \n",
      "[iter 9] loss=1.2105 val_loss=1.3368 scale=1.0000 norm=0.7991                     \n",
      "[iter 10] loss=1.2005 val_loss=1.3152 scale=1.0000 norm=0.7928                    \n",
      "[iter 11] loss=1.1914 val_loss=1.3045 scale=1.0000 norm=0.7871                    \n",
      "[iter 12] loss=1.1837 val_loss=1.2932 scale=1.0000 norm=0.7827                    \n",
      "[iter 13] loss=1.1761 val_loss=1.2780 scale=1.0000 norm=0.7782                    \n",
      "[iter 14] loss=1.1669 val_loss=1.2691 scale=1.0000 norm=0.7727                    \n",
      "[iter 15] loss=1.1599 val_loss=1.2599 scale=1.0000 norm=0.7689                    \n",
      "[iter 16] loss=1.1531 val_loss=1.2514 scale=1.0000 norm=0.7651                    \n",
      "[iter 17] loss=1.1464 val_loss=1.2437 scale=1.0000 norm=0.7616                    \n",
      "[iter 18] loss=1.1388 val_loss=1.2361 scale=1.0000 norm=0.7574                    \n",
      "[iter 19] loss=1.1323 val_loss=1.2217 scale=2.0000 norm=1.5083                    \n",
      "[iter 20] loss=1.1197 val_loss=1.2015 scale=2.0000 norm=1.4953                    \n",
      "[iter 21] loss=1.1029 val_loss=1.1866 scale=2.0000 norm=1.4766                    \n",
      "[iter 22] loss=1.0904 val_loss=1.1733 scale=2.0000 norm=1.4655                    \n",
      "[iter 23] loss=1.0785 val_loss=1.1602 scale=2.0000 norm=1.4558                    \n",
      "[iter 24] loss=1.0665 val_loss=1.1490 scale=2.0000 norm=1.4459                    \n",
      "[iter 25] loss=1.0555 val_loss=1.1434 scale=1.0000 norm=0.7189                    \n",
      "[iter 26] loss=1.0495 val_loss=1.1325 scale=2.0000 norm=1.4328                    \n",
      "[iter 27] loss=1.0384 val_loss=1.1193 scale=2.0000 norm=1.4243                    \n",
      "[iter 28] loss=1.0256 val_loss=1.1084 scale=2.0000 norm=1.4130                    \n",
      "[iter 29] loss=1.0150 val_loss=1.0977 scale=2.0000 norm=1.4052                    \n",
      "[iter 30] loss=1.0048 val_loss=1.0878 scale=2.0000 norm=1.3987                    \n",
      "[iter 31] loss=0.9947 val_loss=1.0772 scale=2.0000 norm=1.3924                    \n",
      "[iter 32] loss=0.9843 val_loss=1.0661 scale=2.0000 norm=1.3853                    \n",
      "[iter 33] loss=0.9725 val_loss=1.0562 scale=2.0000 norm=1.3762                    \n",
      "[iter 34] loss=0.9624 val_loss=1.0465 scale=2.0000 norm=1.3695                    \n",
      "[iter 35] loss=0.9515 val_loss=1.0376 scale=2.0000 norm=1.3618                    \n",
      "[iter 36] loss=0.9423 val_loss=1.0275 scale=2.0000 norm=1.3573                    \n",
      "[iter 37] loss=0.9313 val_loss=1.0185 scale=2.0000 norm=1.3496                    \n",
      "[iter 38] loss=0.9219 val_loss=1.0097 scale=2.0000 norm=1.3451                    \n",
      "[iter 39] loss=0.9120 val_loss=0.9999 scale=2.0000 norm=1.3392                    \n",
      "[iter 40] loss=0.9014 val_loss=0.9922 scale=2.0000 norm=1.3325                    \n",
      "[iter 41] loss=0.8921 val_loss=0.9838 scale=2.0000 norm=1.3271                    \n",
      "[iter 42] loss=0.8822 val_loss=0.9752 scale=2.0000 norm=1.3215                    \n",
      "[iter 43] loss=0.8721 val_loss=0.9678 scale=2.0000 norm=1.3157                    \n",
      "[iter 44] loss=0.8630 val_loss=0.9597 scale=2.0000 norm=1.3110                    \n",
      "[iter 45] loss=0.8535 val_loss=0.9521 scale=2.0000 norm=1.3062                    \n",
      "[iter 46] loss=0.8440 val_loss=0.9442 scale=2.0000 norm=1.3011                    \n",
      "[iter 47] loss=0.8345 val_loss=0.9376 scale=2.0000 norm=1.2962                    \n",
      "[iter 48] loss=0.8260 val_loss=0.9295 scale=2.0000 norm=1.2929                    \n",
      "[iter 49] loss=0.8165 val_loss=0.9218 scale=2.0000 norm=1.2883                    \n",
      "[iter 50] loss=0.8073 val_loss=0.9129 scale=2.0000 norm=1.2841                    \n",
      "[iter 51] loss=0.7972 val_loss=0.9066 scale=2.0000 norm=1.2789                    \n",
      "[iter 52] loss=0.7889 val_loss=0.8991 scale=2.0000 norm=1.2759                    \n",
      "[iter 53] loss=0.7806 val_loss=0.8929 scale=2.0000 norm=1.2724                    \n",
      "[iter 54] loss=0.7722 val_loss=0.8853 scale=2.0000 norm=1.2693                    \n",
      "[iter 55] loss=0.7629 val_loss=0.8783 scale=2.0000 norm=1.2649                    \n",
      "[iter 56] loss=0.7545 val_loss=0.8708 scale=2.0000 norm=1.2618                    \n",
      "[iter 57] loss=0.7463 val_loss=0.8646 scale=2.0000 norm=1.2588                    \n",
      "[iter 58] loss=0.7383 val_loss=0.8581 scale=2.0000 norm=1.2563                    \n",
      "[iter 59] loss=0.7295 val_loss=0.8529 scale=2.0000 norm=1.2526                    \n",
      "[iter 60] loss=0.7218 val_loss=0.8460 scale=2.0000 norm=1.2502                    \n",
      "[iter 61] loss=0.7135 val_loss=0.8403 scale=2.0000 norm=1.2466                    \n",
      "[iter 62] loss=0.7056 val_loss=0.8341 scale=2.0000 norm=1.2440                    \n",
      "[iter 63] loss=0.6975 val_loss=0.8284 scale=2.0000 norm=1.2407                    \n",
      "[iter 64] loss=0.6893 val_loss=0.8232 scale=2.0000 norm=1.2378                    \n",
      "[iter 65] loss=0.6820 val_loss=0.8177 scale=2.0000 norm=1.2361                    \n",
      "[iter 66] loss=0.6746 val_loss=0.8120 scale=2.0000 norm=1.2340                    \n",
      "[iter 67] loss=0.6673 val_loss=0.8060 scale=2.0000 norm=1.2316                    \n",
      "[iter 68] loss=0.6593 val_loss=0.7998 scale=2.0000 norm=1.2290                    \n",
      "[iter 69] loss=0.6517 val_loss=0.7955 scale=2.0000 norm=1.2264                    \n",
      "[iter 70] loss=0.6447 val_loss=0.7899 scale=2.0000 norm=1.2247                    \n",
      "[iter 71] loss=0.6371 val_loss=0.7833 scale=2.0000 norm=1.2223                    \n",
      "[iter 72] loss=0.6290 val_loss=0.7781 scale=2.0000 norm=1.2194                    \n",
      "[iter 73] loss=0.6223 val_loss=0.7728 scale=2.0000 norm=1.2178                    \n",
      "[iter 74] loss=0.6151 val_loss=0.7686 scale=2.0000 norm=1.2159                    \n",
      "[iter 75] loss=0.6081 val_loss=0.7644 scale=2.0000 norm=1.2141                    \n",
      "[iter 76] loss=0.6013 val_loss=0.7601 scale=2.0000 norm=1.2122                    \n",
      "[iter 77] loss=0.5947 val_loss=0.7556 scale=2.0000 norm=1.2106                    \n",
      "[iter 78] loss=0.5878 val_loss=0.7514 scale=2.0000 norm=1.2086                    \n",
      "[iter 79] loss=0.5811 val_loss=0.7491 scale=1.0000 norm=0.6035                    \n",
      "[iter 80] loss=0.5777 val_loss=0.7452 scale=2.0000 norm=1.2062                    \n",
      "[iter 81] loss=0.5717 val_loss=0.7429 scale=1.0000 norm=0.6027                    \n",
      "[iter 82] loss=0.5685 val_loss=0.7408 scale=1.0000 norm=0.6024                    \n",
      "[iter 83] loss=0.5652 val_loss=0.7376 scale=2.0000 norm=1.2040                    \n",
      "[iter 84] loss=0.5590 val_loss=0.7355 scale=1.0000 norm=0.6013                    \n",
      "[iter 85] loss=0.5558 val_loss=0.7308 scale=2.0000 norm=1.2018                    \n",
      "[iter 86] loss=0.5494 val_loss=0.7287 scale=1.0000 norm=0.6002                    \n",
      "[iter 87] loss=0.5463 val_loss=0.7244 scale=2.0000 norm=1.1997                    \n",
      "[iter 88] loss=0.5404 val_loss=0.7207 scale=2.0000 norm=1.1989                    \n",
      "[iter 89] loss=0.5338 val_loss=0.7186 scale=1.0000 norm=0.5985                    \n",
      "[iter 90] loss=0.5308 val_loss=0.7140 scale=2.0000 norm=1.1964                    \n",
      "[iter 91] loss=0.5248 val_loss=0.7101 scale=2.0000 norm=1.1953                    \n",
      "[iter 92] loss=0.5188 val_loss=0.7065 scale=2.0000 norm=1.1939                    \n",
      "[iter 93] loss=0.5129 val_loss=0.7036 scale=2.0000 norm=1.1926                    \n",
      "[iter 94] loss=0.5077 val_loss=0.7014 scale=1.0000 norm=0.5960                    \n",
      "[iter 95] loss=0.5048 val_loss=0.6969 scale=2.0000 norm=1.1915                    \n",
      "[iter 96] loss=0.4992 val_loss=0.6936 scale=2.0000 norm=1.1905                    \n",
      "[iter 97] loss=0.4936 val_loss=0.6892 scale=2.0000 norm=1.1895                    \n",
      "[iter 98] loss=0.4883 val_loss=0.6858 scale=2.0000 norm=1.1886                    \n",
      "[iter 99] loss=0.4831 val_loss=0.6817 scale=2.0000 norm=1.1878                    \n",
      "[iter 100] loss=0.4776 val_loss=0.6797 scale=2.0000 norm=1.1870                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 101] loss=0.4724 val_loss=0.6769 scale=2.0000 norm=1.1857                   \n",
      "[iter 102] loss=0.4675 val_loss=0.6749 scale=2.0000 norm=1.1856                   \n",
      "[iter 103] loss=0.4626 val_loss=0.6726 scale=2.0000 norm=1.1846                   \n",
      "[iter 104] loss=0.4574 val_loss=0.6708 scale=1.0000 norm=0.5917                   \n",
      "[iter 105] loss=0.4550 val_loss=0.6690 scale=2.0000 norm=1.1832                   \n",
      "[iter 106] loss=0.4505 val_loss=0.6672 scale=1.0000 norm=0.5914                   \n",
      "[iter 107] loss=0.4479 val_loss=0.6642 scale=2.0000 norm=1.1824                   \n",
      "[iter 108] loss=0.4432 val_loss=0.6616 scale=2.0000 norm=1.1818                   \n",
      "[iter 109] loss=0.4385 val_loss=0.6602 scale=2.0000 norm=1.1812                   \n",
      "[iter 110] loss=0.4343 val_loss=0.6561 scale=2.0000 norm=1.1810                   \n",
      "[iter 111] loss=0.4299 val_loss=0.6547 scale=1.0000 norm=0.5902                   \n",
      "[iter 112] loss=0.4278 val_loss=0.6522 scale=2.0000 norm=1.1801                   \n",
      "[iter 113] loss=0.4235 val_loss=0.6527 scale=2.0000 norm=1.1799                   \n",
      "[iter 114] loss=0.4193 val_loss=0.6497 scale=2.0000 norm=1.1795                   \n",
      "[iter 115] loss=0.4149 val_loss=0.6482 scale=2.0000 norm=1.1793                   \n",
      "[iter 116] loss=0.4105 val_loss=0.6467 scale=2.0000 norm=1.1781                   \n",
      "[iter 117] loss=0.4062 val_loss=0.6462 scale=1.0000 norm=0.5888                   \n",
      "[iter 118] loss=0.4041 val_loss=0.6460 scale=1.0000 norm=0.5886                   \n",
      "[iter 119] loss=0.4022 val_loss=0.6455 scale=2.0000 norm=1.1771                   \n",
      "[iter 120] loss=0.3985 val_loss=0.6438 scale=1.0000 norm=0.5884                   \n",
      "[iter 121] loss=0.3961 val_loss=0.6426 scale=1.0000 norm=0.5881                   \n",
      "[iter 122] loss=0.3939 val_loss=0.6419 scale=1.0000 norm=0.5878                   \n",
      "[iter 123] loss=0.3919 val_loss=0.6409 scale=1.0000 norm=0.5877                   \n",
      "[iter 124] loss=0.3898 val_loss=0.6387 scale=2.0000 norm=1.1748                   \n",
      "[iter 125] loss=0.3859 val_loss=0.6379 scale=1.0000 norm=0.5870                   \n",
      "[iter 126] loss=0.3839 val_loss=0.6368 scale=1.0000 norm=0.5868                   \n",
      "[iter 127] loss=0.3817 val_loss=0.6360 scale=1.0000 norm=0.5866                   \n",
      "[iter 128] loss=0.3798 val_loss=0.6354 scale=1.0000 norm=0.5865                   \n",
      "[iter 129] loss=0.3779 val_loss=0.6350 scale=1.0000 norm=0.5864                   \n",
      "[iter 130] loss=0.3762 val_loss=0.6342 scale=1.0000 norm=0.5865                   \n",
      "[iter 131] loss=0.3743 val_loss=0.6332 scale=2.0000 norm=1.1728                   \n",
      "[iter 132] loss=0.3709 val_loss=0.6311 scale=2.0000 norm=1.1726                   \n",
      "[iter 133] loss=0.3671 val_loss=0.6301 scale=1.0000 norm=0.5861                   \n",
      "[iter 134] loss=0.3652 val_loss=0.6296 scale=1.0000 norm=0.5861                   \n",
      "[iter 135] loss=0.3634 val_loss=0.6286 scale=1.0000 norm=0.5860                   \n",
      "[iter 136] loss=0.3616 val_loss=0.6280 scale=1.0000 norm=0.5860                   \n",
      "[iter 137] loss=0.3599 val_loss=0.6277 scale=1.0000 norm=0.5859                   \n",
      "[iter 138] loss=0.3583 val_loss=0.6271 scale=1.0000 norm=0.5860                   \n",
      "[iter 139] loss=0.3564 val_loss=0.6262 scale=1.0000 norm=0.5860                   \n",
      "[iter 140] loss=0.3546 val_loss=0.6261 scale=2.0000 norm=1.1717                   \n",
      "[iter 141] loss=0.3516 val_loss=0.6260 scale=1.0000 norm=0.5859                   \n",
      "[iter 142] loss=0.3500 val_loss=0.6255 scale=1.0000 norm=0.5860                   \n",
      "[iter 143] loss=0.3483 val_loss=0.6245 scale=2.0000 norm=1.1719                   \n",
      "[iter 144] loss=0.3453 val_loss=0.6238 scale=1.0000 norm=0.5862                   \n",
      "[iter 145] loss=0.3437 val_loss=0.6231 scale=1.0000 norm=0.5862                   \n",
      "[iter 146] loss=0.3421 val_loss=0.6230 scale=1.0000 norm=0.5862                   \n",
      "[iter 147] loss=0.3406 val_loss=0.6223 scale=1.0000 norm=0.5863                   \n",
      "[iter 148] loss=0.3390 val_loss=0.6219 scale=1.0000 norm=0.5863                   \n",
      "[iter 149] loss=0.3374 val_loss=0.6223 scale=2.0000 norm=1.1726                   \n",
      "[iter 150] loss=0.3349 val_loss=0.6216 scale=1.0000 norm=0.5866                   \n",
      "[iter 151] loss=0.3336 val_loss=0.6211 scale=1.0000 norm=0.5867                   \n",
      "[iter 152] loss=0.3321 val_loss=0.6205 scale=1.0000 norm=0.5867                   \n",
      "[iter 153] loss=0.3306 val_loss=0.6200 scale=1.0000 norm=0.5867                   \n",
      "[iter 154] loss=0.3291 val_loss=0.6193 scale=1.0000 norm=0.5867                   \n",
      "[iter 155] loss=0.3277 val_loss=0.6192 scale=1.0000 norm=0.5868                   \n",
      "[iter 156] loss=0.3261 val_loss=0.6192 scale=1.0000 norm=0.5867                   \n",
      "[iter 157] loss=0.3248 val_loss=0.6186 scale=1.0000 norm=0.5867                   \n",
      "[iter 158] loss=0.3234 val_loss=0.6190 scale=1.0000 norm=0.5867                   \n",
      "[iter 159] loss=0.3220 val_loss=0.6189 scale=1.0000 norm=0.5868                   \n",
      "[iter 160] loss=0.3207 val_loss=0.6185 scale=1.0000 norm=0.5869                   \n",
      "[iter 161] loss=0.3194 val_loss=0.6183 scale=1.0000 norm=0.5869                   \n",
      "[iter 162] loss=0.3180 val_loss=0.6184 scale=1.0000 norm=0.5870                   \n",
      "[iter 163] loss=0.3166 val_loss=0.6182 scale=1.0000 norm=0.5870                   \n",
      "[iter 164] loss=0.3154 val_loss=0.6187 scale=2.0000 norm=1.1740                   \n",
      "[iter 165] loss=0.3129 val_loss=0.6184 scale=1.0000 norm=0.5872                   \n",
      "[iter 166] loss=0.3117 val_loss=0.6184 scale=1.0000 norm=0.5872                   \n",
      "[iter 167] loss=0.3104 val_loss=0.6183 scale=1.0000 norm=0.5873                   \n",
      "[iter 168] loss=0.3092 val_loss=0.6182 scale=1.0000 norm=0.5873                   \n",
      "[iter 169] loss=0.3078 val_loss=0.6179 scale=1.0000 norm=0.5874                   \n",
      "[iter 170] loss=0.3066 val_loss=0.6176 scale=1.0000 norm=0.5875                   \n",
      "[iter 171] loss=0.3054 val_loss=0.6175 scale=1.0000 norm=0.5876                   \n",
      "[iter 172] loss=0.3042 val_loss=0.6176 scale=1.0000 norm=0.5877                   \n",
      "[iter 173] loss=0.3031 val_loss=0.6173 scale=1.0000 norm=0.5879                   \n",
      "[iter 174] loss=0.3019 val_loss=0.6176 scale=1.0000 norm=0.5880                   \n",
      "[iter 175] loss=0.3008 val_loss=0.6178 scale=1.0000 norm=0.5882                   \n",
      "[iter 176] loss=0.2997 val_loss=0.6175 scale=1.0000 norm=0.5883                   \n",
      "[iter 177] loss=0.2986 val_loss=0.6154 scale=1.0000 norm=0.5885                   \n",
      "[iter 178] loss=0.2974 val_loss=0.6152 scale=1.0000 norm=0.5885                   \n",
      "[iter 179] loss=0.2961 val_loss=0.6153 scale=1.0000 norm=0.5886                   \n",
      "[iter 180] loss=0.2950 val_loss=0.6146 scale=1.0000 norm=0.5886                   \n",
      "[iter 181] loss=0.2937 val_loss=0.6145 scale=1.0000 norm=0.5886                   \n",
      "[iter 182] loss=0.2926 val_loss=0.6145 scale=1.0000 norm=0.5887                   \n",
      "[iter 183] loss=0.2915 val_loss=0.6144 scale=1.0000 norm=0.5888                   \n",
      "[iter 184] loss=0.2904 val_loss=0.6112 scale=1.0000 norm=0.5890                   \n",
      "[iter 185] loss=0.2891 val_loss=0.6111 scale=1.0000 norm=0.5889                   \n",
      "[iter 186] loss=0.2881 val_loss=0.6111 scale=1.0000 norm=0.5891                   \n",
      "[iter 187] loss=0.2871 val_loss=0.6111 scale=1.0000 norm=0.5892                   \n",
      "[iter 188] loss=0.2860 val_loss=0.6110 scale=1.0000 norm=0.5894                   \n",
      "[iter 189] loss=0.2850 val_loss=0.6112 scale=1.0000 norm=0.5896                   \n",
      "[iter 190] loss=0.2841 val_loss=0.6110 scale=1.0000 norm=0.5897                   \n",
      "[iter 191] loss=0.2831 val_loss=0.6108 scale=1.0000 norm=0.5898                   \n",
      "[iter 192] loss=0.2821 val_loss=0.6107 scale=1.0000 norm=0.5899                   \n",
      "[iter 193] loss=0.2810 val_loss=0.6081 scale=1.0000 norm=0.5901                   \n",
      "[iter 194] loss=0.2801 val_loss=0.6078 scale=1.0000 norm=0.5902                   \n",
      "[iter 195] loss=0.2790 val_loss=0.6075 scale=1.0000 norm=0.5902                   \n",
      "[iter 196] loss=0.2780 val_loss=0.6073 scale=1.0000 norm=0.5902                   \n",
      "[iter 197] loss=0.2770 val_loss=0.6068 scale=1.0000 norm=0.5904                   \n",
      "[iter 198] loss=0.2760 val_loss=0.6067 scale=1.0000 norm=0.5904                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 199] loss=0.2751 val_loss=0.6065 scale=1.0000 norm=0.5906                   \n",
      "[iter 200] loss=0.2741 val_loss=0.6063 scale=1.0000 norm=0.5906                   \n",
      "[iter 201] loss=0.2733 val_loss=0.6059 scale=1.0000 norm=0.5908                   \n",
      "[iter 202] loss=0.2722 val_loss=0.6060 scale=1.0000 norm=0.5908                   \n",
      "[iter 203] loss=0.2713 val_loss=0.6042 scale=1.0000 norm=0.5909                   \n",
      "[iter 204] loss=0.2701 val_loss=0.6042 scale=1.0000 norm=0.5909                   \n",
      "[iter 205] loss=0.2692 val_loss=0.6043 scale=1.0000 norm=0.5911                   \n",
      "[iter 206] loss=0.2682 val_loss=0.6028 scale=1.0000 norm=0.5911                   \n",
      "[iter 207] loss=0.2673 val_loss=0.6016 scale=1.0000 norm=0.5912                   \n",
      "[iter 208] loss=0.2664 val_loss=0.6018 scale=1.0000 norm=0.5913                   \n",
      "[iter 209] loss=0.2655 val_loss=0.6008 scale=1.0000 norm=0.5913                   \n",
      "[iter 210] loss=0.2645 val_loss=0.6005 scale=1.0000 norm=0.5914                   \n",
      "[iter 211] loss=0.2634 val_loss=0.5996 scale=1.0000 norm=0.5913                   \n",
      "[iter 212] loss=0.2623 val_loss=0.5983 scale=1.0000 norm=0.5912                   \n",
      "[iter 213] loss=0.2614 val_loss=0.5971 scale=1.0000 norm=0.5912                   \n",
      "[iter 214] loss=0.2602 val_loss=0.5958 scale=1.0000 norm=0.5911                   \n",
      "[iter 215] loss=0.2590 val_loss=0.5949 scale=1.0000 norm=0.5910                   \n",
      "[iter 216] loss=0.2582 val_loss=0.5941 scale=1.0000 norm=0.5910                   \n",
      "[iter 217] loss=0.2571 val_loss=0.5934 scale=1.0000 norm=0.5909                   \n",
      "[iter 218] loss=0.2563 val_loss=0.5925 scale=1.0000 norm=0.5910                   \n",
      "[iter 219] loss=0.2554 val_loss=0.5928 scale=1.0000 norm=0.5910                   \n",
      "[iter 220] loss=0.2545 val_loss=0.5932 scale=1.0000 norm=0.5910                   \n",
      "[iter 221] loss=0.2535 val_loss=0.5930 scale=1.0000 norm=0.5910                   \n",
      "[iter 222] loss=0.2526 val_loss=0.5921 scale=1.0000 norm=0.5911                   \n",
      "[iter 223] loss=0.2516 val_loss=0.5913 scale=1.0000 norm=0.5910                   \n",
      "[iter 224] loss=0.2506 val_loss=0.5907 scale=1.0000 norm=0.5910                   \n",
      "[iter 225] loss=0.2497 val_loss=0.5913 scale=1.0000 norm=0.5910                   \n",
      "[iter 226] loss=0.2488 val_loss=0.5907 scale=1.0000 norm=0.5911                   \n",
      "[iter 227] loss=0.2479 val_loss=0.5914 scale=1.0000 norm=0.5911                   \n",
      "[iter 228] loss=0.2471 val_loss=0.5917 scale=1.0000 norm=0.5912                   \n",
      "[iter 229] loss=0.2464 val_loss=0.5912 scale=1.0000 norm=0.5913                   \n",
      "[iter 230] loss=0.2456 val_loss=0.5917 scale=1.0000 norm=0.5914                   \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL226 (val_loss=0.5907)                                      \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=3,\n",
      "                      max_features='sqrt', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.0919359935484889, 'minibatch_frac': 1.0, 'n_estimators': 1000, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.4840 val_loss=1.5108 scale=1.0000 norm=1.0277                     \n",
      "[iter 1] loss=1.3437 val_loss=1.4116 scale=1.0000 norm=0.9048                     \n",
      "[iter 2] loss=1.2672 val_loss=1.3591 scale=1.0000 norm=0.8441                     \n",
      "[iter 3] loss=1.2258 val_loss=1.3208 scale=2.0000 norm=1.6339                     \n",
      "[iter 4] loss=1.1850 val_loss=1.2841 scale=1.0000 norm=0.8012                     \n",
      "[iter 5] loss=1.1528 val_loss=1.2492 scale=2.0000 norm=1.5691                     \n",
      "[iter 6] loss=1.1037 val_loss=1.2169 scale=2.0000 norm=1.5310                     \n",
      "[iter 7] loss=1.0500 val_loss=1.1981 scale=1.0000 norm=0.7461                     \n",
      "[iter 8] loss=1.0290 val_loss=1.1763 scale=2.0000 norm=1.4779                     \n",
      "[iter 9] loss=0.9972 val_loss=1.1591 scale=2.0000 norm=1.4658                     \n",
      "[iter 10] loss=0.9676 val_loss=1.1359 scale=1.0000 norm=0.7288                    \n",
      "[iter 11] loss=0.9410 val_loss=1.0882 scale=2.0000 norm=1.4360                    \n",
      "[iter 12] loss=0.9083 val_loss=1.0678 scale=2.0000 norm=1.4176                    \n",
      "[iter 13] loss=0.8774 val_loss=1.0608 scale=2.0000 norm=1.4085                    \n",
      "[iter 14] loss=0.8560 val_loss=1.0454 scale=2.0000 norm=1.4102                    \n",
      "[iter 15] loss=0.8380 val_loss=1.0265 scale=1.0000 norm=0.7072                    \n",
      "[iter 16] loss=0.8181 val_loss=1.0155 scale=2.0000 norm=1.4021                    \n",
      "[iter 17] loss=0.7999 val_loss=0.9962 scale=2.0000 norm=1.4028                    \n",
      "[iter 18] loss=0.7810 val_loss=0.9944 scale=1.0000 norm=0.7002                    \n",
      "[iter 19] loss=0.7711 val_loss=0.9941 scale=2.0000 norm=1.3989                    \n",
      "[iter 20] loss=0.7551 val_loss=0.9951 scale=1.0000 norm=0.6990                    \n",
      "[iter 21] loss=0.7476 val_loss=0.9824 scale=1.0000 norm=0.6986                    \n",
      "[iter 22] loss=0.7307 val_loss=0.9800 scale=2.0000 norm=1.3886                    \n",
      "[iter 23] loss=0.7193 val_loss=0.9640 scale=1.0000 norm=0.6961                    \n",
      "[iter 24] loss=0.7116 val_loss=0.9483 scale=1.0000 norm=0.6961                    \n",
      "[iter 25] loss=0.7018 val_loss=0.9492 scale=1.0000 norm=0.6933                    \n",
      "[iter 26] loss=0.6957 val_loss=0.9442 scale=2.0000 norm=1.3871                    \n",
      "[iter 27] loss=0.6877 val_loss=0.9460 scale=1.0000 norm=0.6956                    \n",
      "[iter 28] loss=0.6816 val_loss=0.9379 scale=1.0000 norm=0.6954                    \n",
      "[iter 29] loss=0.6689 val_loss=0.9240 scale=1.0000 norm=0.6928                    \n",
      "[iter 30] loss=0.6585 val_loss=0.9277 scale=2.0000 norm=1.3809                    \n",
      "[iter 31] loss=0.6526 val_loss=0.9280 scale=1.0000 norm=0.6941                    \n",
      "[iter 32] loss=0.6484 val_loss=0.9275 scale=1.0000 norm=0.6943                    \n",
      "[iter 33] loss=0.6439 val_loss=0.9165 scale=1.0000 norm=0.6945                    \n",
      "[iter 34] loss=0.6357 val_loss=0.9143 scale=1.0000 norm=0.6928                    \n",
      "[iter 35] loss=0.6248 val_loss=0.9040 scale=1.0000 norm=0.6905                    \n",
      "[iter 36] loss=0.6167 val_loss=0.9078 scale=1.0000 norm=0.6885                    \n",
      "[iter 37] loss=0.6124 val_loss=0.9036 scale=1.0000 norm=0.6889                    \n",
      "[iter 38] loss=0.6085 val_loss=0.9004 scale=1.0000 norm=0.6896                    \n",
      "[iter 39] loss=0.6055 val_loss=0.9070 scale=2.0000 norm=1.3802                    \n",
      "[iter 40] loss=0.6007 val_loss=0.9027 scale=1.0000 norm=0.6919                    \n",
      "[iter 41] loss=0.5919 val_loss=0.9043 scale=1.0000 norm=0.6905                    \n",
      "[iter 42] loss=0.5890 val_loss=0.9085 scale=1.0000 norm=0.6909                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL38 (val_loss=0.9004)                                       \n",
      "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
      "                      max_features=0.75, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                      random_state=69, splitter='best'), 'learning_rate': 0.046672645755786585, 'minibatch_frac': 0.6, 'n_estimators': 100, 'verbose_eval': 1, 'random_state': 69}, 'preprocessing': {'mul': False, 'n_neighbors': 3}}\n",
      "[iter 0] loss=1.5178 val_loss=1.4387 scale=1.0000 norm=1.0617                     \n",
      "[iter 1] loss=1.2892 val_loss=1.2799 scale=2.0000 norm=1.7137                     \n",
      "[iter 2] loss=1.1887 val_loss=1.2171 scale=2.0000 norm=1.5651                     \n",
      "[iter 3] loss=1.1152 val_loss=1.1598 scale=2.0000 norm=1.4609                     \n",
      "[iter 4] loss=1.0795 val_loss=1.1153 scale=2.0000 norm=1.4369                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 5] loss=1.0283 val_loss=1.0800 scale=2.0000 norm=1.3759                     \n",
      "[iter 6] loss=0.9866 val_loss=1.0450 scale=2.0000 norm=1.3204                     \n",
      "[iter 7] loss=0.9586 val_loss=1.0100 scale=2.0000 norm=1.3172                     \n",
      "[iter 8] loss=0.9190 val_loss=0.9786 scale=2.0000 norm=1.2988                     \n",
      "[iter 9] loss=0.8832 val_loss=0.9471 scale=2.0000 norm=1.2690                     \n",
      "[iter 10] loss=0.8451 val_loss=0.9169 scale=2.0000 norm=1.2420                    \n",
      "[iter 11] loss=0.8164 val_loss=0.8872 scale=2.0000 norm=1.2472                    \n",
      "[iter 12] loss=0.7788 val_loss=0.8619 scale=2.0000 norm=1.2069                    \n",
      "[iter 13] loss=0.7399 val_loss=0.8373 scale=2.0000 norm=1.1846                    \n",
      "[iter 14] loss=0.7176 val_loss=0.8129 scale=2.0000 norm=1.1886                    \n",
      "[iter 15] loss=0.6851 val_loss=0.7905 scale=2.0000 norm=1.1728                    \n",
      "[iter 16] loss=0.6567 val_loss=0.7681 scale=2.0000 norm=1.1640                    \n",
      "[iter 17] loss=0.6374 val_loss=0.7472 scale=2.0000 norm=1.1619                    \n",
      "[iter 18] loss=0.6042 val_loss=0.7283 scale=2.0000 norm=1.1507                    \n",
      "[iter 19] loss=0.5742 val_loss=0.7125 scale=2.0000 norm=1.1396                    \n",
      "[iter 20] loss=0.5472 val_loss=0.6877 scale=2.0000 norm=1.1325                    \n",
      "[iter 21] loss=0.5289 val_loss=0.6706 scale=2.0000 norm=1.1390                    \n",
      "[iter 22] loss=0.4991 val_loss=0.6574 scale=2.0000 norm=1.1310                    \n",
      "[iter 23] loss=0.4650 val_loss=0.6434 scale=2.0000 norm=1.0960                    \n",
      "[iter 24] loss=0.4587 val_loss=0.6312 scale=2.0000 norm=1.1102                    \n",
      "[iter 25] loss=0.4321 val_loss=0.6148 scale=2.0000 norm=1.1050                    \n",
      "[iter 26] loss=0.4284 val_loss=0.6051 scale=2.0000 norm=1.1213                    \n",
      "[iter 27] loss=0.3868 val_loss=0.5957 scale=2.0000 norm=1.0800                    \n",
      "[iter 28] loss=0.3778 val_loss=0.5910 scale=2.0000 norm=1.0949                    \n",
      "[iter 29] loss=0.3595 val_loss=0.5846 scale=2.0000 norm=1.0929                    \n",
      "[iter 30] loss=0.3465 val_loss=0.5764 scale=2.0000 norm=1.0880                    \n",
      "[iter 31] loss=0.3325 val_loss=0.5744 scale=2.0000 norm=1.0855                    \n",
      "[iter 32] loss=0.3049 val_loss=0.5771 scale=2.0000 norm=1.0783                    \n",
      "[iter 33] loss=0.3011 val_loss=0.5700 scale=2.0000 norm=1.0841                    \n",
      "[iter 34] loss=0.2821 val_loss=0.5688 scale=2.0000 norm=1.0827                    \n",
      "[iter 35] loss=0.2690 val_loss=0.5728 scale=2.0000 norm=1.0780                    \n",
      "[iter 36] loss=0.2609 val_loss=0.5707 scale=2.0000 norm=1.0786                    \n",
      "[iter 37] loss=0.2456 val_loss=0.5615 scale=2.0000 norm=1.0795                    \n",
      "[iter 38] loss=0.2353 val_loss=0.5627 scale=2.0000 norm=1.0714                    \n",
      "[iter 39] loss=0.2223 val_loss=0.5632 scale=2.0000 norm=1.0664                    \n",
      "[iter 40] loss=0.2184 val_loss=0.5614 scale=2.0000 norm=1.0880                    \n",
      "[iter 41] loss=0.2024 val_loss=0.5629 scale=2.0000 norm=1.0720                    \n",
      "[iter 42] loss=0.1805 val_loss=0.5694 scale=2.0000 norm=1.0577                    \n",
      "[iter 43] loss=0.1765 val_loss=0.5732 scale=2.0000 norm=1.0615                    \n",
      "[iter 44] loss=0.1930 val_loss=0.5782 scale=2.0000 norm=1.0965                    \n",
      "== Early stopping achieved.                                                       \n",
      "== Best iteration / VAL40 (val_loss=0.5614)                                       \n",
      "100%|██████████| 100/100 [04:28<00:00,  2.68s/trial, best loss: 0.5057126232505124]\n",
      "Finished tuning!\n"
     ]
    }
   ],
   "source": [
    "TRIALS = Trials()\n",
    "print('Beginning hyperparameter tuning...')\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    best = fmin(fn=objective,\n",
    "                space=space,\n",
    "                algo=tpe.suggest,\n",
    "                max_evals=100,\n",
    "                trials=TRIALS)\n",
    "print('Finished tuning!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': {'Base': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=6,\n",
       "                        max_features=0.75, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=69, splitter='best'),\n",
       "  'learning_rate': 0.026450537233589103,\n",
       "  'minibatch_frac': 0.6,\n",
       "  'n_estimators': 100},\n",
       " 'preprocessing': {'mul': True, 'n_neighbors': 3}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = space_eval(space, best)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the validation loss with respect to a hyperparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_plot(ft, trials):\n",
    "\n",
    "    print(\"Loss plot for parameter {}\".format(ft))\n",
    "    \n",
    "    _loss = list()\n",
    "    _ft = list()\n",
    "    \n",
    "    for t in trials.trials:\n",
    "        try:\n",
    "            if len(t['misc']['vals'][ft]) > 0:\n",
    "                _ft.append(t['misc']['vals'][ft][0])\n",
    "                _loss.append(t['result']['loss'])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    data = pd.DataFrame([_loss, _ft]).T\n",
    "    data.columns=['loss', ft]\n",
    "        \n",
    "    sns.lineplot(y='loss', x=ft, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss plot for parameter Base\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_plot('Base', TRIALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training the Final Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matt/anaconda3/lib/python3.7/site-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n",
      "/Users/matt/anaconda3/lib/python3.7/site-packages/ngboost/distns/normal.py:68: RuntimeWarning: overflow encountered in square\n",
      "  self.var = self.scale ** 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter 0] loss=1.5178 val_loss=1.4697 scale=1.0000 norm=1.0617\n",
      "[iter 1] loss=1.3167 val_loss=1.3673 scale=2.0000 norm=1.7612\n",
      "[iter 2] loss=1.2346 val_loss=1.2705 scale=2.0000 norm=1.6369\n",
      "[iter 3] loss=1.1755 val_loss=1.2353 scale=2.0000 norm=1.5377\n",
      "[iter 4] loss=1.1525 val_loss=1.2018 scale=2.0000 norm=1.5271\n",
      "[iter 5] loss=1.1108 val_loss=1.1744 scale=2.0000 norm=1.4672\n",
      "[iter 6] loss=1.0814 val_loss=1.1465 scale=2.0000 norm=1.4122\n",
      "[iter 7] loss=1.0668 val_loss=1.1229 scale=2.0000 norm=1.4233\n",
      "[iter 8] loss=1.0405 val_loss=1.0988 scale=2.0000 norm=1.4006\n",
      "[iter 9] loss=1.0174 val_loss=1.0771 scale=2.0000 norm=1.3729\n",
      "[iter 10] loss=0.9913 val_loss=1.0521 scale=2.0000 norm=1.3470\n",
      "[iter 11] loss=0.9762 val_loss=1.0311 scale=2.0000 norm=1.3538\n",
      "[iter 12] loss=0.9487 val_loss=1.0121 scale=2.0000 norm=1.3064\n",
      "[iter 13] loss=0.9184 val_loss=0.9920 scale=2.0000 norm=1.2819\n",
      "[iter 14] loss=0.9084 val_loss=0.9729 scale=2.0000 norm=1.2839\n",
      "[iter 15] loss=0.8816 val_loss=0.9565 scale=2.0000 norm=1.2597\n",
      "[iter 16] loss=0.8656 val_loss=0.9389 scale=2.0000 norm=1.2572\n",
      "[iter 17] loss=0.8520 val_loss=0.9215 scale=2.0000 norm=1.2578\n",
      "[iter 18] loss=0.8260 val_loss=0.9050 scale=2.0000 norm=1.2395\n",
      "[iter 19] loss=0.8051 val_loss=0.8891 scale=2.0000 norm=1.2251\n",
      "[iter 20] loss=0.7880 val_loss=0.8743 scale=2.0000 norm=1.2114\n",
      "[iter 21] loss=0.7758 val_loss=0.8536 scale=2.0000 norm=1.2271\n",
      "[iter 22] loss=0.7501 val_loss=0.8315 scale=2.0000 norm=1.2055\n",
      "[iter 23] loss=0.7231 val_loss=0.8157 scale=2.0000 norm=1.1712\n",
      "[iter 24] loss=0.7164 val_loss=0.8024 scale=2.0000 norm=1.1834\n",
      "[iter 25] loss=0.7000 val_loss=0.7870 scale=2.0000 norm=1.1823\n",
      "[iter 26] loss=0.6934 val_loss=0.7715 scale=2.0000 norm=1.1946\n",
      "[iter 27] loss=0.6592 val_loss=0.7586 scale=2.0000 norm=1.1527\n",
      "[iter 28] loss=0.6543 val_loss=0.7469 scale=2.0000 norm=1.1672\n",
      "[iter 29] loss=0.6353 val_loss=0.7333 scale=2.0000 norm=1.1625\n",
      "[iter 30] loss=0.6218 val_loss=0.7218 scale=2.0000 norm=1.1550\n",
      "[iter 31] loss=0.6061 val_loss=0.7114 scale=2.0000 norm=1.1480\n",
      "[iter 32] loss=0.5838 val_loss=0.7030 scale=2.0000 norm=1.1418\n",
      "[iter 33] loss=0.5749 val_loss=0.6913 scale=2.0000 norm=1.1425\n",
      "[iter 34] loss=0.5594 val_loss=0.6790 scale=2.0000 norm=1.1384\n",
      "[iter 35] loss=0.5456 val_loss=0.6681 scale=2.0000 norm=1.1351\n",
      "[iter 36] loss=0.5309 val_loss=0.6584 scale=2.0000 norm=1.1246\n",
      "[iter 37] loss=0.5125 val_loss=0.6506 scale=2.0000 norm=1.1182\n",
      "[iter 38] loss=0.5012 val_loss=0.6420 scale=2.0000 norm=1.1094\n",
      "[iter 39] loss=0.4865 val_loss=0.6323 scale=2.0000 norm=1.1144\n",
      "[iter 40] loss=0.4793 val_loss=0.6223 scale=2.0000 norm=1.1274\n",
      "[iter 41] loss=0.4594 val_loss=0.6148 scale=2.0000 norm=1.1046\n",
      "[iter 42] loss=0.4384 val_loss=0.6066 scale=2.0000 norm=1.0946\n",
      "[iter 43] loss=0.4312 val_loss=0.5990 scale=2.0000 norm=1.0989\n",
      "[iter 44] loss=0.4311 val_loss=0.5912 scale=2.0000 norm=1.1080\n",
      "[iter 45] loss=0.4247 val_loss=0.5846 scale=2.0000 norm=1.1106\n",
      "[iter 46] loss=0.4042 val_loss=0.5781 scale=2.0000 norm=1.1119\n",
      "[iter 47] loss=0.4004 val_loss=0.5718 scale=2.0000 norm=1.0970\n",
      "[iter 48] loss=0.3770 val_loss=0.5644 scale=2.0000 norm=1.0858\n",
      "[iter 49] loss=0.3695 val_loss=0.5566 scale=2.0000 norm=1.0840\n",
      "[iter 50] loss=0.3565 val_loss=0.5521 scale=2.0000 norm=1.0865\n",
      "[iter 51] loss=0.3508 val_loss=0.5469 scale=2.0000 norm=1.0863\n",
      "[iter 52] loss=0.3361 val_loss=0.5447 scale=2.0000 norm=1.0665\n",
      "[iter 53] loss=0.3276 val_loss=0.5392 scale=2.0000 norm=1.0846\n",
      "[iter 54] loss=0.3144 val_loss=0.5357 scale=2.0000 norm=1.0730\n",
      "[iter 55] loss=0.3064 val_loss=0.5302 scale=2.0000 norm=1.0778\n",
      "[iter 56] loss=0.2999 val_loss=0.5281 scale=2.0000 norm=1.0795\n",
      "[iter 57] loss=0.2962 val_loss=0.5242 scale=2.0000 norm=1.0751\n",
      "[iter 58] loss=0.2890 val_loss=0.5196 scale=2.0000 norm=1.0793\n",
      "[iter 59] loss=0.2771 val_loss=0.5182 scale=2.0000 norm=1.0786\n",
      "[iter 60] loss=0.2766 val_loss=0.5152 scale=2.0000 norm=1.0733\n",
      "[iter 61] loss=0.2748 val_loss=0.5144 scale=1.0000 norm=0.5443\n",
      "[iter 62] loss=0.2729 val_loss=0.5144 scale=2.0000 norm=1.0830\n",
      "[iter 63] loss=0.2508 val_loss=0.5131 scale=2.0000 norm=1.0686\n",
      "[iter 64] loss=0.2349 val_loss=0.5123 scale=2.0000 norm=1.0597\n",
      "[iter 65] loss=0.2365 val_loss=0.5092 scale=2.0000 norm=1.0661\n",
      "[iter 66] loss=0.2296 val_loss=0.5086 scale=2.0000 norm=1.0607\n",
      "[iter 67] loss=0.2243 val_loss=0.5057 scale=2.0000 norm=1.0706\n",
      "[iter 68] loss=0.2210 val_loss=0.5066 scale=2.0000 norm=1.0676\n",
      "[iter 69] loss=0.2271 val_loss=0.5072 scale=1.0000 norm=0.5398\n",
      "[iter 70] loss=0.2136 val_loss=0.5063 scale=1.0000 norm=0.5361\n",
      "[iter 71] loss=0.2098 val_loss=0.5069 scale=2.0000 norm=1.0753\n",
      "== Early stopping achieved.\n",
      "== Best iteration / VAL67 (val_loss=0.5057)\n"
     ]
    }
   ],
   "source": [
    "# Run through data pipeline\n",
    "data_pipeline = pipeline(**best_params['preprocessing'])\n",
    "X_train_c = data_pipeline.fit_transform(X_train)\n",
    "X_val_c = data_pipeline.transform(X_val)\n",
    "\n",
    "# Run through model\n",
    "best_params['model'].update(default_params)\n",
    "final_ngb = NGBRegressor(**best_params['model']).fit(X_train_c, y_train, X_val=X_val_c, Y_val=y_val, early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate our model with various metrics on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test data set through pipeline\n",
    "X_test = data_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.183, MAE: 0.330, NLL: 0.651\n"
     ]
    }
   ],
   "source": [
    "y_pred = final_ngb.predict(X_test)\n",
    "y_dist = final_ngb.pred_dist(X_test)\n",
    "mae = median_absolute_error(y_test, y_pred)\n",
    "mea = mean_absolute_error(y_test, y_pred)\n",
    "nll = -y_dist.logpdf(y_test).mean()\n",
    "print(f'MAE: {mae:0.3f}, MAE: {mea:0.3f}, NLL: {nll:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(final_ngb, open(NGB_PATH, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
